{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6tvli18FuEQ",
        "outputId": "7cd5f223-7faf-4cb7-a113-837b891fcfb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 22 15:05:04 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8    12W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu_-nu-g5hSB",
        "outputId": "f8dde103-96d9-4dca-a573-c6982d901eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio===0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU3AvOlhF5mi",
        "outputId": "f504596f-eda6-4d7b-aea7-1ef865a7b981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
            "Requirement already satisfied: torch==1.10.2+cu113 in /usr/local/lib/python3.7/dist-packages (1.10.2+cu113)\n",
            "Requirement already satisfied: torchvision==0.11.3+cu113 in /usr/local/lib/python3.7/dist-packages (0.11.3+cu113)\n",
            "Requirement already satisfied: torchaudio===0.10.2+cu113 in /usr/local/lib/python3.7/dist-packages (0.10.2+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.2+cu113) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.3+cu113) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.3+cu113) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Ab4X0Vm-G9Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddwVUo_dHIM2",
        "outputId": "86041d63-f4f3-464f-eee2-bfa21965f5ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "config = {\n",
        "    'train_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/train.csv',\n",
        "    'test_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/test.csv',\n",
        "    'train_val_ratio':0.1,\n",
        "    'model_path':'/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model',\n",
        "    'batch_size':16,\n",
        "    'num_epochs':1,\n",
        "    'learning_rate':2e-5,\n",
        "    'logging_step':500,\n",
        "    'seed':2022\n",
        "}\n",
        "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    return seed\n",
        "\n",
        "seed_everything(config['seed'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCFJeMYM4v06",
        "outputId": "3affdd21-92ba-49ce-984a-3115c31c66a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2022"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "def read_data(config, tokenizer, mode = 'train'):\n",
        "    data_df = pd.read_csv(config[f'{mode}_file_path'], sep=',')\n",
        "    if mode == 'train':\n",
        "        # 如果是训练文件 则划分训练集、测试集\n",
        "        X_train, y_train = defaultdict(list),[]\n",
        "        X_val, y_val = defaultdict(list),[]\n",
        "        num_val = int(len(data_df) * config['train_val_ratio'])\n",
        "    else:\n",
        "        #只生成测试集\n",
        "        X_test, y_test = defaultdict(list),[]\n",
        "        \n",
        "        \n",
        "    for i, row in tqdm(data_df.iterrows(), desc=f'preprocess {mode} data', total =len(data_df)):\n",
        "        #得到每个句子的标签\n",
        "        label = row[1] if mode == 'train' else 0\n",
        "        #得到每个句子\n",
        "        sentence = row[-1]\n",
        "        # add_special_tokens 添加一些特殊的toke： CLS、 SEP\n",
        "        # return_token_type_ids 返回每个词所对应的id\n",
        "        # return_attention_mask 批量时， padding部分无需注意\n",
        "        inputs = tokenizer.encode_plus(sentence, add_special_tokens = True, return_token_type_ids =True, return_attention_mask = True )\n",
        "        # return：\n",
        "        # input_ids；\n",
        "        # token_type_ids\n",
        "        # attention_mask\n",
        "        if mode == 'train':\n",
        "            if i < num_val:\n",
        "                X_val['inputs_ids'].append(inputs['input_ids'])\n",
        "                y_val.append(label)\n",
        "                X_val['token_type_ids'].append(inputs['token_type_ids'])\n",
        "                X_val['attention_mask'].append(inputs['attention_mask'])\n",
        "            else:\n",
        "                X_train['inputs_ids'].append(inputs['input_ids'])\n",
        "                y_train.append(label)\n",
        "                X_train['token_type_ids'].append(inputs['token_type_ids'])\n",
        "                X_train['attention_mask'].append(inputs['attention_mask'])\n",
        "                \n",
        "        else:\n",
        "                X_test['inputs_ids'].append(inputs['input_ids'])\n",
        "                y_test.append(label) \n",
        "                X_test['token_type_ids'].append(inputs['token_type_ids'])\n",
        "                X_test['attention_mask'].append(inputs['attention_mask'])\n",
        "                \n",
        "                \n",
        "    if mode == 'train':\n",
        "        label2id ={label: i for i, label in enumerate(np.unique(y_train))}\n",
        "        id2label ={i: label for label, i in label2id.items()}\n",
        "        y_train = torch.tensor([label2id[i] for i in y_train],dtype = torch.long)\n",
        "        y_val = torch.tensor([label2id[i] for i in y_val],dtype = torch.long)\n",
        "        return X_train, y_train, X_val, y_val, label2id, id2label\n",
        "    \n",
        "    else:\n",
        "        y_test = torch.tensor(y_test, dtype = torch.long)\n",
        "        return X_test,y_test        "
      ],
      "metadata": {
        "id": "dLNhG9835UdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class TNEWSData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.x = X\n",
        "        self.y = y \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'inputs_ids': self.x['inputs_ids'][idx],\n",
        "            'label': self.y[idx],\n",
        "            'token_type_ids': self.x['token_type_ids'][idx],\n",
        "            'attention_mask': self.x['attention_mask'][idx]\n",
        "        }\n",
        "    def __len__(self):\n",
        "        return self.y.size(0)"
      ],
      "metadata": {
        "id": "jKBeg2V55W2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(example):\n",
        "    #从 TNEWData 返回的{}中，多了两个key：token_type_ids、attention_mask\n",
        "    input_ids_list = []\n",
        "    labels = []\n",
        "    token_type_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    \n",
        "    for ex in example:\n",
        "        input_ids_list.append(ex['inputs_ids'])\n",
        "        labels.append(ex['label'])\n",
        "        token_type_ids_list.append(ex['token_type_ids'])\n",
        "        attention_mask_list.append(ex['attention_mask'])\n",
        "    \n",
        "    #对齐放入tensor\n",
        "    max_len = max(len(input_ids) for input_ids in input_ids_list)\n",
        "    # shape (len(labels), max_length)\n",
        "    input_ids_tensor = torch.zeros((len(labels), max_len), dtype=torch.long)\n",
        "    token_type_ids_tensor = torch.zeros_like(input_ids_tensor)\n",
        "    attention_mask_tensor = torch.zeros_like(input_ids_tensor)\n",
        "    \n",
        "    \n",
        "    # 把列表中的数据放入tensor里\n",
        "    for i, input_ids in enumerate(input_ids_list):\n",
        "        input_ids_tensor[i, :len(input_ids)] = torch.tensor(input_ids, dtype = torch.long)\n",
        "        token_type_ids_tensor[i, :len(input_ids)] = torch.tensor(token_type_ids_list[i], dtype= torch.long)\n",
        "        attention_mask_tensor[i, :len(input_ids)] = torch.tensor(attention_mask_list[i], dtype= torch.long)\n",
        "        \n",
        "        \n",
        "    return {\n",
        "        'input_ids': input_ids_tensor,\n",
        "        'label': torch.tensor(labels, dtype = torch.long),\n",
        "        'token_type_ids': token_type_ids_tensor,\n",
        "        'attention_mask':  attention_mask_tensor\n",
        "    }"
      ],
      "metadata": {
        "id": "hzZkMxcG5Yr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "def build_dataloader(config):\n",
        "    #加载词表\n",
        "    tokenizer = BertTokenizer.from_pretrained(config['model_path'])\n",
        "    X_train, y_train, X_val, y_val, label2id, id2label = read_data(config, tokenizer, mode='train')\n",
        "    X_test, y_test = read_data(config, tokenizer, mode='test')\n",
        "    \n",
        "    train_dataset = TNEWSData(X_train, y_train)\n",
        "    val_dataset=TNEWSData(X_val, y_val)\n",
        "    test_dataset = TNEWSData(X_test, y_test)\n",
        "    \n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=True, collate_fn=collate_fn)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=False, collate_fn=collate_fn)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=False, collate_fn=collate_fn)\n",
        "    \n",
        "    \n",
        "    return train_dataloader, val_dataloader, test_dataloader, id2label"
      ],
      "metadata": {
        "id": "xYe1suVy5bTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymm_5Oou5_G6",
        "outputId": "1faa5b72-346a-4b59-ed9b-18e96d3af194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, val_dataloader, test_dataloader, id2label = build_dataloader(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnqvqaje5dXq",
        "outputId": "052f2351-d8b3-44c0-f483-058f8ad4b339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "preprocess train data: 100%|██████████| 53360/53360 [00:23<00:00, 2297.39it/s]\n",
            "preprocess test data: 100%|██████████| 10000/10000 [00:04<00:00, 2360.47it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in tqdm(iter(train_dataloader)):\n",
        "    print(batch)\n",
        "    break"
      ],
      "metadata": {
        "id": "mFt7majE5p9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7db76fa-e553-444e-e542-16e4f3c802c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "  0%|          | 0/3002 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  6206,  2682,  4381,  6760,  5994,  2877,  5500,  4873,  8024,\n",
            "          7444,  6206,  2110,   739,  1290,   711,  4638,  6411,   928,  1501,\n",
            "          6574,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  3299,  1057,  8283,  1914,  5543,  1762,  7028,  2412,  2356,\n",
            "          2902,  2999,   743,  2791,  1408,  8043,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  3797,  1977,   677,  4028,   749,   671,  1767,  2661,  2552,\n",
            "          1220,  7790,  4638,  4260,  6121,  6381,  8024,  3797,  1977,  1079,\n",
            "          2552,  8038,  3315,  3797,  2218,  3221,  6821,   720,  7390,  2692,\n",
            "           102,     0,     0],\n",
            "        [  101,  1506,  4633,  8038,   924,  5384,  1213,   924,  2769,   812,\n",
            "          8024,   800,  2970,  5052,   749,  3683,  6612,  8024,  2802,  2533,\n",
            "          2523,  3472,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  3284,  3319,  2051,  1358,   679,  1062,  3633,  2521,  6878,\n",
            "          8013,  7566,  1946,  3187,  1937,  6134,  2658,  6375,   782,  2552,\n",
            "          7000,  8024,   686,  4518,  5018,   671,   782,  6158,  6768,  6228,\n",
            "          1408,   102,     0],\n",
            "        [  101,  3330,  1046,  2487,  8038,   704,  1744,  1398,  2692,  5314,\n",
            "           750,  3189,  3315,  8202,   783,  1039, 11635,  9864,  8169,  7583,\n",
            "          2428,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  1963,   862,  4692,  2521,   153, 12569,  2347,  5307,  8128,\n",
            "          6825,  5526, 11998,  8181,   119,   155,  3417,  2552,   704,  1296,\n",
            "          3274,  7599,  7229,  6612,  1767,   100,  4312,  1965,   100,  6427,\n",
            "          2497,  8043,   102],\n",
            "        [  101,  4495,  3833,   704,  1282,  2897,  1282,  4937,  4638,   948,\n",
            "          6756,  3175,  3791,  8024,   833,  4500,  6821,   702,  2207,   691,\n",
            "          6205,  8024,   948,  6756,   794,  3341,   679,  4500,  5018,   753,\n",
            "          2828,   102,     0],\n",
            "        [  101,   967,  1814,   809,  5470,  1316,  2768,   711,  3959,  7582,\n",
            "           118,  3457,  3959,  2096,  7411,  1814,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,   704,  1744,  1744,   772,  7674,  1798,  4958,   704,  1217,\n",
            "          3779,  3322,  6764,   118,   127,  8197,  3284,  1045,  8024,   704,\n",
            "          1744,  1348,  3924,   671,  1164,  1690,  8024,  1059,  3696,  2920,\n",
            "          1939,   102,     0],\n",
            "        [  101,  9854,  3322,  6663,  4772,  1469,  6663,  1555,  2787,  3221,\n",
            "           784,   720,  8043,   102,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  6821,   697,   702,  2870,  2861,  3322,  4685,  2345,   788,\n",
            "          8108,   702,  7716,  1213,  8024,  1377,   711,   784,   720,   817,\n",
            "          3419,  1316,  4685,  1068,  8108,   674,  1779,  7178,  1450,  8043,\n",
            "           102,     0,     0],\n",
            "        [  101,  7511,  1068,  1453,  6804,   676,  3189,  3952,  1963,   862,\n",
            "          6226,  1153,  6662,  5296,  8043,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  6821,  4905,  7029,  5831,  7270,  2533,  1008,  4344,  4259,\n",
            "          2094,  8024,  2356,  1767,  4958,  7313,  1920,  8024,  2768,   749,\n",
            "          1093,  3696,  5636,  2168,  4638,   100,  3031,  7178,  3409,   100,\n",
            "           102,     0,     0],\n",
            "        [  101,  2582,   720,  3416,  1377,   809,  3297,  2571,  6862,  4638,\n",
            "          2990,  7770,  3159,  1765,   712,  4638,  3717,  2398,  8043,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  3749,  6756,   924,  7372,  3221,   679,  3221,  5326,   677,\n",
            "           671,  2399,  6632,   707,  6818,  1168,  3309,  3189,   743,  2218,\n",
            "          6632,   912,  2139,  8043,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0]]), 'label': tensor([ 4,  5, 14,  3,  3, 11, 14,  6,  5,  9,  8, 13, 10, 13,  3,  4]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def evaluation(config, model, val_dataloader):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    labels = []\n",
        "    val_loss = 0.\n",
        "    val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in val_iterator:\n",
        "            labels.append(batch['labels'])\n",
        "            batch = {item: value.to(config['device']) for item, value in batch.items()}\n",
        "            loss, logits = model(**batch)[:2]\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            preds.append(logits.argmax(dim = -1).detach().cpu())\n",
        "            \n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    labels = torch.cat(labels, dim = 0).numpy()\n",
        "    preds = torch.cat(preds, dim=0).numpy()\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "    return avg_val_loss, f1"
      ],
      "metadata": {
        "id": "eoB8gWJUMonr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train BERT\n",
        "from transformers import BertConfig, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from tqdm import trange\n",
        "def train(config, id2label, train_dataloader, val_dataloader):\n",
        "    #配置文件\n",
        "    bert_config = BertConfig.from_pretrained(config['model_path'])\n",
        "    bert_config.num_labels = len(id2label)\n",
        "    model = BertForSequenceClassification.from_pretrained(config['model_path'], config = bert_config)\n",
        "    \n",
        "    #优化器\n",
        "    optimizer = AdamW(model.parameters(), lr = config['learning_rate'])\n",
        "    \n",
        "    #放入GPU\n",
        "    model.to(config['device'])\n",
        "    epoch_iterator = trange(config['num_epochs'])\n",
        "    global_steps = 0\n",
        "    train_loss = 0.\n",
        "    logging_loss =0.\n",
        "    \n",
        "    #跑了几个迭代\n",
        "    for epoch in epoch_iterator:\n",
        "        train_iterator = tqdm(train_dataloader, desc='Train', total=len(train_dataloader))\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            #字典中的value送入GPU\n",
        "            batch = {item: value.to(config['device']) for item, value in batch.items()}\n",
        "            \n",
        "            \n",
        "            #将字典作为关键字参数传递给python中函数\n",
        "            loss = model(**batch)[0]\n",
        "            \n",
        "            #模型参数梯度设置为0\n",
        "            model.zero_grad()\n",
        "            \n",
        "            #反向传播\n",
        "            loss.backward()\n",
        "            \n",
        "            #更新参数\n",
        "            optimizer.step()\n",
        "            \n",
        "            #叠加loss\n",
        "            train_loss += loss.item()\n",
        "            global_steps += 1\n",
        "            \n",
        "            if gloval_steps % config['logging_step'] == 0:\n",
        "                print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
        "                \n",
        "                logging_loss = train_loss\n",
        "                \n",
        "                avg_val_loss, f1 = evaluation(config, model, val_dataloader)\n",
        "                \n",
        "                print_log = f'>>> training loss: {print_train_loss: .5f}, val loss: {avg_val_loss: .5f}, valid f1 score: {f1: .5f}'\n",
        "                print(print_log)\n",
        "                model.train()\n",
        "    return model\n",
        "            "
      ],
      "metadata": {
        "id": "hS112NMnMsPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(config, id2label, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "WQO00UA7MuoB",
        "outputId": "3aac7170-3a75-40bd-82c5-49c9ed7d79a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m                     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1440\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: unexpected EOF, expected 1675237 more bytes. The file might be corrupted.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"version\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m                                 raise OSError(\n",
            "\u001b[0;32m/usr/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-be2bad6e5e17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-7d1d25aa754e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, id2label, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbert_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbert_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#优化器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m                         raise OSError(\n\u001b[0;32m-> 1453\u001b[0;31m                             \u001b[0;34mf\"Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1454\u001b[0m                             \u001b[0;34mf\"at '{resolved_archive_file}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m                             \u001b[0;34m\"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for '/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model' at '/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
          ]
        }
      ]
    }
  ]
}