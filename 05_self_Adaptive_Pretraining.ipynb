{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1fef4a68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fef4a68",
        "outputId": "52f2c49e-f572-4c88-d1e1-daee7af59098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dc9e15cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc9e15cb",
        "outputId": "860ca9c8-d172-4de1-b9d5-fa7ecd587469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "#设置路径\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "eb14c151",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb14c151",
        "outputId": "919bb0b8-1755-41f6-a703-8442d5ebeccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.0.1\n",
            "  Downloading transformers-4.0.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 215 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 225 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 235 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 245 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 256 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 266 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 276 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 286 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 296 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 307 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 317 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 327 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 337 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 348 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 358 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 368 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 378 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 389 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 399 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 409 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 419 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 430 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 440 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 450 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 460 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 471 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 481 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 491 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 501 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 512 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 522 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 532 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 542 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 552 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 563 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 573 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 583 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 593 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 604 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 614 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 624 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 634 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 645 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 655 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 665 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 675 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 686 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 696 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 706 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 716 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 727 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 737 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 747 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 757 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 768 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 778 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 788 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 798 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 808 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 819 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 829 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 839 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 849 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 860 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 870 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 880 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 890 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 901 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 911 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 921 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 931 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 942 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 952 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 962 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 972 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 983 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 993 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.2 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.0.1) (3.0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1fa016cf6ce544dc6cb86d50008bd7856227cebc72f5ebc148718623d851d742\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.9.4 transformers-4.0.1\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers==4.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "01ecf1ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01ecf1ca",
        "outputId": "da7ee29e-0a5a-4391-c56f-bd1db8d15f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.6.0\n"
          ]
        }
      ],
      "source": [
        "! pip install torch==1.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "365865c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "365865c6",
        "outputId": "13f7a5c4-a84f-47e3-9174-49324ff0cfbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.7.0\n",
            "  Downloading torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (1.21.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (7.1.2)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->torchvision==0.7.0) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "Successfully installed torchvision-0.7.0\n"
          ]
        }
      ],
      "source": [
        "! pip install torchvision==0.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5924d70e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "5924d70e",
        "outputId": "b8f3c19b-2a30-4d5a-b284-d5033d453cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.17.0\n",
            "  Downloading numpy-1.17.0-cp37-cp37m-manylinux1_x86_64.whl (20.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.3 MB 638 kB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.17.0 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.17.0 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.17.0 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.17.0 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.17.0 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.17.0 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.17.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install numpy==1.17.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cccccd71",
      "metadata": {
        "id": "cccccd71"
      },
      "source": [
        "## logging\n",
        "Transformers 有一个集中的日志记录系统，因此您可以轻松设置库的详细程度。\n",
        "目前该库的默认详细程度是WARNING.\n",
        "* transformers.logging.CRITICALor transformers.logging.FATAL(int value, 50)：只报告最严重的错误。\n",
        "* transformers.logging.ERROR(int value, 40)：只报告错误。\n",
        "* transformers.logging.WARNINGor transformers.logging.WARN(int value, 30)：只报告错误和警告。这是库使用的默认级别。\n",
        "* transformers.logging.INFO(int value, 20)：报告错误、警告和基本信息。\n",
        "* transformers.logging.DEBUG(int value, 10)：上报所有信息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ef63b10d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef63b10d",
        "outputId": "c47039ac-4249-4907-9d57-f8041e1b3744"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2022"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from transformers import logging\n",
        "from transformers import BertTokenizer\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  return seed\n",
        "\n",
        "logging.set_verbosity_info()\n",
        "seed_everything(2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8099e3d9",
      "metadata": {
        "id": "8099e3d9"
      },
      "outputs": [],
      "source": [
        "corpus_path = '/content/drive/MyDrive/Colab Notebooks/dataset/ESIM'\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model'\n",
        "output_dir = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e7183f11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7183f11",
        "outputId": "705b334b-831d-4717-c2d6-194c8fe8220c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model name '/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "Didn't find file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/special_tokens_map.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/tokenizer_config.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/tokenizer.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/vocab.txt\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "796b487c",
      "metadata": {
        "id": "796b487c"
      },
      "outputs": [],
      "source": [
        "def parse_data(path, data_type='train'):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "\n",
        "  with open(path, 'r', encoding = 'utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      if data_type != 'test':\n",
        "        labels.append(int(line['label']))\n",
        "      else:\n",
        "        labels.append(0)\n",
        "\n",
        "  df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns = ['text_a', 'text_b', 'labels'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d47951b0",
      "metadata": {
        "id": "d47951b0"
      },
      "outputs": [],
      "source": [
        "def read_data(config, tokenizer):\n",
        "  train_df = parse_data(os.path.join(corpus_path, 'train.json'), data_type = 'train')\n",
        "  dev_df = parse_data(os.path.join(corpus_path, 'dev.json'), data_type = 'dev')\n",
        "  test_df = parse_data(os.path.join(corpus_path, 'test.json'), data_type = 'test')\n",
        "\n",
        "  train_df.append(dev_df)\n",
        "  train_df.append(test_df)\n",
        "  inputs = defaultdict(list)\n",
        "\n",
        "\n",
        "  for i, row in tqdm(train_df.iterrows(), desc= f'Preprocessing train data', total = len(train_df)):\n",
        "      inputs_dict = tokenizer.encode_plus(row[0] + row[1], add_special_tokens = True,\n",
        "                                          return_token_type_ids = True, return_attention_mask = True)\n",
        "      inputs['input_ids'].append(inputs_dict['input_ids'])\n",
        "      inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
        "      inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
        "    \n",
        "    \n",
        "    \n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "afc2b796",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afc2b796",
        "outputId": "37b4489e-3ee4-4552-e7be-d3700a53d505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 245634.21it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 239582.53it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 232916.35it/s]\n",
            "Preprocessing train data: 100%|██████████| 34334/34334 [00:16<00:00, 2136.78it/s]\n"
          ]
        }
      ],
      "source": [
        "data = read_data(corpus_path, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "  \n",
        "  # 返回一个example\n",
        "  def __getitem__(self, idx):\n",
        "    data = (self.data_dict['input_ids'][idx],\n",
        "         self.data_dict['token_type_ids'][idx],\n",
        "         self.data_dict['attention_mask'][idx])\n",
        "    return data\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])"
      ],
      "metadata": {
        "id": "uIF0E30pjtDO"
      },
      "id": "uIF0E30pjtDO",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AFQMCDataset(data)"
      ],
      "metadata": {
        "id": "z4jLz1EUlxpb"
      },
      "id": "z4jLz1EUlxpb",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "from transformers import BertTokenizerFast, BertForMaskedLM\n",
        "from transformers import LineByLineTextDataset, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "def train(self):\n",
        "    # model_path是预训练模型的位置\n",
        "    # 分词 BertTokenizerFast\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_path, max_len=512)\n",
        "    # 定义一个模型 BertForMaskedLM\n",
        "    model = BertForMaskedLM.from_pretrained(model_path)\n",
        "    # daseset LineByLineTextDataset\n",
        "    dataset = LineByLineTextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=\"语料文件.txt\",\n",
        "        block_size=128,\n",
        "    )\n",
        "    # collator DataCollatorForLanguageModeling\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=True, mlm_probability=0.10\n",
        "    )\n",
        "    # TrainingArguments 训练参数，使用的接口类\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"xxxx\",\n",
        "        ...各种参数，参见文档或者源码\n",
        "    )\n",
        "    \n",
        "    # 模型训练用Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(\"保存位置\")\n",
        "```"
      ],
      "metadata": {
        "id": "2SYAtGRXmEiT"
      },
      "id": "2SYAtGRXmEiT"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM\n",
        "model = BertForMaskedLM.from_pretrained(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8PGodkZnEEy",
        "outputId": "7baef083-422e-48ae-acbd-95b87a3b457d"
      },
      "id": "G8PGodkZnEEy",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertForMaskedLM were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# N -Gram N: 1,2,3\n",
        "ngrams = np.arange(1, 4, dtype = np.int64)\n",
        "print('ngrams:',ngrams)\n",
        "pvals = 1. / np.arange(1, 4)\n",
        "print('pvals(1):',pvals) \n",
        "p = pvals.sum(keepdims = True)\n",
        "print('psum:',p)\n",
        "pvals /= pvals.sum(keepdims = True)\n",
        "print('pvals(2):',pvals)\n",
        "pvals = pvals[::-1]\n",
        "print('pvals(3):',pvals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbJmI-7vnS0y",
        "outputId": "b911f401-b02f-4c71-f327-a96f7c52359a"
      },
      "id": "fbJmI-7vnS0y",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrams: [1 2 3]\n",
            "pvals(1): [1.         0.5        0.33333333]\n",
            "psum: [1.83333333]\n",
            "pvals(2): [0.54545455 0.27272727 0.18181818]\n",
            "pvals(3): [0.18181818 0.27272727 0.54545455]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "05_self_Adaptive_Pretraining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}