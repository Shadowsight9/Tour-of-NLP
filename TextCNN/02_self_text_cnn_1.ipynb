{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49925154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e4cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_embedding import get_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44eeac",
   "metadata": {},
   "source": [
    "### about self：\n",
    "\n",
    "在Python类中规定，函数的第一个参数是实例对象本身，并且约定俗成，把其名字写为self。其左右相当于java中的this，表示当前类的对象，可以调用当前类中的属性和方法。\n",
    "\n",
    "对应的self.valueName，self.function()中的valueName和function()具体含义如下：valueName：表示self对象，即实例的变量。与其他的，Class的变量，全局的变量，局部的变量，是相对应的。function：表示是调用的是self对象，即实例的函数。与其他的全局的函数，是相对应的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9d3e8",
   "metadata": {},
   "source": [
    "## nn.Mudule\n",
    "torcn.nn是专门为神经网络设计的模块化接口. nn构建于autograd之上，可以用来定义和运行神经网络。\n",
    "\n",
    "nn.Module是nn中十分重要的类，包含网络各层的定义及forward方法。\n",
    "\n",
    "nn.Module 有 8 个属性，都是OrderDict(有序字典)。在 LeNet 的__init__()方法中会调用父类nn.Module的__init__()方法，创建这 8 个属性。\n",
    "\n",
    "_parameters 属性：存储管理 nn.Parameter 类型的参数\n",
    "\n",
    "_modules 属性：存储管理 nn.Module 类型的参数\n",
    "\n",
    "_buffers 属性：存储管理缓冲属性，如 BN 层中的 running_mean\n",
    "\n",
    "5 个 ***_hooks 属性：存储管理钩子函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a44d3f",
   "metadata": {},
   "source": [
    "## Conv2d\n",
    "二维卷积可以处理二维数据\n",
    "\n",
    "nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True))\n",
    "参数：\n",
    "\n",
    "  in_channel:　输入数据的通道数，例RGB图片通道数为3；\n",
    "  \n",
    "  out_channel: 输出数据的通道数，这个根据模型调整；\n",
    "  \n",
    "  kennel_size: 卷积核大小，可以是int，或tuple；kennel_size=2,意味着卷积大小(2,2)， kennel_size=（2,3），意味着卷积大小（2，3）即非正方形卷积\n",
    "  \n",
    "  stride：步长，默认为1，与kennel_size类似，stride=2,意味着步长上下左右扫描皆为2， stride=（2,3），左右扫描步长为2，上下为3；\n",
    "  \n",
    "  padding：　零填充\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3513128",
   "metadata": {},
   "source": [
    "## nn.Dropout\n",
    "Dropout是指在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了\n",
    "让某个神经元的激活值以一定的概率p，让其停止工作，这次训练过程中不更新权值，也不参加神经网络的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3851b73",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "nn.Linear（）是用于设置网络中的全连接层的，需要注意在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量，形状通常为[batch_size, size]，不同于卷积层要求输入输出是四维张量。\n",
    "\n",
    "具体而言：\n",
    "torch. $\\mathrm{nn} .$ Linear(in_features, out_features, bias=True)\n",
    "\n",
    "in_features指的是输入的二维张量的大小，即输入的[batch_size, size]中的size。\n",
    "\n",
    "out_features指的是输出的二维张量的大小，即输出的二维张量的形状为[batch_size，output_size]，当然，它也代表了该全连接层的神经元个数。\n",
    "\n",
    "从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f9ff9",
   "metadata": {},
   "source": [
    "## squeeze函数\n",
    "torch.squeeze(input, dim=None, out=None)\n",
    "\n",
    "squeeze()函数的功能是维度压缩。返回一个tensor（张量），其中 input 中大小为1的所有维都已删除。\n",
    "\n",
    "举个例子：如果 input 的形状为 (A×1×B×C×1×D)，那么返回的tensor的形状则为 (A×B×C×D)\n",
    "\n",
    "当给定 dim 时，那么只在给定的维度（dimension）上进行压缩操作。\n",
    "\n",
    "举个例子：如果 input 的形状为 (A×1×B)，squeeze(input, 0)后，返回的tensor不变；squeeze(input, 1)后，返回的tensor将被压缩为 (A×B)\n",
    "\n",
    "还有 **unsqueeze()** 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded420c7",
   "metadata": {},
   "source": [
    "## max_pool1d\n",
    "\n",
    "torch.nn.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n",
    "\n",
    "参数：\n",
    "\n",
    "kernel_size(int or tuple) - max pooling的窗口大小\n",
    "\n",
    "stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size\n",
    "\n",
    "padding(int or tuple, optional) - 输入的每一条边补充0的层数\n",
    "\n",
    "dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数\n",
    "\n",
    "return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助\n",
    "\n",
    "ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作\n",
    "\n",
    "shape:\n",
    "\n",
    "$N$ is batch dimension, $C$ is the channel dimension\n",
    "\n",
    "输入: ($N$, $C$,$L_{i n}$)\n",
    "\n",
    "输出: ($N$, $C$,$L_{o u t}$)\n",
    "\n",
    "$L_{o u t}=\\left\\lfloor\\frac{L_{i n}+2 \\times \\text { padding }-\\text { dilation } \\times(\\text { kernel_size }-1)-1}{\\text { stride }}+1\\right\\rfloor$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc12a8",
   "metadata": {},
   "source": [
    "## torch.cat\n",
    "torch.cat是将两个张量（tensor）拼接在一起，cat是concatenate的意思，即拼接，联系在一起。\n",
    "\n",
    "C=torch.cat((A,B),0)就表示按维数0（行）拼接A和B，也就是竖着拼接，A上B下。此时需要注意：列数必须一致，即维数1数值要相同，这里都是3列，方能列对齐。拼接后的C的第0维是两个维数0数值和，即2+4=6.\n",
    "\n",
    "C=torch.cat((A,B),1)就表示按维数1（列）拼接A和B，也就是横着拼接，A左B右。此时需要注意：行数必须一致，即维数0数值要相同，这里都是2行，方能行对齐。拼接后的C的第1维是两个维数1数值和，即3+4=7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d60621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, word_embedding, each_filter_num, filter_heights, drop_out, num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embedding, freeze = True)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels = 1, out_channels = each_filter_num, kernel_size = (h, word_embedding.shape[1]))for h in filter_heights\n",
    "            #(h, word_embedding.shape[0])\n",
    "    \n",
    "        ])\n",
    "        \n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "        self.fc = nn.Linear(each_filter_num * len(filter_heights), num_classes)\n",
    "        \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self, input_ids = None):\n",
    "        word_embeddings = self.embedding(input_ids)\n",
    "        sentence_embedding = word_embeddings.unsqueeze(1)\n",
    "        \n",
    "        out = torch.cat([self.conv_and_pool(sentence_embedding, conv) for conv in self.convs], 1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        outputs = (out, )\n",
    "       \n",
    "            \n",
    "        return outputs\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9771d17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\90392\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.568 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 195202/195202 [00:02<00:00, 97162.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0428,  0.0379, -0.0926, -0.0111,  0.0021,  0.0827,  0.0329, -0.0138,\n",
      "          0.1736,  0.0675, -0.0370,  0.0514,  0.0610,  0.1683, -0.0648]],\n",
      "       grad_fn=<AddmmBackward0>),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    some_text_sentence = '今天上海大学封闭，股市大跌'\n",
    "    words = list(jieba.cut(some_text_sentence))\n",
    "    embedding, token2id, _ = get_embedding(set(words))\n",
    "    \n",
    "    text_cnn_model = TextCNN(embedding, each_filter_num = 128, filter_heights = [2, 3, 4], drop_out = 0.3, num_classes = 15)\n",
    "    ids = [token2id[w] for w in words]\n",
    "    out = text_cnn_model(torch.tensor([ids])) \n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b305fd",
   "metadata": {},
   "source": [
    "**改了一点参数，也许可以work了。。。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
