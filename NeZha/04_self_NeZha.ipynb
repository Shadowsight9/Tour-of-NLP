{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_self_NeZha.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wYwJCiyVbNt",
        "outputId": "7ef599f3-a56c-4019-9e59-fa71ad41978a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr  4 08:46:43 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8    28W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp5l0tK0Vx-g",
        "outputId": "0e0c907e-bd0e-4fb9-b06e-ffb7a304126e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers==4.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7EE2VLkWYBA",
        "outputId": "592d749e-1bed-4d21-91d2-e7db2172b0a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.0.1 in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (21.3)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (0.0.49)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.0.1) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==1.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfLrRWvXWrBH",
        "outputId": "0d390b94-89a8-4e08-e9be-66eda9a0d04f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.7/dist-packages (1.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "config = {\n",
        "    'train_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/train.csv',\n",
        "    'test_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/test.csv',\n",
        "    'train_val_ratio':0.1,\n",
        "    'model_path':'/content/drive/MyDrive/Colab Notebooks/dataset/NeZha_model',\n",
        "    'batch_size':16,\n",
        "    'head': 'cnn',\n",
        "    'num_epochs':1,\n",
        "    'learning_rate':2e-5,\n",
        "    'logging_step':500,\n",
        "    'seed':2022\n",
        "}\n",
        "\n",
        "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  return seed\n",
        "\n",
        "seed_everything(config['seed'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3jxobSvWx1f",
        "outputId": "ecda2d19-82dc-403b-9291-703241d947b9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2022"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas.DataFrame iterrows()\n",
        "iterrows() 是在数据框中的行进行迭代的一个生成器，它返回每行的索引及一个包含行本身的对象。"
      ],
      "metadata": {
        "id": "mXxL-pNraUBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "def read_data(config, tokenizer, mode = 'train'):\n",
        "  data_df = pd.read_csv(config[f'{mode}_file_path'], sep=',')\n",
        "  if mode == 'train':\n",
        "    X_train, y_train = defaultdict(list),[]\n",
        "    X_val, y_val = defaultdict(list),[]\n",
        "    num_val = int(len(data_df) * config['train_val_ratio'])\n",
        "  else:\n",
        "    X_test, y_test = defaultdict(list),[]\n",
        "\n",
        "  for i, row in tqdm(data_df.iterrows(), desc=f'preprocess {mode} data', colour = 'blue', total = len(data_df)):\n",
        "#desc（'str'）: 传入进度条的前缀\n",
        "    label = row[1] if mode == 'train' else 0\n",
        "    sentence = row[-1]\n",
        "\n",
        "    inputs = tokenizer.encode_plus(sentence, add_special_tokens = True, return_token_type_ids = True, return_attention_mask = True)\n",
        "\n",
        "    if mode == 'train':\n",
        "      if i < num_val:\n",
        "        X_val['inputs_ids'].append(inputs['input_ids'])\n",
        "        y_val.append(label)\n",
        "        X_val['token_type_ids'].append(inputs['token_type_ids'])\n",
        "        X_val['attention_mask'].append(inputs['attention_mask'])\n",
        "      else:\n",
        "        X_train['inputs_ids'].append(inputs['input_ids'])\n",
        "        y_train.append(label)\n",
        "        X_train['token_type_ids'].append(inputs['token_type_ids'])\n",
        "        X_train['attention_mask'].append(inputs['attention_mask'])\n",
        "\n",
        "    else:\n",
        "        X_test['inputs_ids'].append(inputs['input_ids'])\n",
        "        y_test.append(label)\n",
        "        X_test['token_type_ids'].append(inputs['token_type_ids'])\n",
        "        X_test['attention_mask'].append(inputs['attention_mask'])\n",
        "\n",
        "  if mode == 'train':\n",
        "    label2id = {label: i for i, label in enumerate(np.unique(y_train))}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "    y_train = torch.tensor([label2id[i] for i in y_train], dtype =torch.long)\n",
        "\n",
        "    y_val = torch.tensor([label2id[i] for i in y_val], dtype =torch.long)\n",
        "    return X_train, y_train, X_val, y_val, label2id, id2label\n",
        "\n",
        "  else:\n",
        "    y_test = torch.tensor(y_test, dtype = torch.long)\n",
        "    return X_test, y_test\n"
      ],
      "metadata": {
        "id": "X2DUyQWhYMNX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class TNEWSData(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.x = X\n",
        "    self.y = y\n",
        "  \n",
        "  #如果在类中定义了__getitem__()方法，那么他的实例对象（假设为P）就可以这样P[key]取值。\n",
        "  def __getitem__(self, idx):\n",
        "    return{\n",
        "        'inputs_ids': self.x['inputs_ids'][idx],\n",
        "        'label':self.y[idx],\n",
        "        'token_type_ids':self.x['token_type_ids'][idx],\n",
        "        'attention_mask':self.x['attention_mask'][idx]\n",
        "\n",
        "    }\n",
        "\n",
        "  #__len__()的作用是返回容器中元素的个数，要想使len()函数成功执行，必须要在类中定义__len__()。\n",
        "  def __len__(self):\n",
        "    return self.y.size(0)"
      ],
      "metadata": {
        "id": "gJQ7_VRWiTMN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(example):\n",
        "  input_ids_list = []\n",
        "  labels = []\n",
        "  token_type_ids_list = []\n",
        "  attention_mask_list = []\n",
        "\n",
        "  for ex in example:\n",
        "    input_ids_list.append(ex['inputs_ids'])\n",
        "    labels.append(ex['label'])\n",
        "    token_type_ids_list.append(ex['token_type_ids'])\n",
        "    attention_mask_list.append(ex['attention_mask'])\n",
        "\n",
        "  max_len = max(len(input_ids) for input_ids in input_ids_list)\n",
        "  input_ids_tensor = torch.zeros((len(labels), max_len),dtype=torch.long)\n",
        "  token_type_ids_tensor = torch.zeros_like(input_ids_tensor)\n",
        "  attention_mask_tensor = torch.zeros_like(input_ids_tensor)\n",
        "\n",
        "  for i, input_ids in enumerate(input_ids_list):\n",
        "    input_ids_tensor[i, :len(input_ids)] = torch.tensor(input_ids, dtype = torch.long)\n",
        "    token_type_ids_tensor[i, :len(input_ids)] = torch.tensor(token_type_ids_list[i], dtype = torch.long)\n",
        "    attention_mask_tensor[i, :len(input_ids)] = torch.tensor(attention_mask_list[i], dtype = torch.long)\n",
        "\n",
        "  return {\n",
        "      'input_ids': input_ids_tensor,\n",
        "      'labels': torch.tensor(labels ,dtype= torch.long),\n",
        "      'token_type_ids':token_type_ids_tensor,\n",
        "      'attention_mask':attention_mask_tensor\n",
        "  }  "
      ],
      "metadata": {
        "id": "UGuxw5wNjYYM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "def build_dataloader(config):\n",
        "  tokenizer = BertTokenizer.from_pretrained(config['model_path'])\n",
        "  X_train, y_train, X_val, y_val, label2id, id2label = read_data(config, tokenizer, mode='train')\n",
        "  X_test, y_test = read_data(config, tokenizer, mode='test')\n",
        "\n",
        "  train_dataset = TNEWSData(X_train, y_train)\n",
        "  val_dataset = TNEWSData(X_val, y_val)\n",
        "  test_dataset = TNEWSData(X_test, y_test)\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], num_workers = 4, shuffle = True, collate_fn=collate_fn)\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=config['batch_size'], num_workers = 4, shuffle = False, collate_fn=collate_fn)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], num_workers = 4, shuffle = False, collate_fn=collate_fn)\n",
        "\n",
        "  return train_dataloader, val_dataloader, test_dataloader, id2label\n"
      ],
      "metadata": {
        "id": "F7pExDatsi8Y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, val_dataloader, test_dataloader, id2label = build_dataloader(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxIJTuCiupAI",
        "outputId": "cec3af1c-3c27-4c6e-f7f4-8c3321db83bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "preprocess train data: 100%|\u001b[34m██████████\u001b[0m| 53360/53360 [00:33<00:00, 1584.16it/s]\n",
            "preprocess test data: 100%|\u001b[34m██████████\u001b[0m| 10000/10000 [00:06<00:00, 1658.11it/s]\n"
          ]
        }
      ]
    }
  ]
}