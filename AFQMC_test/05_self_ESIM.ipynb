{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faxwvxbp34mR",
        "outputId": "e8e55386-1bf8-4926-ce68-64631ee3a96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 24 07:01:53 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    35W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd2KAF6r4j-J",
        "outputId": "f61016dd-def1-488e-a0a1-5c83f7daa042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "#设置路径\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwtR4C9e5Vca",
        "outputId": "ae4dcd13-71f5-4fad-9a28-4946d2ff6e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.0.1\n",
            "  Downloading transformers-4.0.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 34.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.0.1) (3.0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.49 tokenizers-0.9.4 transformers-4.0.1\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers==4.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNWZrNKAJFr_",
        "outputId": "ffb1d845-1816-4e0f-871a-7f02fe059fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.6 kB/s \n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.4.0\n"
          ]
        }
      ],
      "source": [
        "! pip install torch==1.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mdTOeMT6U1w",
        "outputId": "2dcbacac-727e-4845-c53d-9f0495c13da7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2022"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "config = {\n",
        "    'train_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/train.json',\n",
        "    'dev_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/dev.json',\n",
        "    'test_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/test.json',\n",
        "    'embedding_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/sgns.weibo.word.bz2',\n",
        "    'train_val_ratio':0.1,\n",
        "    'vocab_size':30000,\n",
        "    'batch_size':64,\n",
        "    # 从后面可以看出 64能容纳99%的句子长度\n",
        "    'max_seq_len':64,\n",
        "    'num_epochs':1,\n",
        "    'learning_rate':2e-5,\n",
        "    'logging_step':100,\n",
        "    'seed':2022\n",
        "}\n",
        "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  return seed\n",
        "\n",
        "seed_everything(config['seed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v9eF7CtP5ado"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def read_data(path):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "  with open(path, 'r', encoding='utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc='Reading data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      labels.append(int(line['label']))\n",
        "  \n",
        "  df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a','text_b','labels'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkIjefdbLZnZ",
        "outputId": "6cfc7423-6c65-40aa-9308-997bea15281e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([1, 2, 3], [4, 5, 6], 1) ([777, 888, 999], [321, 543, 654], 0)\n",
            "<zip object at 0x7fd04ff83500>\n",
            "[([1, 2, 3], [777, 888, 999]), ([4, 5, 6], [321, 543, 654]), (1, 0)]\n"
          ]
        }
      ],
      "source": [
        "# 复习一下zip函数:\n",
        "# 这里有两个元组()，每个元组有两个句子，一个label\n",
        "datas = [([1,2,3],[4,5,6],1),([777,888,999],[321,543,654],0)]\n",
        "# 取出列表中每个元素（元组）\n",
        "print(*datas)\n",
        "print(zip(*datas))\n",
        "# 把 datas 中的同一类别的放在一起\n",
        "print(list(zip(*datas)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6SDvo_BHzxr",
        "outputId": "9eeab65c-53e7-44b3-8b06-ceee3897386e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data: 100%|██████████| 34334/34334 [00:00<00:00, 200883.61it/s]\n"
          ]
        }
      ],
      "source": [
        "train_df = read_data(config['train_file_path'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "u7g5tNarIggE",
        "outputId": "390a6481-d87c-453b-d1d8-e9d0dd3d63a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                text_a                                 text_b  labels\n",
              "0    蚂蚁借呗等额还款可以换成先息后本吗                             借呗有先息到期还本吗       0\n",
              "1           蚂蚁花呗说我违约一次                            蚂蚁花呗违约行为是什么       0\n",
              "2     帮我看一下本月花呗账单有没有结清                                 下月花呗账单       0\n",
              "3       蚂蚁借呗多长时间综合评估一次                                借呗得评估多久       0\n",
              "4  我的花呗账单是***，还款怎么是***  我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aab2f0d7-b9d4-4715-a8f5-025f3e2d8cfb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_a</th>\n",
              "      <th>text_b</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
              "      <td>借呗有先息到期还本吗</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>蚂蚁花呗说我违约一次</td>\n",
              "      <td>蚂蚁花呗违约行为是什么</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
              "      <td>下月花呗账单</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>蚂蚁借呗多长时间综合评估一次</td>\n",
              "      <td>借呗得评估多久</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我的花呗账单是***，还款怎么是***</td>\n",
              "      <td>我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aab2f0d7-b9d4-4715-a8f5-025f3e2d8cfb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aab2f0d7-b9d4-4715-a8f5-025f3e2d8cfb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aab2f0d7-b9d4-4715-a8f5-025f3e2d8cfb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEQpzZ1qI7FB",
        "outputId": "b055e8bf-3d40-4ad6-b9bc-8270898a3324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data: 100%|██████████| 4316/4316 [00:00<00:00, 221946.42it/s]\n"
          ]
        }
      ],
      "source": [
        "dev_df = read_data(config['dev_file_path'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "xYHahZ0BJBLJ",
        "outputId": "126ed2d2-a45b-4e90-b7fb-fac8ad158f63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             text_a                text_b  labels\n",
              "0         双十一花呗提额在哪              里可以提花呗额度       0\n",
              "1        花呗支持高铁票支付吗         为什么友付宝不支持花呗付款       0\n",
              "2  我的蚂蚁花呗支付金额怎么会有限制  我到支付宝实体店消费用花呗支付受金额限制       1\n",
              "3    为什么有花呗额度不能分期付款              花呗分期额度不足       0\n",
              "4       赠品不能设置用花呗付款            怎么不能花呗分期付款       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48849b70-7469-4b29-81a1-3be5be0ad463\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_a</th>\n",
              "      <th>text_b</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>双十一花呗提额在哪</td>\n",
              "      <td>里可以提花呗额度</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>花呗支持高铁票支付吗</td>\n",
              "      <td>为什么友付宝不支持花呗付款</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我的蚂蚁花呗支付金额怎么会有限制</td>\n",
              "      <td>我到支付宝实体店消费用花呗支付受金额限制</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>为什么有花呗额度不能分期付款</td>\n",
              "      <td>花呗分期额度不足</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>赠品不能设置用花呗付款</td>\n",
              "      <td>怎么不能花呗分期付款</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48849b70-7469-4b29-81a1-3be5be0ad463')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48849b70-7469-4b29-81a1-3be5be0ad463 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48849b70-7469-4b29-81a1-3be5be0ad463');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dev_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8wo2ged9JNGI",
        "outputId": "7b836438-b844-4487-c872-f903c9d98cca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "(train_df.text_a.str.len() + train_df.text_b.str.len()).hist(bins = 20);\n",
        "(dev_df.text_a.str.len() + dev_df.text_b.str.len()).hist(bins = 20);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpJl4UuwJmAC",
        "outputId": "8c618520-f8db-4f04-d85a-bca9900a7c4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# 64能容纳99%的句子长度\n",
        "(train_df.text_a.str.len() + train_df.text_b.str.len()).quantile(0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "64kVEP5GBNg6"
      },
      "outputs": [],
      "source": [
        "# 处理文件，生层词表放回关于train_df[train.json + dev.json],test_df\n",
        "from collections import Counter\n",
        "import jieba\n",
        "import bz2\n",
        "def preprocess(config):\n",
        "  def convert2df(file_path, dataset='train'):\n",
        "    sentence_a = []\n",
        "    sentence_b = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r', encoding='utf8') as f:\n",
        "      for line in tqdm(f.readlines(), desc=f'Reading {dataset} data'):\n",
        "        line = json.loads(line)\n",
        "        sentence_a.append(line['sentence1'])\n",
        "        sentence_b.append(line['sentence2'])\n",
        "        if dataset != 'test':\n",
        "          labels.append(int(line['label']))\n",
        "        else:\n",
        "          labels.append(0)\n",
        "        # tokens为每次新加入的句子\n",
        "        tokens = list(jieba.cut(sentence_a[-1])) + list(jieba.cut(sentence_b[-1])) \n",
        "        # print('tokens:',tokens)\n",
        "        # tokens: ['蚂蚁', '花', '呗', '说', '我', '违约', '一次', '蚂蚁', '花', '呗', '违约', '行为', '是', '什么']\n",
        "        token_counter.update(tokens)\n",
        "    df = pd.DataFrame(zip(sentence_a,sentence_b,labels),columns=['text_a','text_b','labels'])\n",
        "    return df\n",
        "  \n",
        "  token_counter = Counter()\n",
        "\n",
        "  train_df = convert2df(config['train_file_path'],'train')\n",
        "  dev_df = convert2df(config['dev_file_path'],'dev')\n",
        "  test_df = convert2df(config['test_file_path'],'test')\n",
        "\n",
        "  train_df = train_df.append(dev_df)\n",
        "  vocab = set(token for token, _ in token_counter.most_common(config['vocab_size']))\n",
        "  # print('vocab:',vocab)\n",
        "  # vocab: {'城', '帮不上', '百度', '钱分', '会加分', '我晚', '手机短信'.......}\n",
        "  return train_df, test_df, vocab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmSHkwAyEwmI",
        "outputId": "6ac9678c-151a-4fd8-c517-7a0cd38425b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rReading train data:   0%|          | 0/34334 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.213 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Reading train data: 100%|██████████| 34334/34334 [00:11<00:00, 2895.95it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:02<00:00, 1745.13it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:02<00:00, 1693.52it/s]\n"
          ]
        }
      ],
      "source": [
        "train_df, test_df, vocab = preprocess(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U0PRXceWF3Fv"
      },
      "outputs": [],
      "source": [
        "# TextCNN 词表对应成向量\n",
        "def get_embedding(vocab, embedding_file_path):\n",
        "  print('processing embedding file, please wait...')\n",
        "\n",
        "  token2embedding = {}\n",
        "\n",
        "  with bz2.open(embedding_file_path) as f:\n",
        "    token_vectors = f.readlines()\n",
        "    # print('token_vectors:',list(token_vectors[0:10]))\n",
        "    # token_vectors: [b'195202 300\\n', b'\\xef\\xbc\\x8c 0.094386 -0.200944 -0.030828 0.277130 ... 0.126182 -0.554329 -0.328050 \\n', b'......\\n'...]\n",
        "    meta_info = token_vectors[0].split()\n",
        "    # print('meta_info:',meta_info)\n",
        "    # meta_info: [b'195202', b'300']\n",
        "    print(f'{meta_info[0]} tokens in embedding file in total, vector size is {meta_info[-1]}')\n",
        "\n",
        "    for line in tqdm(token_vectors[1:]):\n",
        "      line = line.split()\n",
        "      token = line[0].decode('utf8')\n",
        "      # print('token:',token)\n",
        "      # token: ！\n",
        "      # token: 可以\n",
        "      # token: 等\n",
        "      # ...\n",
        "      vector = line[1:]\n",
        "      if token in vocab:\n",
        "        token2embedding[token] = [float(num) for num in vector]\n",
        "        # print('token2embedding[token]:',token2embedding[token])\n",
        "        # token2embedding[token]: [-0.124044, -0.053688, 0.157958, ... -0.127075, -0.020528, 0.032646]\n",
        "  #从4开始\n",
        "  token2idx = {token: idx for idx, token in enumerate(token2embedding.keys(),4)}\n",
        "  UNK, PAD, BOS, EOS = '<unk>', '<pad>', '<bos>', '<eos>'\n",
        "  token2idx[PAD] = 0 \n",
        "  token2idx[UNK] = 1\n",
        "  token2idx[BOS] = 2\n",
        "  token2idx[EOS] = 3\n",
        "  idx2token = {idx: token for token, idx in token2idx.items()}\n",
        "  idx2embedding = {token2idx[token]: embedding for token, embedding in token2embedding.items()}\n",
        "\n",
        "  idx2embedding[0] = [.0] * int(meta_info[-1])\n",
        "  idx2embedding[1] = [.0] * int(meta_info[-1])\n",
        "  idx2embedding[2] = np.random.random(int(meta_info[-1])).tolist()\n",
        "  idx2embedding[3] = np.random.random(int(meta_info[-1])).tolist()\n",
        "  emb_mat = [idx2embedding[idx] for idx in range(len(idx2embedding))]\n",
        "\n",
        "  return torch.tensor(emb_mat, dtype=torch.float), token2idx, len(vocab) + 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD-tn73jSuBi",
        "outputId": "518be36a-8f39-4e67-f938-e7386bf4f726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing embedding file, please wait...\n",
            "b'195202' tokens in embedding file in total, vector size is b'300'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195202/195202 [00:03<00:00, 55337.85it/s]\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix, token2idx, config['vocab_size'] = get_embedding(vocab, config['embedding_file_path'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qgfv519gw_JU"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "def tokenizer(sent, token2id):\n",
        "  # .get() 找到返回 token的id, 没找到就返回 1 1->UNK\n",
        "  ids = [token2id.get(token, 1) for token in jieba.cut(sent)]\n",
        "  return ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-qPPSz-z9rj"
      },
      "source": [
        "```\n",
        "df = pd.DataFrame(np.random.randn(4, 3), columns = ['col1', 'col2', 'col3'])\n",
        "df\n",
        "\n",
        "\tcol1\t  col2\t   col3\n",
        "0\t0.956880\t0.787811\t0.099237\n",
        "1\t0.413166\t-0.541869\t0.548336\n",
        "2\t0.951179\t0.113981\t1.130187\n",
        "3\t0.802346\t1.953860\t-2.062042\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "  print('i:',i,'row:',row)\n",
        "\n",
        "i: 0 row: col1    0.956880\n",
        "col2    0.787811\n",
        "col3    0.099237\n",
        "Name: 0, dtype: float64\n",
        "i: 1 row: col1    0.413166\n",
        "col2   -0.541869\n",
        "col3    0.548336\n",
        "Name: 1, dtype: float64\n",
        "i: 2 row: col1    0.951179\n",
        "col2    0.113981\n",
        "col3    1.130187\n",
        "Name: 2, dtype: float64\n",
        "i: 3 row: col1    0.802346\n",
        "col2    1.953860\n",
        "col3   -2.062042\n",
        "Name: 3, dtype: float64\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "  print('row[0]:',row[0])\n",
        "\n",
        "row[0]: 0.9568798249976214\n",
        "row[0]: 0.41316597459220283\n",
        "row[0]: 0.9511794577235149\n",
        "row[0]: 0.8023455023210482\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "l8ICD-VFxjym"
      },
      "outputs": [],
      "source": [
        "def read_data(data_df, train_val_ratio, token2id, mode = 'train'):\n",
        "  if mode == 'train':\n",
        "    X_train, y_train = defaultdict(list),[]\n",
        "    X_val, y_val = defaultdict(list),[]\n",
        "    num_val = int(len(data_df) * train_val_ratio)\n",
        "  else:\n",
        "    X_test, y_test = defaultdict(list),[]\n",
        "  \n",
        "  for i, row in tqdm(data_df.iterrows(), desc=f'Preprocessing {mode} data', total = len(data_df)):\n",
        "    text_left = row[0]\n",
        "    text_right = row[1]\n",
        "    label = row[2]\n",
        "\n",
        "    input_a = tokenizer(text_left, token2id = token2idx)\n",
        "    input_b = tokenizer(text_right, token2id = token2idx)\n",
        "    \n",
        "    if mode == 'train':\n",
        "      if i<num_val:\n",
        "        X_val['text_left'].append(input_a)\n",
        "        X_val['text_right'].append(input_b)\n",
        "        y_val.append(label)\n",
        "      else:\n",
        "        X_train['text_left'].append(input_a)\n",
        "        X_train['text_right'].append(input_b)\n",
        "        y_train.append(label)\n",
        "    else:\n",
        "      X_test['text_left'].append(input_a)\n",
        "      X_test['text_right'].append(input_b)\n",
        "      y_test.append(label)\n",
        "\n",
        "  if mode == 'train':\n",
        "    label2id = {label : i for i,label in enumerate(np.unique(y_train))}\n",
        "    id2label = {i : label for label, i in label2id.items()}\n",
        "    y_train = torch.tensor([label2id[label] for label in y_train], dtype= torch.long)\n",
        "    y_val = torch.tensor([label2id[label] for label in y_val], dtype= torch.long)\n",
        "    return X_train, y_train, X_val, y_val, label2id, id2label\n",
        "  else:\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "    return X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJxabq4n4zYw",
        "outputId": "bc0176bb-7ef2-4b94-9b20-067e0e45933f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing train data: 100%|██████████| 38650/38650 [00:13<00:00, 2787.59it/s]\n",
            "Preprocessing test data: 100%|██████████| 3861/3861 [00:01<00:00, 2883.79it/s]\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_val, y_val, label2id, id2label = read_data(train_df, config['train_val_ratio'], token2idx, mode='train')\n",
        "X_test, y_test = read_data(test_df, config['train_val_ratio'], token2idx, mode='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4FM96AIK-19O"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    data = (self.x['text_left'][idx],\n",
        "         self.x['text_right'][idx],\n",
        "         self.y[idx]   \n",
        "            )\n",
        "    return data\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.y.size(0)#行数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42aK8fiAagL"
      },
      "source": [
        "```\n",
        "TextCNN中collete_fn函数\n",
        "def collete_fn(examples):\n",
        "    input_ids_list = []\n",
        "    labels =[]\n",
        "    for example in examples:\n",
        "        input_ids_list.append(example['input_ids'])\n",
        "        labels.append(example['label'])\n",
        "\n",
        "    # 对齐操作 -- 找到 input_ids_list 中 最长的 句子， 执行短句子补齐\n",
        "    # 1. 找到 input_ids_list 中 最长的 句子\n",
        "    max_length = max(len(input_ids) for input_ids in input_ids_list) \n",
        "    # 2. 定义一个 input_ids_tensor, 我们要把 每个 input_ids 放入 tensor 中\n",
        "    input_ids_tensor = torch.zeros((len(labels), max_length), dtype=torch.long)\n",
        "    for i, input_ids in enumerate(input_ids_list):\n",
        "        # 得到当前句子的长度\n",
        "        seq_len = len(input_ids)\n",
        "        # 第i个句子，填充 seq_len 这么长\n",
        "        input_ids_tensor[i, :seq_len] = torch.tensor(input_ids, dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'input_ids' : input_ids_tensor,\n",
        "        'labels' : torch.tensor(labels, dtype=torch.long)\n",
        "    }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SngM5VBdATU8"
      },
      "outputs": [],
      "source": [
        "# __call__方法的调用\n",
        "class MyClass():\n",
        "  def __call__(self):\n",
        "    print('__call__方法被调用')\n",
        "    return 'done'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mxbuYVYqBhnk"
      },
      "outputs": [],
      "source": [
        "# 类实例化\n",
        "obj = MyClass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mru9AQZJBlEr",
        "outputId": "bc5942be-eacd-44d4-fadc-f66a7cc6b1e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__call__方法被调用\n"
          ]
        }
      ],
      "source": [
        "# 实例化的对象 当作函数用\n",
        "res = obj()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vPyMg2fQCYzT"
      },
      "outputs": [],
      "source": [
        "# 从 AFQMCDataset 输出的 data  = (sentence1, sentence2, label])\n",
        "# 1. 将元组中属于sentence1的放在一起，属于sentence2的放在一起，属于label的放在一起\n",
        "# 2. 对齐操作，找到sentence1, sentence2,最长的句子，执行短句子补齐\n",
        "# 3. 定义一个tensor，把数据放里面\n",
        "\n",
        "class Collator:\n",
        "  def __init__(self, max_seq_len):\n",
        "    self.max_seq_len = max_seq_len\n",
        "  \n",
        "  def get_max_seq_len(self, ids_list):\n",
        "    cur_max_seq_len = max(len(input_id) for input_id in ids_list)\n",
        "    max_seq_len = min (self.max_seq_len, cur_max_seq_len)\n",
        "    return max_seq_len\n",
        "\n",
        "  #当某个方法不需要用到对象中的任何资源(没有self),将这个方法改为一个静态方法, 加一个@staticmethod。\n",
        "  #加上之后, 这个方法就和普通的函数没有什么区别了, 只不过写在了一个类中, 可以使用这个类的对象调用,\n",
        "  @staticmethod\n",
        "  def pad_and_truncate(text_ids_list, max_seq_len):\n",
        "    input_ids = torch.zeros((len(text_ids_list), max_seq_len),dtype=torch.long)\n",
        "    for i, text_ids in enumerate(text_ids_list):\n",
        "      seq_len = min(len(text_ids), max_seq_len)\n",
        "      input_ids[i, :seq_len] = torch.tensor(text_ids[:seq_len],dtype=torch.long)\n",
        "    return input_ids\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    # 1. 将元组中属于sentence1的放在一起，属于sentence2的放在一起，属于label的放在一起\n",
        "    text_ids_left_list, text_ids_right_list, labels_list = list(zip(*examples))\n",
        "    # 2.1 找到 text_ids_left_list, text_ids_right_list 最长的句子长度\n",
        "    max_text_left_length = self.get_max_seq_len(text_ids_left_list)\n",
        "    max_text_right_length = self.get_max_seq_len(text_ids_right_list)\n",
        "\n",
        "    # 2.2 执行短句子补齐, 3.定义一个tensor，把数据放里面\n",
        "    text_left_ids = self.pad_and_truncate(text_ids_left_list, max_text_left_length)\n",
        "    text_right_ids = self.pad_and_truncate(text_ids_right_list, max_text_right_length)\n",
        "    labels = torch.tensor(labels_list, dtype = torch.long)\n",
        "\n",
        "    data_list = [text_left_ids, text_right_ids, labels]\n",
        "    return data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UckMgCUjF7u5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def build_dataloader(train_df, test_df, config, vocab):\n",
        "  X_train, y_train ,X_val, y_val, label2id, id2label = read_data(train_df, config['train_val_ratio'], vocab, mode='train')\n",
        "  X_test, y_test = read_data(test_df, config['train_val_ratio'], vocab, mode='test')\n",
        "\n",
        "  train_dataset = AFQMCDataset(X_train, y_train)\n",
        "  val_dataset = AFQMCDataset(X_val, y_val)\n",
        "  test_dataset = AFQMCDataset(X_test, y_test)\n",
        "  \n",
        "  collate_fn = Collator(config['max_seq_len'])\n",
        "\n",
        "  train_dataloader = DataLoader(dataset = train_dataset, batch_size = config['batch_size'], num_workers = 4, shuffle = True, collate_fn = collate_fn)\n",
        "  val_dataloader = DataLoader(dataset = val_dataset, batch_size = config['batch_size'], num_workers = 4, shuffle = False, collate_fn = collate_fn)\n",
        "  test_dataloader = DataLoader(dataset = test_dataset, batch_size = config['batch_size'], num_workers = 4, shuffle = False, collate_fn = collate_fn)\n",
        "\n",
        "  return id2label, test_dataloader, val_dataloader, train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvgSVGf-Hxew",
        "outputId": "ee81f1aa-1597-4bac-cb4c-a23f92d3f492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing train data: 100%|██████████| 38650/38650 [00:14<00:00, 2743.76it/s]\n",
            "Preprocessing test data: 100%|██████████| 3861/3861 [00:01<00:00, 2838.65it/s]\n"
          ]
        }
      ],
      "source": [
        "id2label, test_dataloader, val_dataloader, train_dataloader = build_dataloader(train_df, test_df, config, token2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeJ8-aPCtn3y",
        "outputId": "a2ffccdc-e763-41a6-dc9c-e74851b72512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataloader中一个batch数据为（左+右+label）: [tensor([[ 272, 1359,  410,  ...,    0,    0,    0],\n",
            "        [2493, 1448, 1359,  ...,    0,    0,    0],\n",
            "        [ 190, 1412,   22,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [2095,  189, 1226,  ...,    0,    0,    0],\n",
            "        [ 925,  981,  407,  ...,    0,    0,    0],\n",
            "        [2493, 1448, 1359,  ...,    0,    0,    0]]), tensor([[ 410,  272, 1359,  ...,    0,    0,    0],\n",
            "        [2493, 1448, 1359,  ...,    0,    0,    0],\n",
            "        [  22,    5, 1448,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [  22,    5,  272,  ...,    0,    0,    0],\n",
            "        [ 272, 1359,  925,  ...,    0,    0,    0],\n",
            "        [ 410,  130, 1448,  ..., 1090,   21,   69]]), tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0])]\n"
          ]
        }
      ],
      "source": [
        "for i in train_dataloader:\n",
        "  print('dataloader中一个batch数据为（左+右+label）:',i)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzAIHuyr1QwL",
        "outputId": "99bf323b-fc9b-4532-f663-197233c35ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test_item:   0%|          | 0/121 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: [tensor([[2493, 1448, 1359,  ...,    0,    0,    0],\n",
            "        [2493,  272, 1359,  ...,    0,    0,    0],\n",
            "        [ 407,   22,   70,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 272, 1359,  607,  ...,    0,    0,    0],\n",
            "        [   1, 1359,   16,  ...,    0,    0,    0],\n",
            "        [ 272, 1359, 4406,  ...,    0,    0,    0]]), tensor([[1448, 1359,   15,  ...,    0,    0,    0],\n",
            "        [2493,  272, 1359,  ...,    0,    0,    0],\n",
            "        [ 111,   69,  272,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [1090, 1090, 1090,  ...,    0,    0,    0],\n",
            "        [   1,  272, 1359,  ...,  703,   38,    0],\n",
            "        [ 272, 1359,  272,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
            "item: tensor([[2493, 1448, 1359,  ...,    0,    0,    0],\n",
            "        [2493,  272, 1359,  ...,    0,    0,    0],\n",
            "        [ 407,   22,   70,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 272, 1359,  607,  ...,    0,    0,    0],\n",
            "        [   1, 1359,   16,  ...,    0,    0,    0],\n",
            "        [ 272, 1359, 4406,  ...,    0,    0,    0]])\n",
            "item: tensor([[1448, 1359,   15,  ...,    0,    0,    0],\n",
            "        [2493,  272, 1359,  ...,    0,    0,    0],\n",
            "        [ 111,   69,  272,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [1090, 1090, 1090,  ...,    0,    0,    0],\n",
            "        [   1,  272, 1359,  ...,  703,   38,    0],\n",
            "        [ 272, 1359,  272,  ...,    0,    0,    0]])\n",
            "item: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "val_iterator = tqdm(val_dataloader, desc='Test_item', total=len(val_dataloader))\n",
        "for batch in val_iterator:\n",
        "  print('batch:',batch)\n",
        "  i=0\n",
        "  for item in batch:\n",
        "    print('item:',item)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Wgjrz85kt48Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def evaluation(config, model, val_dataloader):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  labels = []\n",
        "  val_loss = 0.\n",
        "  val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
        "  with torch.no_grad():\n",
        "    for batch in val_iterator:\n",
        "      labels.append(batch[-1])\n",
        "      batch = [item.to(config['device']) for item in batch]\n",
        "      loss, logits = model(batch)[:2]\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      # 返回逻辑值最大的位置，要么0，要么1\n",
        "      preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "\n",
        "  avg_val_loss = val_loss / len(val_dataloader)\n",
        "  labels = torch.cat(labels, dim=0).numpy()\n",
        "  preds = torch.cat(preds, dim=0).numpy()\n",
        "  f1 = f1_score(labels, preds, average='macro')\n",
        "\n",
        "  acc = accuracy_score(labels, preds)\n",
        "\n",
        "  return avg_val_loss, f1, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NYMC8N5W4yoK"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from transformers import AdamW\n",
        "\n",
        "def train(model, config, id2label, train_dataloader, val_dataloader):\n",
        "  optimizer = AdamW(model.parameters(), lr = config['learning_rate'])\n",
        "  epoch_iterator = trange(config['num_epochs'])\n",
        "\n",
        "  global_steps = 0\n",
        "  train_loss = 0.\n",
        "  logging_loss = 0.\n",
        "  model.to(config['device'])\n",
        "  \n",
        "  for epoch in epoch_iterator:\n",
        "    train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
        "    model.train()\n",
        "    for batch in train_iterator:\n",
        "\n",
        "      batch =  [item.to(config['device']) for item in batch]\n",
        "      loss = model(batch)[0]\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      global_steps += 1\n",
        "\n",
        "      if global_steps % config['logging_step'] == 0:\n",
        "        print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
        "        logging_loss = train_loss\n",
        "\n",
        "        avg_val_loss, f1, acc = evaluation(config, model, val_dataloader)\n",
        "\n",
        "        print_log = f'>>>traing loss:{print_train_loss: .5f}, valid loss:{avg_val_loss: .5f}, valid f1 score:{f1: .5f}, valid acc:{acc: .5f}'\n",
        "\n",
        "        print(print_log)\n",
        "        model.train()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-Z4-MtoB9qQP"
      },
      "outputs": [],
      "source": [
        "def predict(config, id2label, model, test_dataloader):\n",
        "  test_iterator = tqdm(test_dataloader, desc='Predicting', total=len(test_dataloader))\n",
        "  model.eval()\n",
        "  test_preds = []\n",
        "  with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "      batch = [item.to(config['device']) for item in batch]\n",
        "      logits = model(batch)[1]\n",
        "      test_preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "  test_preds = torch.cat(test_preds, dim = 0).numpy()\n",
        "  test_preds = [id2label[id_] for id_ in test_preds]\n",
        "  return test_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfox0JdZHzKH"
      },
      "source": [
        "# 预备知识"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lekcYK7VHyl4",
        "outputId": "5183588e-c4db-46b3-be08-d637c99e5831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[2., 2., 2., 2.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [4., 4., 4., 4.]],\n",
            "\n",
            "        [[5., 5., 5., 5.],\n",
            "         [6., 6., 6., 6.],\n",
            "         [7., 7., 7., 7.]]])\n",
            "torch.Size([2, 3, 4])\n"
          ]
        }
      ],
      "source": [
        "# 定义一个[2,3,4]的tensor -> [batch_size, seq_len, embedding_dim]  一个batch有两个句子，每个句子3个词，每个词的维度为4\n",
        "a = torch.tensor([[[2., 2., 2., 2.],[3., 3., 3., 3.],[4., 4., 4., 4.]],[[5., 5., 5., 5.],[6., 6., 6., 6.],[7., 7., 7., 7.]]])\n",
        "print(a)\n",
        "print(a.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVxkqiQiItQ1",
        "outputId": "d3d1f25d-06e2-42c1-dfee-b9864a02c770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 4])\n"
          ]
        }
      ],
      "source": [
        "# [2,1,4]的tensor\n",
        "# torch.Tensor是一种包含单一数据类型元素的多维矩阵，torch.Tensor是默认的tensor类型（torch.FlaotTensor）的简称。\n",
        "# torch.ByteTensor 是 CPU tensor, 8-bit integer (unsigned)\n",
        "ones = torch.ByteTensor([[[1, 1, 0, 0]],[[0, 1, 1, 0]]])\n",
        "print(ones.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKyKg9NaLHOE",
        "outputId": "83867f55-d70e-476e-defb-ff0ad8743489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[2., 2., 0., 0.],\n",
            "         [3., 3., 0., 0.],\n",
            "         [4., 4., 0., 0.]],\n",
            "\n",
            "        [[0., 5., 5., 0.],\n",
            "         [0., 6., 6., 0.],\n",
            "         [0., 7., 7., 0.]]])\n",
            "torch.Size([2, 3, 4])\n"
          ]
        }
      ],
      "source": [
        "# 相当于将词向量某些维度清零\n",
        "print(ones * a) # 首先，ones. 从[2,1,4] 变成 [2,3,4] （填充） 再与a中逐个元素相乘\n",
        "print((ones * a).size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie4SqRHVb5Mg"
      },
      "source": [
        "## masked fill 方法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hjnj8kn1b8zA",
        "outputId": "675b96ad-9d31-465e-c097-2d611db2b17c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1],\n",
            "         [1],\n",
            "         [0]],\n",
            "\n",
            "        [[0],\n",
            "         [1],\n",
            "         [1]]], dtype=torch.uint8)\n",
            "torch.Size([2, 3, 1])\n"
          ]
        }
      ],
      "source": [
        "# [2, 3, 1]\n",
        "mask = torch.ByteTensor([[[1],[1],[0]],[[0],[1],[1]]])\n",
        "print(mask)\n",
        "print(mask.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u6XC1WJdbYl"
      },
      "source": [
        "```\n",
        "a:\n",
        "tensor([[[2., 2., 2., 2.],\n",
        "    [3., 3., 3., 3.],\n",
        "    [4., 4., 4., 4.]],\n",
        "\n",
        "    [[5., 5., 5., 5.],\n",
        "    [6., 6., 6., 6.],\n",
        "    [7., 7., 7., 7.]]])\n",
        "\n",
        "mask:\n",
        "tensor([[[1],\n",
        "    [1],\n",
        "    [0]],\n",
        "\n",
        "    [[0],\n",
        "    [1],\n",
        "    [1]]], dtype=torch.uint8)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og5QgPilc6wo",
        "outputId": "24550611-80c0-4125-ccea-65ce80f24eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
            "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
            "         [ 4.0000e+00,  4.0000e+00,  4.0000e+00,  4.0000e+00]],\n",
            "\n",
            "        [[ 5.0000e+00,  5.0000e+00,  5.0000e+00,  5.0000e+00],\n",
            "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
            "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07]]])\n",
            "torch.Size([2, 3, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:41: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ]
        }
      ],
      "source": [
        "# a [2,3,4] mask[2,3,1]\n",
        "# 是将 mask 中 为1 的元素所在的索引，在 a 中 相同索引处替换为 value值\n",
        "# 把某个词向量给mask掉\n",
        "b = a.masked_fill(mask, value = torch.tensor(-1e7))\n",
        "print(b)\n",
        "print(b.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcY4I42-gtdM"
      },
      "source": [
        "``` \n",
        "ones:\n",
        "tensor([[[1, 1, 0, 0]],\n",
        "    [[0, 1, 1, 0]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyOsoRhBfuGm",
        "outputId": "678d09d5-830c-492b-8d6c-395b08f783ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.0000e+07, -1.0000e+07,  2.0000e+00,  2.0000e+00],\n",
            "         [-1.0000e+07, -1.0000e+07,  3.0000e+00,  3.0000e+00],\n",
            "         [-1.0000e+07, -1.0000e+07,  4.0000e+00,  4.0000e+00]],\n",
            "\n",
            "        [[ 5.0000e+00, -1.0000e+07, -1.0000e+07,  5.0000e+00],\n",
            "         [ 6.0000e+00, -1.0000e+07, -1.0000e+07,  6.0000e+00],\n",
            "         [ 7.0000e+00, -1.0000e+07, -1.0000e+07,  7.0000e+00]]])\n",
            "torch.Size([2, 3, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:41: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ]
        }
      ],
      "source": [
        "# 同理，可以用[2, 1, 4] mask掉 [2, 3, 4]\n",
        "# 把 mask 改成 ones\n",
        "mask = ones #[2, 1, 4]\n",
        "b= a.masked_fill(mask, value = torch.tensor(-1e7))\n",
        "print(b)\n",
        "print(b.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxMZp4BGiwQk"
      },
      "source": [
        "# ESIM \n",
        "## Enhanced LSTM for Natural Language Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz6BW8CW2JSx"
      },
      "source": [
        "![ESIM](https://img-blog.csdnimg.cn/img_convert/1adb67ec46e87da23fa042f298ff88bb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8otyCrk2LvS"
      },
      "source": [
        "![ESIM2](https://img-blog.csdnimg.cn/img_convert/6cfe48bd15e9616c0d92eaeebfa50e7b.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0bJsbb72M6a"
      },
      "source": [
        "### 传统的pytorch lstm接口（2层为例）\n",
        "2*hidden_size ： 双向\n",
        "![pytorch LSTM](https://img-blog.csdnimg.cn/5697d3a5bfec44039e4c1b407c4ec924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryXgccE_2VCI"
      },
      "source": [
        "### 论文中的lstm\n",
        "最后拿两个output做拼接\n",
        "\n",
        "![stackRNN](https://img-blog.csdnimg.cn/02b1b24b629c4defabb888776f9d3f57.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNU0xH2E2YyZ"
      },
      "source": [
        "![Local Inference Modeling](https://img-blog.csdnimg.cn/c8afbd13e0da49a080fd8207e31eac8c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Ix3t5R2Zdq"
      },
      "source": [
        "![Local inference collected over sequences](https://img-blog.csdnimg.cn/759a1006a9354deeabdb8e0a7bc5f20a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWY1B3hr2ccK"
      },
      "source": [
        "![Local inference 3](https://img-blog.csdnimg.cn/176f587346994ceaa2d6e0d3393a69b4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmM8KtN_2e4y"
      },
      "source": [
        "![The composition layer](https://img-blog.csdnimg.cn/bb58ab8d53454eb2aaf450b5378bcb74.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-mCkVSw2gTK"
      },
      "source": [
        "![Pooling](https://img-blog.csdnimg.cn/66a661431da8440d9cd0efdb3aeac523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z8U42ps2E3q"
      },
      "source": [
        "![all of gongshi](https://img-blog.csdnimg.cn/652165f9f0584ac683c0df8d412514be.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3eZUHxRBiz58"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    'embedding': embedding_matrix, #torch.Size([5251,300])\n",
        "    'freeze_emb': True,\n",
        "    'hidden_size': 256,\n",
        "    'dropout': 0.3,\n",
        "    'num_layers': 2,\n",
        "    'concat_layers': True,\n",
        "    'rnn_type': 'lstm',\n",
        "    'num_labels': len(id2label)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FYVfzkKA6xX"
      },
      "source": [
        "### 一些参数\n",
        "```\n",
        "B: batch_size\n",
        "L = 'inputs left'  sequence length\n",
        "R = 'inputs right'  sequence length\n",
        "D = embedding size\n",
        "H = hidden size\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NROHeernAMmO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class RNNDropout(nn.Dropout):\n",
        "  # 将词向量 某些维度 清0\n",
        "  # sequences_batch [B, L, D] (和query、doc一样)\n",
        "  def forward(self, sequences_batch):\n",
        "    # ones [B, D]\n",
        "    ones = sequences_batch.data.new_ones(sequences_batch.shape[0], sequences_batch.shape[-1])\n",
        "    # 随机mask ones\n",
        "    # dropout_mask [B, D] ones后面是dropout API的参数  \n",
        "    # p – probability of an element to be zeroed. Default: 0.5\n",
        "    # training – apply dropout if is True. Default: True\n",
        "    # inplace – If set to True, will do this operation in-place[将原地执行此操作]. Default: False\n",
        "    dropout_mask = nn.functional.dropout(ones, self.p, self.training, inplace=False)\n",
        "    # unsqueeze(1)增加一个维度\n",
        "    return dropout_mask.unsqueeze(1) * sequences_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Oi8C7HTgF7Lq"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class StackedBRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers,\n",
        "         dropout_rate = 0, dropout_output = False,\n",
        "         rnn_type = nn.LSTM, concat_layers = False):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.dropout_output = dropout_output\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.num_layers = num_layers\n",
        "    self.concat_layers = concat_layers\n",
        "    self.rnns = nn.ModuleList()\n",
        "    # 共有2层lstm\n",
        "    for i in range(num_layers):\n",
        "      input_size = input_size if i == 0 else 2*hidden_size\n",
        "      self.rnns.append(rnn_type(input_size, hidden_size, num_layers=1, bidirectional=True))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x (B, L, D)[此时不能输入到LSTM中] -> (L, B, D)\n",
        "    x= x.transpose(0, 1)\n",
        "    outputs = [x]\n",
        "    for i in range(self.num_layers):\n",
        "      rnn_input = outputs[-1]\n",
        "\n",
        "      if self.dropout_rate > 0:\n",
        "        rnn_input = F.dropout(rnn_input, p=self.dropout_rate, training = self.training)\n",
        "\n",
        "\n",
        "      # self.rnn[i](rnn_input) (output, (h_n, c_n))\n",
        "      rnn_output = self.rnns[i](rnn_input)[0]\n",
        "      outputs.append(rnn_output)\n",
        "    # outputs [x, output0, output1]\n",
        "    if self.concat_layers:\n",
        "      output = torch.cat(outputs[1:], 2)\n",
        "    else:\n",
        "      output = outputs[-1]\n",
        "    # output (L, B, D) -> (B, L, D)\n",
        "    output = output.transpose(0, 1)\n",
        "\n",
        "    if self.dropout_output and self.dropout_rate > 0:\n",
        "      output = F.dropout(output, p = self.dropout_rate, training = self.training)\n",
        "      \n",
        "    # 进行 transpose之后，tensor在内存中不连续， contiguous将output内存连续\n",
        "    return output.contiguous()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ViVQT1nIpF6Q"
      },
      "outputs": [],
      "source": [
        "class BidirectionalAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # 维度        \n",
        "    # v1 [B, L, H]\n",
        "    # v1_mask [B, L]\n",
        "    # v2 [B, R, H]\n",
        "    # v2_mask [B, R]\n",
        "  def forward(self, v1, v1_mask, v2, v2_mask):\n",
        "    # v2:句子a v1:句子b \n",
        "\n",
        "    # bmm 矩阵相乘\n",
        "    # 计算两个tensor的矩阵乘法，torch.bmm(a,b)\n",
        "    # tensor a 的size为(b,h,w) tensor b的size为(b,w,m), 输出（b,h,m）\n",
        "    # 注意两个tensor的维度必须为3.\n",
        "\n",
        "    # 1.计算矩阵相似度\n",
        "    # similarity_matrix [B, L, R]\n",
        "    similarity_matrix = v1.bmm(v2.transpose(2,1).contiguous())\n",
        "\n",
        "    # 2.计算attention时没有必要计算pad=0, 要进行mask操作 3.进行softmax\n",
        "    # 将similarity_matrix v1中pad对应的权重给mask\n",
        "    # [B, L, R]\n",
        "    # v1_mask.unsqueeze(2) 给v1_mask增加一维\n",
        "    # 在第一维 L 进行softmax\n",
        "    v2_v1_attn = F.softmax(similarity_matrix.masked_fill(\n",
        "          v1_mask.unsqueeze(2),-1e8), dim = 1)\n",
        "      \n",
        "    v1_v2_attn = F.softmax(similarity_matrix.masked_fill(\n",
        "          v2_mask.unsqueeze(1),-1e8), dim = 2)\n",
        "      \n",
        "    # 4.计算attention\n",
        "    # [B, L, R] @ [B, R, H] 矩阵运算\n",
        "    # 句子a 对b的影响 [B, L, H]\n",
        "    # attented_v1 [B, L, H]\n",
        "    attented_v1 = v1_v2_attn.bmm(v2)\n",
        "\n",
        "    # 句子b 对a的影响 \n",
        "    # v2_v1_attn [B, L, R] -> [B, R, L] @[B, L, H] -> [B, R, H]\n",
        "    # attented_v2 [B, R, H]\n",
        "    attented_v2 = v2_v1_attn.transpose(1,2).bmm(v1)\n",
        "\n",
        "    # 使用attented_v1 将v1对应的pad填充为0\n",
        "    # 使用attented_v2 将v2对应的pad填充为0\n",
        "    # v1/2_mask 随意增加一个维度进行mask\n",
        "    attented_v1.masked_fill(v1_mask.unsqueeze(2), 0)\n",
        "    attented_v2.masked_fill(v2_mask.unsqueeze(2), 0)\n",
        "    return attented_v1, attented_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1EDATCZXPJMf"
      },
      "outputs": [],
      "source": [
        "class ESIM(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    # -----------------------   input encoding  ---------------------#\n",
        "    rnn_mapping = {'lstm': nn.LSTM, 'gru': nn.GRU}\n",
        "    self.embedding = nn.Embedding.from_pretrained(config['embedding'], freeze = config['freeze_emb'])\n",
        "    self.rnn_dropout = RNNDropout(p = config['dropout'])\n",
        "    rnn_size = config['hidden_size']\n",
        "\n",
        "    if config['concat_layers']:\n",
        "      # 取整除赋值运算符\tc //= a 等效于 c = c // a\n",
        "      rnn_size //= config['num_layers']\n",
        "    self.input_encoding = StackedBRNN(input_size = config['embedding'].size(1),\n",
        "                      hidden_size = rnn_size // 2,\n",
        "                      num_layers = config['num_layers'],\n",
        "                      rnn_type = rnn_mapping[config['rnn_type']],\n",
        "                      concat_layers = config['concat_layers'])\n",
        "    # -----------------------   input encoding  ---------------------#\n",
        "    # -----------------------   Local inference collected over sequences  ---------------------#\n",
        "    self.attention = BidirectionalAttention()\n",
        "    # -----------------------   Local inference collected over sequences  ---------------------#\n",
        "\n",
        "    # -----------------------   the compositon layer  ---------------------#\n",
        "    self.projection = nn.Sequential(\n",
        "        nn.Linear(4 * config['hidden_size'], config['hidden_size']),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.composition = StackedBRNN(input_size= config['hidden_size'],\n",
        "                   hidden_size= rnn_size // 2,\n",
        "                   num_layers = config['num_layers'],\n",
        "                   rnn_type = rnn_mapping[config['rnn_type']],\n",
        "                   concat_layers = config['concat_layers'])\n",
        "    # -----------------------   the compositon layer  ---------------------#\n",
        "\n",
        "    self.classification = nn.Sequential(\n",
        "        nn.Dropout(p = config['dropout']),\n",
        "        nn.Linear(4 * config['hidden_size'], config['hidden_size']),\n",
        "        nn.Tanh(),\n",
        "        nn.Dropout(p = config['dropout']))\n",
        "    self.out = nn.Linear(config['hidden_size'], config['num_labels'])\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # inputs: [sentence1_tensor, sentence2_tensor, labels_tensor]\n",
        "    # B: batch_size\n",
        "    # L = 'inputs left'  sequence length\n",
        "    # R = 'inputs right'  sequence length\n",
        "    # D = embedding size\n",
        "    # H = hidden size \n",
        "\n",
        "    # -----------------------   input encoding  ---------------------#\n",
        "    # query: sentence1_tensor\n",
        "    # doc: sentence2_tensor\n",
        "   \n",
        "    # query [B, L]\n",
        "    # doc [B, R]   \n",
        "    query, doc = inputs[0].long(), inputs[1].long()\n",
        "\n",
        "    # 判断 query，doc中的每一个数是不是0， 是1则表示该位置是pad\n",
        "    # query：[2,3,4,5,0,0,0] -> query_mask：[0,0,0,0,1,1,1]\n",
        "    # query_mask [B, L]\n",
        "    # doc_mask [B, R]\n",
        "    query_mask = (query == 0)\n",
        "    doc_mask = (doc == 0)\n",
        "\n",
        "    # query [B, L, D]\n",
        "    # doc [B, R, D]\n",
        "    query = self.embedding(query)\n",
        "    doc = self.embedding(doc)\n",
        "\n",
        "    # query [B, L, D]\n",
        "    # doc [B, R, D]\n",
        "    query = self.rnn_dropout(query)\n",
        "    doc = self.rnn_dropout(doc)\n",
        "\n",
        "    # query [B, L, H]\n",
        "    # doc [B, R, H]\n",
        "    query = self.input_encoding(query)\n",
        "    doc = self.input_encoding(doc)\n",
        "    # -----------------------   input encoding  ---------------------#\n",
        "\n",
        "    # 1.计算矩阵相似度\n",
        "    # 2.计算attention时没有必要计算pad=0（补齐时，填充为0的单元）, 要进行mask操作\n",
        "    # 3.进行softmax\n",
        "    # 4.计算attention\n",
        "    # -----------------------   Local inference collected over sequences  ---------------------#\n",
        "    # query [B, L, H]\n",
        "    # query_mask [B, L]\n",
        "    # doc [B, R, H]\n",
        "    # doc_mask [B, R]\n",
        "    attended_query, attended_doc = self.attention(query, query_mask, doc, doc_mask)\n",
        "\n",
        "    # -----------------------   Local inference collected over sequences  ---------------------#\n",
        "    \n",
        "    # -----------------------  Enhancement of local inference information ---------------------#\n",
        "    # enhanced_query [B, L, 4*h] (在最后一层连接的)\n",
        "    # enhanced_doc [B, R, 4*h]\n",
        "    # dim = -1 按倒数第一维cat\n",
        "    enhanced_query = torch.cat([query,attended_query,\n",
        "                   query-attended_query,\n",
        "                   query*attended_query],dim = -1)\n",
        "    enhanced_doc = torch.cat([doc,attended_doc,\n",
        "                  doc-attended_doc,\n",
        "                  doc*attended_doc],dim = -1)\n",
        "    \n",
        "    # -----------------------  Enhancement of local inference information ---------------------#\n",
        "\n",
        "    # -----------------------   the compositon layer  ---------------------#\n",
        "    # projected_query [B, L, H]\n",
        "    # projected_doc [B, R, H]\n",
        "    projected_query = self.projection(enhanced_query)\n",
        "    projected_doc = self.projection(enhanced_doc)\n",
        "\n",
        "    # query [B, L, H]\n",
        "    # doc [B, R, H]\n",
        "    query = self.composition(projected_query)\n",
        "    doc = self.composition(projected_doc)\n",
        "    # -----------------------   the compositon layer  ---------------------#\n",
        "\n",
        "    # -----------------------   Pooling  ---------------------#\n",
        "    # query_mask， doc_mask. 判断 query，doc中的每一个数是不是0， 是1则表示该位置是pad\n",
        "    # reverse_query_mask 0的位置代表pad\n",
        "    # reverse_query_mask [B, L]\n",
        "    # reverse_doc_mask [B, R]\n",
        "    reverse_query_mask = 1. - query_mask.float()\n",
        "    reverse_doc_mask = 1. - doc_mask.float()\n",
        "\n",
        "\n",
        "    # torch.sum(input, dim, keepdim=False, *, dtype=None) → Tensor\n",
        "    # Returns the sum of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them.\n",
        "    # If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).\n",
        "    # 分子中 以dim=1 即 L 维度 相加\n",
        "    query_avg = torch.sum(query * reverse_query_mask.unsqueeze(2),dim = 1) / (torch.sum(reverse_query_mask, dim = 1, keepdim= True) + 1e-8)\n",
        "    doc_avg = torch.sum(doc * reverse_doc_mask.unsqueeze(2),dim = 1) / (torch.sum(reverse_doc_mask, dim = 1, keepdim= True) + 1e-8)\n",
        "       \n",
        "    # 防止取出pad\n",
        "    query =query.masked_fill(query_mask.unsqueeze(2), -1e7)\n",
        "    doc = doc.masked_fill(doc_mask.unsqueeze(2), -1e7)\n",
        "\n",
        "    # _ 是最大值的位置\n",
        "    query_max, _ = query.max(dim = 1)\n",
        "    doc_max, _ = doc.max(dim = 1)\n",
        "\n",
        "    # v [B, 4*H]\n",
        "    v = torch.cat([query_avg, query_max, doc_avg, doc_max], dim = -1)\n",
        "    # -----------------------   Pooling  ---------------------#\n",
        "    \n",
        "\n",
        "    # -----------------------   prediction  ---------------------#\n",
        "    # hidden [B, H]\n",
        "    hidden = self.classification(v)\n",
        "\n",
        "    out = self.out(hidden)\n",
        "    outputs = (out, )\n",
        "    # -----------------------   prediction  ---------------------#\n",
        "\n",
        "    if len(inputs) == 3:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(out, inputs[-1])\n",
        "      outputs = (loss, )+ outputs\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHIfOrgSZ3Kw"
      },
      "source": [
        "### torch.sum\n",
        "```\n",
        "torch.sum(input, dim, keepdim=False, *, dtype=None) → Tensor\n",
        ">>> a = torch.randn(4, 4)\n",
        ">>> a\n",
        "    tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n",
        "        [-0.2993,  0.9138,  0.9337, -1.6864],\n",
        "        [ 0.1132,  0.7892, -0.1003,  0.5688],\n",
        "        [ 0.3637, -0.9906, -0.4752, -1.5197]])\n",
        ">>> torch.sum(a, 1)\n",
        "tensor([-0.4598, -0.1381,  1.3708, -2.6217])\n",
        "-0.4598 为第一行加起来\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Y8xsFB3Qep6F"
      },
      "outputs": [],
      "source": [
        "model = ESIM(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfpmmqe3fLXe",
        "outputId": "af8a5444-9036-4b81-e6cb-d1e97cc90e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Training:   0%|          | 0/484 [00:00<?, ?it/s]\u001b[A\n",
            "Training:   0%|          | 1/484 [00:00<02:01,  3.96it/s]\u001b[A\n",
            "Training:   1%|          | 3/484 [00:00<01:07,  7.15it/s]\u001b[A\n",
            "Training:   1%|          | 5/484 [00:00<00:54,  8.83it/s]\u001b[A\n",
            "Training:   1%|▏         | 7/484 [00:00<00:46, 10.34it/s]\u001b[A\n",
            "Training:   2%|▏         | 9/484 [00:00<00:43, 11.03it/s]\u001b[A\n",
            "Training:   2%|▏         | 11/484 [00:01<00:41, 11.29it/s]\u001b[A\n",
            "Training:   3%|▎         | 13/484 [00:01<00:40, 11.63it/s]\u001b[A\n",
            "Training:   3%|▎         | 15/484 [00:01<00:38, 12.04it/s]\u001b[A\n",
            "Training:   4%|▎         | 17/484 [00:01<00:40, 11.64it/s]\u001b[A\n",
            "Training:   4%|▍         | 19/484 [00:01<00:41, 11.13it/s]\u001b[A\n",
            "Training:   4%|▍         | 21/484 [00:02<00:46, 10.04it/s]\u001b[A\n",
            "Training:   5%|▍         | 23/484 [00:02<00:42, 10.85it/s]\u001b[A\n",
            "Training:   5%|▌         | 25/484 [00:02<00:38, 12.00it/s]\u001b[A\n",
            "Training:   6%|▌         | 27/484 [00:02<00:38, 11.83it/s]\u001b[A\n",
            "Training:   6%|▌         | 29/484 [00:02<00:40, 11.27it/s]\u001b[A\n",
            "Training:   6%|▋         | 31/484 [00:02<00:39, 11.61it/s]\u001b[A\n",
            "Training:   7%|▋         | 33/484 [00:03<00:36, 12.22it/s]\u001b[A\n",
            "Training:   7%|▋         | 35/484 [00:03<00:36, 12.43it/s]\u001b[A\n",
            "Training:   8%|▊         | 37/484 [00:03<00:34, 12.89it/s]\u001b[A\n",
            "Training:   8%|▊         | 39/484 [00:03<00:33, 13.29it/s]\u001b[A\n",
            "Training:   8%|▊         | 41/484 [00:03<00:35, 12.64it/s]\u001b[A\n",
            "Training:   9%|▉         | 43/484 [00:03<00:33, 13.32it/s]\u001b[A\n",
            "Training:   9%|▉         | 45/484 [00:03<00:34, 12.73it/s]\u001b[A\n",
            "Training:  10%|▉         | 47/484 [00:04<00:35, 12.32it/s]\u001b[A\n",
            "Training:  10%|█         | 49/484 [00:04<00:33, 12.95it/s]\u001b[A\n",
            "Training:  11%|█         | 51/484 [00:04<00:32, 13.47it/s]\u001b[A\n",
            "Training:  11%|█         | 53/484 [00:04<00:32, 13.34it/s]\u001b[A\n",
            "Training:  11%|█▏        | 55/484 [00:04<00:32, 13.33it/s]\u001b[A\n",
            "Training:  12%|█▏        | 57/484 [00:04<00:32, 13.27it/s]\u001b[A\n",
            "Training:  12%|█▏        | 59/484 [00:04<00:32, 13.14it/s]\u001b[A\n",
            "Training:  13%|█▎        | 61/484 [00:05<00:32, 13.08it/s]\u001b[A\n",
            "Training:  13%|█▎        | 63/484 [00:05<00:31, 13.31it/s]\u001b[A\n",
            "Training:  13%|█▎        | 65/484 [00:05<00:31, 13.12it/s]\u001b[A\n",
            "Training:  14%|█▍        | 67/484 [00:05<00:33, 12.60it/s]\u001b[A\n",
            "Training:  14%|█▍        | 69/484 [00:05<00:32, 12.87it/s]\u001b[A\n",
            "Training:  15%|█▍        | 71/484 [00:05<00:31, 13.01it/s]\u001b[A\n",
            "Training:  15%|█▌        | 73/484 [00:06<00:31, 13.03it/s]\u001b[A\n",
            "Training:  15%|█▌        | 75/484 [00:06<00:30, 13.44it/s]\u001b[A\n",
            "Training:  16%|█▌        | 77/484 [00:06<00:29, 13.71it/s]\u001b[A\n",
            "Training:  16%|█▋        | 79/484 [00:06<00:30, 13.12it/s]\u001b[A\n",
            "Training:  17%|█▋        | 81/484 [00:06<00:33, 12.10it/s]\u001b[A\n",
            "Training:  17%|█▋        | 83/484 [00:06<00:38, 10.32it/s]\u001b[A\n",
            "Training:  18%|█▊        | 85/484 [00:07<00:37, 10.52it/s]\u001b[A\n",
            "Training:  18%|█▊        | 87/484 [00:07<00:36, 10.80it/s]\u001b[A\n",
            "Training:  18%|█▊        | 89/484 [00:07<00:35, 11.11it/s]\u001b[A\n",
            "Training:  19%|█▉        | 91/484 [00:07<00:38, 10.09it/s]\u001b[A\n",
            "Training:  19%|█▉        | 93/484 [00:08<00:43,  8.90it/s]\u001b[A\n",
            "Training:  20%|█▉        | 95/484 [00:08<00:40,  9.57it/s]\u001b[A\n",
            "Training:  20%|██        | 97/484 [00:08<00:36, 10.62it/s]\u001b[A\n",
            "Training:  20%|██        | 99/484 [00:08<00:32, 11.72it/s]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/121 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|          | 1/121 [00:00<00:55,  2.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   2%|▏         | 3/121 [00:00<00:19,  6.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   5%|▍         | 6/121 [00:00<00:10, 11.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   8%|▊         | 10/121 [00:00<00:06, 17.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 14/121 [00:00<00:04, 21.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 18/121 [00:01<00:03, 25.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 22/121 [00:01<00:03, 29.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██▏       | 26/121 [00:01<00:03, 29.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▍       | 30/121 [00:01<00:02, 31.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 34/121 [00:01<00:02, 33.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 39/121 [00:01<00:02, 35.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  36%|███▌      | 43/121 [00:01<00:02, 35.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  39%|███▉      | 47/121 [00:01<00:02, 35.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  42%|████▏     | 51/121 [00:01<00:02, 34.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  45%|████▌     | 55/121 [00:02<00:01, 33.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▉     | 59/121 [00:02<00:01, 34.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  52%|█████▏    | 63/121 [00:02<00:01, 33.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  55%|█████▌    | 67/121 [00:02<00:01, 34.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▊    | 71/121 [00:02<00:01, 34.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 75/121 [00:02<00:01, 32.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▌   | 79/121 [00:02<00:01, 33.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▊   | 83/121 [00:02<00:01, 33.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 87/121 [00:03<00:00, 34.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 91/121 [00:03<00:00, 35.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▊  | 95/121 [00:03<00:00, 35.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 99/121 [00:03<00:00, 33.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 103/121 [00:03<00:00, 32.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 107/121 [00:03<00:00, 31.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  92%|█████████▏| 111/121 [00:03<00:00, 32.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  95%|█████████▌| 115/121 [00:03<00:00, 33.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 121/121 [00:04<00:00, 29.18it/s]\n",
            "\n",
            "Training:  21%|██        | 101/484 [00:12<04:30,  1.41it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>traing loss: 0.63075, valid loss: 0.64200, valid f1 score: 0.39919, valid acc: 0.66442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Training:  21%|██▏       | 103/484 [00:12<03:16,  1.94it/s]\u001b[A\n",
            "Training:  22%|██▏       | 105/484 [00:13<02:26,  2.59it/s]\u001b[A\n",
            "Training:  22%|██▏       | 107/484 [00:13<01:50,  3.41it/s]\u001b[A\n",
            "Training:  23%|██▎       | 109/484 [00:13<01:25,  4.41it/s]\u001b[A\n",
            "Training:  23%|██▎       | 111/484 [00:13<01:06,  5.60it/s]\u001b[A\n",
            "Training:  23%|██▎       | 113/484 [00:13<00:55,  6.70it/s]\u001b[A\n",
            "Training:  24%|██▍       | 115/484 [00:13<00:46,  7.85it/s]\u001b[A\n",
            "Training:  24%|██▍       | 117/484 [00:13<00:40,  8.95it/s]\u001b[A\n",
            "Training:  25%|██▍       | 119/484 [00:14<00:37,  9.86it/s]\u001b[A\n",
            "Training:  25%|██▌       | 121/484 [00:14<00:33, 10.83it/s]\u001b[A\n",
            "Training:  25%|██▌       | 123/484 [00:14<00:31, 11.51it/s]\u001b[A\n",
            "Training:  26%|██▌       | 125/484 [00:14<00:30, 11.71it/s]\u001b[A\n",
            "Training:  26%|██▌       | 127/484 [00:14<00:29, 12.00it/s]\u001b[A\n",
            "Training:  27%|██▋       | 129/484 [00:14<00:27, 13.00it/s]\u001b[A\n",
            "Training:  27%|██▋       | 131/484 [00:15<00:26, 13.11it/s]\u001b[A\n",
            "Training:  27%|██▋       | 133/484 [00:15<00:25, 13.73it/s]\u001b[A\n",
            "Training:  28%|██▊       | 135/484 [00:15<00:25, 13.68it/s]\u001b[A\n",
            "Training:  28%|██▊       | 137/484 [00:15<00:25, 13.64it/s]\u001b[A\n",
            "Training:  29%|██▊       | 139/484 [00:15<00:25, 13.40it/s]\u001b[A\n",
            "Training:  29%|██▉       | 141/484 [00:15<00:24, 13.73it/s]\u001b[A\n",
            "Training:  30%|██▉       | 143/484 [00:15<00:25, 13.53it/s]\u001b[A\n",
            "Training:  30%|██▉       | 145/484 [00:16<00:25, 13.49it/s]\u001b[A\n",
            "Training:  30%|███       | 147/484 [00:16<00:24, 13.85it/s]\u001b[A\n",
            "Training:  31%|███       | 149/484 [00:16<00:23, 14.08it/s]\u001b[A\n",
            "Training:  31%|███       | 151/484 [00:16<00:22, 14.95it/s]\u001b[A\n",
            "Training:  32%|███▏      | 153/484 [00:16<00:24, 13.27it/s]\u001b[A\n",
            "Training:  32%|███▏      | 155/484 [00:16<00:27, 11.78it/s]\u001b[A\n",
            "Training:  32%|███▏      | 157/484 [00:16<00:28, 11.51it/s]\u001b[A\n",
            "Training:  33%|███▎      | 159/484 [00:17<00:26, 12.47it/s]\u001b[A\n",
            "Training:  33%|███▎      | 161/484 [00:17<00:25, 12.75it/s]\u001b[A\n",
            "Training:  34%|███▎      | 163/484 [00:17<00:24, 13.07it/s]\u001b[A\n",
            "Training:  34%|███▍      | 165/484 [00:17<00:25, 12.61it/s]\u001b[A\n",
            "Training:  35%|███▍      | 167/484 [00:17<00:25, 12.25it/s]\u001b[A\n",
            "Training:  35%|███▍      | 169/484 [00:17<00:26, 11.87it/s]\u001b[A\n",
            "Training:  35%|███▌      | 171/484 [00:18<00:25, 12.33it/s]\u001b[A\n",
            "Training:  36%|███▌      | 173/484 [00:18<00:25, 12.17it/s]\u001b[A\n",
            "Training:  36%|███▌      | 175/484 [00:18<00:24, 12.83it/s]\u001b[A\n",
            "Training:  37%|███▋      | 177/484 [00:18<00:25, 12.19it/s]\u001b[A\n",
            "Training:  37%|███▋      | 179/484 [00:18<00:22, 13.50it/s]\u001b[A\n",
            "Training:  37%|███▋      | 181/484 [00:18<00:23, 12.72it/s]\u001b[A\n",
            "Training:  38%|███▊      | 183/484 [00:19<00:22, 13.39it/s]\u001b[A\n",
            "Training:  38%|███▊      | 185/484 [00:19<00:21, 13.98it/s]\u001b[A\n",
            "Training:  39%|███▊      | 187/484 [00:19<00:20, 14.23it/s]\u001b[A\n",
            "Training:  39%|███▉      | 189/484 [00:19<00:21, 13.80it/s]\u001b[A\n",
            "Training:  39%|███▉      | 191/484 [00:19<00:20, 14.09it/s]\u001b[A\n",
            "Training:  40%|███▉      | 193/484 [00:19<00:20, 13.86it/s]\u001b[A\n",
            "Training:  40%|████      | 195/484 [00:19<00:20, 13.87it/s]\u001b[A\n",
            "Training:  41%|████      | 197/484 [00:19<00:19, 14.35it/s]\u001b[A\n",
            "Training:  41%|████      | 199/484 [00:20<00:20, 14.07it/s]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/121 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|          | 1/121 [00:00<00:22,  5.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 5/121 [00:00<00:06, 17.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 9/121 [00:00<00:04, 24.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  11%|█         | 13/121 [00:00<00:03, 29.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  14%|█▍        | 17/121 [00:00<00:03, 31.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  17%|█▋        | 21/121 [00:00<00:03, 32.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 25/121 [00:00<00:03, 31.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▍       | 29/121 [00:01<00:02, 32.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  27%|██▋       | 33/121 [00:01<00:02, 34.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███▏      | 38/121 [00:01<00:02, 37.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▍      | 42/121 [00:01<00:02, 36.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 46/121 [00:01<00:01, 37.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████▏     | 50/121 [00:01<00:01, 37.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  45%|████▍     | 54/121 [00:01<00:01, 37.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  48%|████▊     | 58/121 [00:01<00:01, 37.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████     | 62/121 [00:01<00:01, 35.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  55%|█████▍    | 66/121 [00:01<00:01, 37.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  58%|█████▊    | 70/121 [00:02<00:01, 37.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  61%|██████    | 74/121 [00:02<00:01, 36.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  64%|██████▍   | 78/121 [00:02<00:01, 36.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 82/121 [00:02<00:01, 35.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 86/121 [00:02<00:00, 35.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 91/121 [00:02<00:00, 37.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▊  | 95/121 [00:02<00:00, 37.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 99/121 [00:02<00:00, 36.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 103/121 [00:03<00:00, 36.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 107/121 [00:03<00:00, 35.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  92%|█████████▏| 111/121 [00:03<00:00, 35.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  95%|█████████▌| 115/121 [00:03<00:00, 35.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 121/121 [00:03<00:00, 33.58it/s]\n",
            "\n",
            "Training:  42%|████▏     | 201/484 [00:23<02:54,  1.62it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>traing loss: 0.61434, valid loss: 0.64106, valid f1 score: 0.39919, valid acc: 0.66442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Training:  42%|████▏     | 203/484 [00:24<02:07,  2.20it/s]\u001b[A\n",
            "Training:  42%|████▏     | 205/484 [00:24<01:33,  2.98it/s]\u001b[A\n",
            "Training:  43%|████▎     | 207/484 [00:24<01:11,  3.88it/s]\u001b[A\n",
            "Training:  43%|████▎     | 209/484 [00:24<00:54,  5.00it/s]\u001b[A\n",
            "Training:  44%|████▎     | 211/484 [00:24<00:43,  6.30it/s]\u001b[A\n",
            "Training:  44%|████▍     | 213/484 [00:24<00:36,  7.47it/s]\u001b[A\n",
            "Training:  44%|████▍     | 215/484 [00:24<00:30,  8.71it/s]\u001b[A\n",
            "Training:  45%|████▍     | 217/484 [00:24<00:25, 10.34it/s]\u001b[A\n",
            "Training:  45%|████▌     | 219/484 [00:25<00:23, 11.35it/s]\u001b[A\n",
            "Training:  46%|████▌     | 221/484 [00:25<00:21, 12.00it/s]\u001b[A\n",
            "Training:  46%|████▌     | 223/484 [00:25<00:20, 12.69it/s]\u001b[A\n",
            "Training:  46%|████▋     | 225/484 [00:25<00:20, 12.88it/s]\u001b[A\n",
            "Training:  47%|████▋     | 227/484 [00:25<00:19, 13.42it/s]\u001b[A\n",
            "Training:  47%|████▋     | 229/484 [00:25<00:18, 13.56it/s]\u001b[A\n",
            "Training:  48%|████▊     | 231/484 [00:25<00:19, 12.96it/s]\u001b[A\n",
            "Training:  48%|████▊     | 233/484 [00:26<00:18, 13.74it/s]\u001b[A\n",
            "Training:  49%|████▊     | 235/484 [00:26<00:18, 13.55it/s]\u001b[A\n",
            "Training:  49%|████▉     | 237/484 [00:26<00:17, 14.18it/s]\u001b[A\n",
            "Training:  49%|████▉     | 239/484 [00:26<00:17, 13.88it/s]\u001b[A\n",
            "Training:  50%|████▉     | 241/484 [00:26<00:17, 13.78it/s]\u001b[A\n",
            "Training:  50%|█████     | 243/484 [00:26<00:17, 13.63it/s]\u001b[A\n",
            "Training:  51%|█████     | 245/484 [00:26<00:16, 14.44it/s]\u001b[A\n",
            "Training:  51%|█████     | 247/484 [00:27<00:15, 15.09it/s]\u001b[A\n",
            "Training:  51%|█████▏    | 249/484 [00:27<00:15, 15.48it/s]\u001b[A\n",
            "Training:  52%|█████▏    | 251/484 [00:27<00:15, 15.28it/s]\u001b[A\n",
            "Training:  52%|█████▏    | 253/484 [00:27<00:15, 15.17it/s]\u001b[A\n",
            "Training:  53%|█████▎    | 255/484 [00:27<00:15, 14.63it/s]\u001b[A\n",
            "Training:  53%|█████▎    | 257/484 [00:27<00:15, 14.84it/s]\u001b[A\n",
            "Training:  54%|█████▎    | 259/484 [00:27<00:14, 15.19it/s]\u001b[A\n",
            "Training:  54%|█████▍    | 261/484 [00:28<00:15, 14.57it/s]\u001b[A\n",
            "Training:  54%|█████▍    | 263/484 [00:28<00:15, 14.35it/s]\u001b[A\n",
            "Training:  55%|█████▍    | 265/484 [00:28<00:15, 14.42it/s]\u001b[A\n",
            "Training:  55%|█████▌    | 267/484 [00:28<00:17, 12.37it/s]\u001b[A\n",
            "Training:  56%|█████▌    | 269/484 [00:28<00:17, 12.25it/s]\u001b[A\n",
            "Training:  56%|█████▌    | 271/484 [00:28<00:17, 12.49it/s]\u001b[A\n",
            "Training:  56%|█████▋    | 273/484 [00:28<00:16, 12.94it/s]\u001b[A\n",
            "Training:  57%|█████▋    | 275/484 [00:29<00:15, 13.58it/s]\u001b[A\n",
            "Training:  57%|█████▋    | 277/484 [00:29<00:14, 14.45it/s]\u001b[A\n",
            "Training:  58%|█████▊    | 279/484 [00:29<00:14, 14.26it/s]\u001b[A\n",
            "Training:  58%|█████▊    | 281/484 [00:29<00:13, 14.86it/s]\u001b[A\n",
            "Training:  58%|█████▊    | 283/484 [00:29<00:13, 15.09it/s]\u001b[A\n",
            "Training:  59%|█████▉    | 285/484 [00:29<00:13, 14.53it/s]\u001b[A\n",
            "Training:  59%|█████▉    | 287/484 [00:29<00:13, 14.46it/s]\u001b[A\n",
            "Training:  60%|█████▉    | 289/484 [00:30<00:14, 13.89it/s]\u001b[A\n",
            "Training:  60%|██████    | 291/484 [00:30<00:13, 13.87it/s]\u001b[A\n",
            "Training:  61%|██████    | 293/484 [00:30<00:13, 14.33it/s]\u001b[A\n",
            "Training:  61%|██████    | 295/484 [00:30<00:12, 14.63it/s]\u001b[A\n",
            "Training:  61%|██████▏   | 297/484 [00:30<00:13, 13.82it/s]\u001b[A\n",
            "Training:  62%|██████▏   | 299/484 [00:30<00:13, 14.00it/s]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/121 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|          | 1/121 [00:00<00:21,  5.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 5/121 [00:00<00:06, 18.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 9/121 [00:00<00:04, 24.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  11%|█         | 13/121 [00:00<00:03, 28.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  14%|█▍        | 17/121 [00:00<00:03, 30.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  17%|█▋        | 21/121 [00:00<00:03, 32.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 25/121 [00:00<00:03, 31.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▍       | 29/121 [00:01<00:02, 33.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  27%|██▋       | 33/121 [00:01<00:02, 35.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 37/121 [00:01<00:02, 36.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 41/121 [00:01<00:02, 35.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 46/121 [00:01<00:02, 37.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████▏     | 50/121 [00:01<00:01, 37.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  45%|████▍     | 54/121 [00:01<00:01, 35.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  48%|████▊     | 58/121 [00:01<00:01, 36.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████     | 62/121 [00:01<00:01, 35.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  55%|█████▍    | 66/121 [00:02<00:01, 35.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  58%|█████▊    | 70/121 [00:02<00:01, 35.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  61%|██████    | 74/121 [00:02<00:01, 34.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  64%|██████▍   | 78/121 [00:02<00:01, 35.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 82/121 [00:02<00:01, 35.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 86/121 [00:02<00:01, 35.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 91/121 [00:02<00:00, 37.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▊  | 95/121 [00:02<00:00, 36.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 99/121 [00:02<00:00, 34.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 103/121 [00:03<00:00, 35.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 107/121 [00:03<00:00, 34.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  92%|█████████▏| 111/121 [00:03<00:00, 34.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  95%|█████████▌| 115/121 [00:03<00:00, 35.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 121/121 [00:03<00:00, 33.02it/s]\n",
            "\n",
            "Training:  62%|██████▏   | 301/484 [00:34<01:54,  1.60it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>traing loss: 0.62116, valid loss: 0.64125, valid f1 score: 0.39919, valid acc: 0.66442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Training:  63%|██████▎   | 303/484 [00:34<01:23,  2.16it/s]\u001b[A\n",
            "Training:  63%|██████▎   | 305/484 [00:34<01:02,  2.84it/s]\u001b[A\n",
            "Training:  63%|██████▎   | 307/484 [00:35<00:47,  3.72it/s]\u001b[A\n",
            "Training:  64%|██████▍   | 309/484 [00:35<00:36,  4.83it/s]\u001b[A\n",
            "Training:  64%|██████▍   | 311/484 [00:35<00:28,  6.13it/s]\u001b[A\n",
            "Training:  65%|██████▍   | 313/484 [00:35<00:22,  7.50it/s]\u001b[A\n",
            "Training:  65%|██████▌   | 315/484 [00:35<00:19,  8.67it/s]\u001b[A\n",
            "Training:  65%|██████▌   | 317/484 [00:35<00:17,  9.82it/s]\u001b[A\n",
            "Training:  66%|██████▌   | 319/484 [00:35<00:15, 10.69it/s]\u001b[A\n",
            "Training:  66%|██████▋   | 321/484 [00:36<00:13, 11.65it/s]\u001b[A\n",
            "Training:  67%|██████▋   | 323/484 [00:36<00:13, 11.60it/s]\u001b[A\n",
            "Training:  67%|██████▋   | 325/484 [00:36<00:12, 12.61it/s]\u001b[A\n",
            "Training:  68%|██████▊   | 327/484 [00:36<00:11, 13.37it/s]\u001b[A\n",
            "Training:  68%|██████▊   | 329/484 [00:36<00:11, 13.69it/s]\u001b[A\n",
            "Training:  68%|██████▊   | 331/484 [00:36<00:10, 14.11it/s]\u001b[A\n",
            "Training:  69%|██████▉   | 333/484 [00:36<00:11, 13.68it/s]\u001b[A\n",
            "Training:  69%|██████▉   | 335/484 [00:37<00:10, 14.46it/s]\u001b[A\n",
            "Training:  70%|██████▉   | 337/484 [00:37<00:09, 14.99it/s]\u001b[A\n",
            "Training:  70%|███████   | 339/484 [00:37<00:10, 14.33it/s]\u001b[A\n",
            "Training:  70%|███████   | 341/484 [00:37<00:10, 13.40it/s]\u001b[A\n",
            "Training:  71%|███████   | 343/484 [00:37<00:10, 13.68it/s]\u001b[A\n",
            "Training:  71%|███████▏  | 345/484 [00:37<00:09, 14.25it/s]\u001b[A\n",
            "Training:  72%|███████▏  | 347/484 [00:37<00:09, 14.45it/s]\u001b[A\n",
            "Training:  72%|███████▏  | 349/484 [00:38<00:09, 14.25it/s]\u001b[A\n",
            "Training:  73%|███████▎  | 351/484 [00:38<00:09, 14.56it/s]\u001b[A\n",
            "Training:  73%|███████▎  | 353/484 [00:38<00:08, 14.86it/s]\u001b[A\n",
            "Training:  73%|███████▎  | 355/484 [00:38<00:09, 14.25it/s]\u001b[A\n",
            "Training:  74%|███████▍  | 357/484 [00:38<00:09, 13.79it/s]\u001b[A\n",
            "Training:  74%|███████▍  | 359/484 [00:38<00:09, 13.59it/s]\u001b[A\n",
            "Training:  75%|███████▍  | 361/484 [00:38<00:08, 13.69it/s]\u001b[A\n",
            "Training:  75%|███████▌  | 363/484 [00:39<00:08, 13.77it/s]\u001b[A\n",
            "Training:  75%|███████▌  | 365/484 [00:39<00:08, 13.78it/s]\u001b[A\n",
            "Training:  76%|███████▌  | 367/484 [00:39<00:08, 13.16it/s]\u001b[A\n",
            "Training:  76%|███████▌  | 369/484 [00:39<00:08, 13.81it/s]\u001b[A\n",
            "Training:  77%|███████▋  | 371/484 [00:39<00:08, 14.00it/s]\u001b[A\n",
            "Training:  77%|███████▋  | 373/484 [00:39<00:08, 13.50it/s]\u001b[A\n",
            "Training:  77%|███████▋  | 375/484 [00:39<00:07, 14.05it/s]\u001b[A\n",
            "Training:  78%|███████▊  | 377/484 [00:40<00:07, 14.31it/s]\u001b[A\n",
            "Training:  78%|███████▊  | 379/484 [00:40<00:07, 14.97it/s]\u001b[A\n",
            "Training:  79%|███████▊  | 381/484 [00:40<00:06, 14.77it/s]\u001b[A\n",
            "Training:  79%|███████▉  | 383/484 [00:40<00:07, 14.25it/s]\u001b[A\n",
            "Training:  80%|███████▉  | 385/484 [00:40<00:06, 14.85it/s]\u001b[A\n",
            "Training:  80%|███████▉  | 387/484 [00:40<00:06, 13.97it/s]\u001b[A\n",
            "Training:  80%|████████  | 389/484 [00:40<00:06, 14.14it/s]\u001b[A\n",
            "Training:  81%|████████  | 391/484 [00:41<00:06, 14.28it/s]\u001b[A\n",
            "Training:  81%|████████  | 393/484 [00:41<00:06, 14.96it/s]\u001b[A\n",
            "Training:  82%|████████▏ | 395/484 [00:41<00:05, 15.07it/s]\u001b[A\n",
            "Training:  82%|████████▏ | 397/484 [00:41<00:05, 15.88it/s]\u001b[A\n",
            "Training:  82%|████████▏ | 399/484 [00:41<00:05, 15.04it/s]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/121 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|          | 1/121 [00:00<00:25,  4.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 5/121 [00:00<00:06, 16.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 8/121 [00:00<00:05, 20.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  11%|█         | 13/121 [00:00<00:03, 28.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  14%|█▍        | 17/121 [00:00<00:03, 30.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  17%|█▋        | 21/121 [00:00<00:03, 32.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 25/121 [00:00<00:03, 31.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▍       | 29/121 [00:01<00:02, 33.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  27%|██▋       | 33/121 [00:01<00:02, 34.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 37/121 [00:01<00:02, 35.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 41/121 [00:01<00:02, 35.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 45/121 [00:01<00:02, 36.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|████      | 49/121 [00:01<00:01, 36.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 53/121 [00:01<00:01, 37.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 57/121 [00:01<00:01, 37.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 61/121 [00:01<00:01, 36.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▎    | 65/121 [00:02<00:01, 37.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 69/121 [00:02<00:01, 36.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 73/121 [00:02<00:01, 35.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  64%|██████▎   | 77/121 [00:02<00:01, 35.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  67%|██████▋   | 81/121 [00:02<00:01, 34.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  70%|███████   | 85/121 [00:02<00:00, 36.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 89/121 [00:02<00:00, 35.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  77%|███████▋  | 93/121 [00:02<00:00, 36.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  80%|████████  | 97/121 [00:02<00:00, 36.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  83%|████████▎ | 101/121 [00:03<00:00, 35.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 105/121 [00:03<00:00, 34.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|█████████ | 109/121 [00:03<00:00, 33.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 113/121 [00:03<00:00, 34.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 121/121 [00:03<00:00, 33.13it/s]\n",
            "\n",
            "Training:  83%|████████▎ | 401/484 [00:45<00:51,  1.62it/s]\u001b[A\n",
            "Training:  83%|████████▎ | 403/484 [00:45<00:36,  2.22it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>traing loss: 0.60766, valid loss: 0.64006, valid f1 score: 0.39919, valid acc: 0.66442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Training:  84%|████████▎ | 405/484 [00:45<00:26,  3.00it/s]\u001b[A\n",
            "Training:  84%|████████▍ | 407/484 [00:45<00:19,  3.96it/s]\u001b[A\n",
            "Training:  85%|████████▍ | 409/484 [00:45<00:14,  5.11it/s]\u001b[A\n",
            "Training:  85%|████████▍ | 411/484 [00:45<00:11,  6.41it/s]\u001b[A\n",
            "Training:  85%|████████▌ | 413/484 [00:46<00:09,  7.60it/s]\u001b[A\n",
            "Training:  86%|████████▌ | 415/484 [00:46<00:07,  8.90it/s]\u001b[A\n",
            "Training:  86%|████████▌ | 417/484 [00:46<00:06, 10.22it/s]\u001b[A\n",
            "Training:  87%|████████▋ | 419/484 [00:46<00:05, 11.57it/s]\u001b[A\n",
            "Training:  87%|████████▋ | 421/484 [00:46<00:05, 12.60it/s]\u001b[A\n",
            "Training:  87%|████████▋ | 423/484 [00:46<00:04, 13.53it/s]\u001b[A\n",
            "Training:  88%|████████▊ | 425/484 [00:46<00:04, 13.60it/s]\u001b[A\n",
            "Training:  88%|████████▊ | 427/484 [00:47<00:04, 13.95it/s]\u001b[A\n",
            "Training:  89%|████████▊ | 429/484 [00:47<00:03, 14.71it/s]\u001b[A\n",
            "Training:  89%|████████▉ | 431/484 [00:47<00:03, 14.81it/s]\u001b[A\n",
            "Training:  89%|████████▉ | 433/484 [00:47<00:03, 14.31it/s]\u001b[A\n",
            "Training:  90%|████████▉ | 435/484 [00:47<00:03, 14.50it/s]\u001b[A\n",
            "Training:  90%|█████████ | 437/484 [00:47<00:03, 14.86it/s]\u001b[A\n",
            "Training:  91%|█████████ | 439/484 [00:47<00:03, 14.35it/s]\u001b[A\n",
            "Training:  91%|█████████ | 441/484 [00:47<00:03, 13.31it/s]\u001b[A\n",
            "Training:  92%|█████████▏| 443/484 [00:48<00:03, 13.12it/s]\u001b[A\n",
            "Training:  92%|█████████▏| 445/484 [00:48<00:03, 12.99it/s]\u001b[A\n",
            "Training:  92%|█████████▏| 447/484 [00:48<00:02, 13.19it/s]\u001b[A\n",
            "Training:  93%|█████████▎| 449/484 [00:48<00:02, 12.82it/s]\u001b[A\n",
            "Training:  93%|█████████▎| 451/484 [00:48<00:02, 13.07it/s]\u001b[A\n",
            "Training:  94%|█████████▎| 453/484 [00:48<00:02, 13.36it/s]\u001b[A\n",
            "Training:  94%|█████████▍| 455/484 [00:49<00:02, 13.03it/s]\u001b[A\n",
            "Training:  94%|█████████▍| 457/484 [00:49<00:01, 13.96it/s]\u001b[A\n",
            "Training:  95%|█████████▍| 459/484 [00:49<00:01, 14.42it/s]\u001b[A\n",
            "Training:  95%|█████████▌| 461/484 [00:49<00:01, 15.38it/s]\u001b[A\n",
            "Training:  96%|█████████▌| 463/484 [00:49<00:01, 15.55it/s]\u001b[A\n",
            "Training:  96%|█████████▌| 465/484 [00:49<00:01, 14.64it/s]\u001b[A\n",
            "Training:  96%|█████████▋| 467/484 [00:49<00:01, 14.82it/s]\u001b[A\n",
            "Training:  97%|█████████▋| 469/484 [00:49<00:01, 14.39it/s]\u001b[A\n",
            "Training:  97%|█████████▋| 471/484 [00:50<00:00, 14.28it/s]\u001b[A\n",
            "Training:  98%|█████████▊| 473/484 [00:50<00:00, 14.32it/s]\u001b[A\n",
            "Training:  98%|█████████▊| 475/484 [00:50<00:00, 13.98it/s]\u001b[A\n",
            "Training:  99%|█████████▊| 477/484 [00:50<00:00, 14.04it/s]\u001b[A\n",
            "Training:  99%|█████████▉| 479/484 [00:50<00:00, 14.94it/s]\u001b[A\n",
            "Training:  99%|█████████▉| 481/484 [00:50<00:00, 14.73it/s]\u001b[A\n",
            "Training: 100%|██████████| 484/484 [00:51<00:00,  9.48it/s]\n",
            "100%|██████████| 1/1 [00:53<00:00, 53.16s/it]\n"
          ]
        }
      ],
      "source": [
        "ESIM_model = train(model, config, id2label, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnImB2K1GRNH",
        "outputId": "21beba52-3348-4cdf-a1c0-21b36640eee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 61/61 [00:01<00:00, 35.32it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "predict(config, id2label, ESIM_model, test_dataloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "05_self_ESIM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}