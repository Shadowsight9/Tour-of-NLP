{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_self_AFQMC_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TBd0MFciyFAR",
        "MAp0Hp7L2duD",
        "lvgR0VQpw_SV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dVXLkF-4_zR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d2148f-e969-4e91-beeb-f10f0dec61c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  1 13:34:41 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "#设置路径\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "9KtM0UMx_8TA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8266c386-3f60-4302-8945-56c5c8a1f270"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers==4.0.1"
      ],
      "metadata": {
        "id": "VE9Fr9z4_-l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80959734-9ec3-420e-c986-13fe41e81507"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.0.1\n",
            "  Downloading transformers-4.0.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 13.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.0.1) (3.0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.49 tokenizers-0.9.4 transformers-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch版本为1.6\n",
        "! pip install torch==1.6.0"
      ],
      "metadata": {
        "id": "CGn36OdbAAN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3a3aea-efe7-480a-e3eb-a8266f8678ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 18 kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torchvision 是PyTorch中专门用来处理图像的库\n",
        "! pip install torchvision==0.7.0"
      ],
      "metadata": {
        "id": "k4GiZN9pAHqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92f0fc4-36ff-4440-e8ba-6cdaa3980203"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.7.0\n",
            "  Downloading torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (7.1.2)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->torchvision==0.7.0) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "Successfully installed torchvision-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "config = {\n",
        "    'train_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/train.json',\n",
        "    'dev_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/dev.json',\n",
        "    'test_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/test.json',\n",
        "    'model_path':'/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model',\n",
        "    'output_path': '.',\n",
        "    'train_val_ratio':0.1,\n",
        "    'vocab_size':30000,\n",
        "    'batch_size':64,\n",
        "    'max_seq_len':64,\n",
        "    'num_epochs':1,\n",
        "    'learning_rate':2e-5,\n",
        "    'eps': 0.1,\n",
        "    'alpha': 0.3,\n",
        "    'adv': 'fgm',\n",
        "    'warmup_ratio': 0.05,\n",
        "    'weight_decay': 0.01,\n",
        "    'use_bucket': True,\n",
        "    'bucket_multiplier': 200,\n",
        "    'n_gpus': 0,\n",
        "    'use_amp': True, # 只针对有 tensor core 的gpu有效\n",
        "    'ema_start_step': 500,\n",
        "    'ema_start': False,\n",
        "    'logging_step':100,\n",
        "    'device': 'cuda',\n",
        "    'seed':2022\n",
        "}\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  config['device'] = 'cpu'\n",
        "else:\n",
        "  config['n_gpus'] = torch.cuda.device_count()\n",
        "  config['batch_size'] *= config['n_gpus']\n",
        "\n",
        "if not os.path.exists(config['output_path']):\n",
        "    os.makedirs((config['output_path']))\n",
        "\n",
        "    \n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  return seed\n",
        "\n",
        "seed_everything(config['seed'])"
      ],
      "metadata": {
        "id": "wf9S2cB1ASJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a73574-89cb-4b35-e02c-2322a9dfdb97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2022"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data(path, data_type='train'):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "\n",
        "  with open(path, 'r', encoding = 'utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      if data_type != 'test':\n",
        "        labels.append(int(line['label']))\n",
        "      else:\n",
        "        labels.append(0)\n",
        "\n",
        "  df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns = ['text_a', 'text_b', 'labels'])\n",
        "  return df"
      ],
      "metadata": {
        "id": "uI_qfoiepHf-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## encode和encode_plus的区别\n",
        "1. encode仅返回input_ids\n",
        "2. encode_plus返回所有的编码信息，具体如下：\n",
        "’input_ids:是单词在词典中的编码; \n",
        "‘token_type_ids’:区分两个句子的编码（上句全为0，下句全为1）; \n",
        "‘attention_mask’:指定对哪些词进行self-Attention操作\n",
        "\n",
        "```\n",
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "# a.通过词典导入分词器\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "sentence = \"Hello, my son is laughing.\"\n",
        "\n",
        "print(tokenizer.encode(sentence))\n",
        "print(tokenizer.encode_plus(sentence))\n",
        "\n",
        "\n",
        "运行结果：\n",
        "\n",
        "[101, 7592, 1010, 2026, 2365, 2003, 5870, 1012, 102]\n",
        "{'input_ids': [101, 7592, 1010, 2026, 2365, 2003, 5870, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "```"
      ],
      "metadata": {
        "id": "TBd0MFciyFAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs: defaultdict(list)\n",
        "def build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
        "  # add_special_tokens [CLS] [SEP]\n",
        "  # return_token_type_ids 该词属于sentence_a(返回0) or sentence_b(返回1). \n",
        "  # return_attention_mask pad=0, 不是pad的部分标为1， 是pad标为0.\n",
        "  inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens = True,\n",
        "                     return_token_type_ids = True,\n",
        "                     return_attention_mask = True)\n",
        "  inputs['input_ids'].append(inputs_dict['input_ids'])\n",
        "  inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
        "  inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
        "  inputs['labels'].append(label)"
      ],
      "metadata": {
        "id": "MpNesXYtvHhB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## defaultdict(list)\n",
        "```\n",
        "from collections import defaultdict\n",
        "result = defaultdict(list)\n",
        "data = [(\"p\", 1), (\"p\", 2), (\"p\", 3),\n",
        "     (\"h\", 1), (\"h\", 2), (\"h\", 3)]\n",
        " \n",
        "for (key, value) in data:\n",
        "    result[key].append(value)\n",
        "print(result)#defaultdict(<class 'list'>, {'p': [1, 2, 3], 'h': [1, 2, 3]})\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "MAp0Hp7L2duD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "def read_data(config, tokenizer):\n",
        "  train_df = parse_data(config['train_file_path'], data_type = 'train')\n",
        "  dev_df = parse_data(config['dev_file_path'], data_type = 'dev')\n",
        "  test_df = parse_data(config['test_file_path'], data_type = 'test')\n",
        "\n",
        "  # 把这些 df 打包成字典\n",
        "  data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
        "  #保存 BERT 的输入\n",
        "  processed_data = {}\n",
        "  # 遍历字典(data_df)\n",
        "  for data_type, df in data_df.items():\n",
        "    inputs = defaultdict(list)\n",
        "    #遍历每一行\n",
        "    for i, row in tqdm(df.iterrows(), desc= f'Preprocessing {data_type} data', total = len(df)):\n",
        "      label = row[2]\n",
        "      sentence_a, sentence_b = row[0], row[1]\n",
        "      build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
        "\n",
        "    processed_data[data_type] = inputs\n",
        "  return processed_data"
      ],
      "metadata": {
        "id": "M5nlW7HV5IjC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(config['model_path'])\n",
        "dt = read_data(config, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iHgufu3Hr-w",
        "outputId": "993e7076-7a86-4ef0-ad5c-cdaa5b1d0658"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 83943.54it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 36928.44it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 121829.66it/s]\n",
            "Preprocessing train data: 100%|██████████| 34334/34334 [00:30<00:00, 1126.07it/s]\n",
            "Preprocessing dev data: 100%|██████████| 4316/4316 [00:02<00:00, 1448.29it/s]\n",
            "Preprocessing test data: 100%|██████████| 3861/3861 [00:02<00:00, 1347.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('train_df中 input_ids的第一条数据',dt['train']['input_ids'][0])\n",
        "print('dev_df中 token_type_ids的第一条数据',dt['dev']['token_type_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JAghBwfII4n",
        "outputId": "07cd34e5-a4da-45ea-8009-30cb6732a8b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df中 input_ids的第一条数据 [101, 6010, 6009, 955, 1446, 5023, 7583, 6820, 3621, 1377, 809, 2940, 2768, 1044, 2622, 1400, 3315, 1408, 102, 955, 1446, 3300, 1044, 2622, 1168, 3309, 6820, 3315, 1408, 102]\n",
            "dev_df中 token_type_ids的第一条数据 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "  \n",
        "  # 返回一个example\n",
        "  def __getitem__(self, idx):\n",
        "    data = (self.data_dict['input_ids'][idx],\n",
        "         self.data_dict['token_type_ids'][idx],\n",
        "         self.data_dict['attention_mask'][idx],\n",
        "         self.data_dict['labels'][idx])\n",
        "    return data\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])"
      ],
      "metadata": {
        "id": "H6u3Lttbs8qx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Collator:\n",
        "  def __init__(self, max_seq_len, tokenizer):\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.tokenizer = tokenizer\n",
        "  \n",
        "  def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
        "    input_ids = torch.zeros((len(input_ids_list), max_seq_len),dtype=torch.long)\n",
        "    token_type_ids = torch.zeros_like(input_ids)\n",
        "    attention_mask = torch.zeros_like(input_ids)\n",
        "    \n",
        "    for i in range(len(input_ids_list)):\n",
        "      seq_len = len(input_ids_list[i])\n",
        "      if seq_len <= max_seq_len:\n",
        "        input_ids[i, :seq_len] = torch.tensor(input_ids_list[i], dtype = torch.long)\n",
        "        token_type_ids[i, :seq_len] = torch.tensor(token_type_ids_list[i], dtype = torch.long)\n",
        "        attention_mask[i, :seq_len] = torch.tensor(attention_mask_list[i], dtype = torch.long)\n",
        "      else:\n",
        "        # input_ids 最后一位放上一个特殊的token\n",
        "        input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len-1] + [self.tokenizer.sep_token_id], dtype = torch.long)\n",
        "        # token_type_ids 和 attention_mask 不需要加上特殊token\n",
        "        token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype = torch.long)\n",
        "        attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype = torch.long)\n",
        "    labels = torch.tensor(labels_list, dtype = torch.long)\n",
        "    return input_ids, token_type_ids, attention_mask, labels\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    input_ids_list, token_type_ids_list, attention_mask_list, labels_list = list(zip(*examples))\n",
        "    cur_max_seq_len = max(len(input_id) for input_id in input_ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "    \n",
        "    input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len)                     \n",
        "    \n",
        "    data_dict = {\n",
        "        'input_ids': input_ids,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels\n",
        "    }\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "1Xf4ahXKw6PX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn = Collator(config['max_seq_len'], tokenizer)"
      ],
      "metadata": {
        "id": "4eU_XHZx6F24"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 采样（Dataloader）"
      ],
      "metadata": {
        "id": "lvgR0VQpw_SV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Dataloader](https://img-blog.csdnimg.cn/b80cee8a1c7d49b79e7b80cc81150d66.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "DSqsAdWE9Vf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampler \n",
        "所有采样器都继承自Sampler这个类\n",
        "\n",
        "每个Sampler子类都要实现iter方法【迭代数据集example索引的方法】，以及返回迭代器长度的len方法"
      ],
      "metadata": {
        "id": "QS-HVTDV9xfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sampler](https://img-blog.csdnimg.cn/1c40aedade9f40a493b4df97d0c1def0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "HhylX0uH9Vb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 顺序采样\n",
        "![sequentialsampler](https://img-blog.csdnimg.cn/9e8ee018cea84729ac6b5742395d8ea2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)\n",
        "\n",
        "***在初始化时拿到数据集data_source， 按顺序对元素进行采样，每次只返回一个索引值 ***"
      ],
      "metadata": {
        "id": "FXjatAc6BUGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 顺序采样举例\n",
        "# randperm 把 0-23 数据打乱 形成3维tensor\n",
        "# (2,3,4) batch_size:2 seq_len=3, embedding_dim=4，每个 batch 有2条数据，每个句子包含3个词， 每个词的维度是4\n",
        "a = torch.randperm(24).reshape((2,3,4))\n",
        "print('a:',a)\n",
        "b = torch.utils.data.SequentialSampler(a)\n",
        "print('b:',b)\n",
        "# i 是索引\n",
        "for i in b:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dZh5C6acm0S",
        "outputId": "aaf4ac15-8166-4c94-eed5-6057cc2e4603"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([[[ 5, 14,  9,  2],\n",
            "         [13, 12, 20,  1],\n",
            "         [16, 15,  7,  4]],\n",
            "\n",
            "        [[17,  0,  3, 19],\n",
            "         [10, 22,  6, 18],\n",
            "         [ 8, 23, 11, 21]]])\n",
            "b: <torch.utils.data.sampler.SequentialSampler object at 0x7f0ce6e01810>\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 随机采样\n",
        "replacement : True 表示可以重复采样\n",
        "\n",
        "num_samples: 指定采样的数量\n",
        "\n",
        "PS:当使用replacement=False，不应制定num_samples\n",
        "![randomsampler](https://img-blog.csdnimg.cn/9d2e2afdbe4d4df4aee3102e46054650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "7wyFCf2CEq7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 随机采样举例\n",
        "a = torch.randperm(60).reshape((5,3,4))\n",
        "print('a:',a)\n",
        "# 随机采样3条数据\n",
        "b = torch.utils.data.RandomSampler(a, replacement=True, num_samples=3)\n",
        "print('b:',b)\n",
        "for i in b:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52NgI_URFidd",
        "outputId": "d07f03b1-2204-4cc8-b68b-9d2671e7e895"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([[[47, 36, 58, 12],\n",
            "         [49,  7, 24,  9],\n",
            "         [27, 13, 45,  0]],\n",
            "\n",
            "        [[ 3, 28, 23, 39],\n",
            "         [37, 29, 10, 59],\n",
            "         [ 4, 35, 56, 53]],\n",
            "\n",
            "        [[54, 32, 18, 42],\n",
            "         [41, 46, 30, 14],\n",
            "         [38, 22, 11,  5]],\n",
            "\n",
            "        [[48, 33, 57, 26],\n",
            "         [15, 19, 55, 16],\n",
            "         [20, 40, 31,  6]],\n",
            "\n",
            "        [[51, 17,  1, 25],\n",
            "         [34,  2, 43, 21],\n",
            "         [52, 50,  8, 44]]])\n",
            "b: <torch.utils.data.sampler.RandomSampler object at 0x7f0c5c29b250>\n",
            "3\n",
            "3\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subset随机采样\n",
        "SubsetRandomSampler： 从给定的索引列表中随机采样元素，不放回采样 \n",
        "\n",
        "indices(sequence): 索引序列\n",
        "![sunsetRandomSampler](https://img-blog.csdnimg.cn/e80f6a1bafe042f28da652dc5a2388ab.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "5BoYgZchGmqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset采样举例\n",
        "a = torch.randperm(60).reshape((5,3,4))\n",
        "print('a:',a)\n",
        "# 从索引2以后的样本中随机采样\n",
        "b = torch.utils.data.SubsetRandomSampler(indices=a[2:])\n",
        "for i in b:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDJ0vAv4HCci",
        "outputId": "b0e7c07f-2b04-435c-db15-68aa85de57c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([[[ 8, 18, 55, 16],\n",
            "         [49, 54, 14,  7],\n",
            "         [33, 37, 39,  2]],\n",
            "\n",
            "        [[45,  6, 24, 29],\n",
            "         [58, 57,  3, 47],\n",
            "         [46, 56, 26, 21]],\n",
            "\n",
            "        [[12, 25, 52, 40],\n",
            "         [ 9, 53, 10, 50],\n",
            "         [48, 59, 27, 22]],\n",
            "\n",
            "        [[ 0, 20, 34, 13],\n",
            "         [41, 32, 35, 51],\n",
            "         [15,  4, 36, 38]],\n",
            "\n",
            "        [[11, 19,  5, 43],\n",
            "         [23, 31, 44, 30],\n",
            "         [28,  1, 42, 17]]])\n",
            "tensor([[ 0, 20, 34, 13],\n",
            "        [41, 32, 35, 51],\n",
            "        [15,  4, 36, 38]])\n",
            "tensor([[12, 25, 52, 40],\n",
            "        [ 9, 53, 10, 50],\n",
            "        [48, 59, 27, 22]])\n",
            "tensor([[11, 19,  5, 43],\n",
            "        [23, 31, 44, 30],\n",
            "        [28,  1, 42, 17]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 分批采样\n",
        "sampler: 基采样器 \n",
        "\n",
        "batch_size: size of mini-batch\n",
        "\n",
        "drop_last=True, 如果一个batch的长度小于batch_size则丢弃\n",
        "![BatchSampler](https://img-blog.csdnimg.cn/8a1b2f5ae320453c9fae8ae8e0ef2080.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)\n",
        "\n"
      ],
      "metadata": {
        "id": "SF2c2rgKoquU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 分批采样举例\n",
        "a = torch.randperm(60).reshape((5,3,4))\n",
        "print('a:',a)\n",
        "# 要传一个基采样器torch.utils.data.RandomSampler(a)\n",
        "b = torch.utils.data.BatchSampler(torch.utils.data.RandomSampler(a), 2, drop_last=True)\n",
        "# 上面的i都是一个数；现在是batch_size的列表\n",
        "for i in b:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNi3EU11oqEc",
        "outputId": "bb21f148-e515-4e27-8804-60de3ae132df"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([[[34, 50, 28, 24],\n",
            "         [46,  0, 35, 21],\n",
            "         [51, 52, 33, 59]],\n",
            "\n",
            "        [[31,  5, 26, 42],\n",
            "         [11, 49,  8, 29],\n",
            "         [ 9, 17, 53, 36]],\n",
            "\n",
            "        [[ 1, 37, 22, 40],\n",
            "         [18, 20, 45,  7],\n",
            "         [10, 47, 19, 32]],\n",
            "\n",
            "        [[38, 14, 58,  3],\n",
            "         [13, 25, 27, 48],\n",
            "         [ 6, 44, 55, 30]],\n",
            "\n",
            "        [[56,  2, 57, 12],\n",
            "         [ 4, 23, 16, 15],\n",
            "         [43, 54, 41, 39]]])\n",
            "[3, 4]\n",
            "[1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 桶采样\n",
        "sort_key: 按XXX排序\n",
        "\n",
        "bucket_sampler: batch_size * bucket_size_multiplier 相当于 n * batch_size\n",
        "；len(sampler)最大为数据集的长度\n",
        "![BucketSampler](https://img-blog.csdnimg.cn/6413cea5dfbf4494a6b2b64504f74a97.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "66V_qSLzuZpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![SortedSampler](https://img-blog.csdnimg.cn/64f60217df474cf1b0d7aa1c3558cc1f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "V-VMY39qxSKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![BucketSampler](https://img-blog.csdnimg.cn/d7e03938f2824f9cb8a6c3a895f5a78a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "_WX-4OucxiqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 桶采样举例\n",
        "# Dataset -> 得到‘大桶'的排序索引\n",
        "\n",
        "# 真实train中数据，前6条\n",
        "mini_dataset = {k: v[:6] for k, v in dt['train'].items()}\n",
        "mini_data = AFQMCDataset(mini_dataset)\n",
        "print(mini_data)\n",
        "# mini_data 的前6条数据的长度\n",
        "for i, d in enumerate(mini_data):\n",
        "    print(d[0]) # input_ids\n",
        "    print(len(d[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epOJ-plhx-XG",
        "outputId": "1f5a0c37-2c99-4da8-cacf-54663d253bcc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.AFQMCDataset object at 0x7f0c5c686310>\n",
            "[101, 6010, 6009, 955, 1446, 5023, 7583, 6820, 3621, 1377, 809, 2940, 2768, 1044, 2622, 1400, 3315, 1408, 102, 955, 1446, 3300, 1044, 2622, 1168, 3309, 6820, 3315, 1408, 102]\n",
            "30\n",
            "[101, 6010, 6009, 5709, 1446, 6432, 2769, 6824, 5276, 671, 3613, 102, 6010, 6009, 5709, 1446, 6824, 5276, 6121, 711, 3221, 784, 720, 102]\n",
            "24\n",
            "[101, 2376, 2769, 4692, 671, 678, 3315, 3299, 5709, 1446, 6572, 1296, 3300, 3766, 3300, 5310, 3926, 102, 678, 3299, 5709, 1446, 6572, 1296, 102]\n",
            "25\n",
            "[101, 6010, 6009, 955, 1446, 1914, 7270, 3198, 7313, 5341, 1394, 6397, 844, 671, 3613, 102, 955, 1446, 2533, 6397, 844, 1914, 719, 102]\n",
            "24\n",
            "[101, 2769, 4638, 5709, 1446, 6572, 1296, 3221, 115, 115, 115, 8024, 6820, 3621, 2582, 720, 3221, 115, 115, 115, 102, 2769, 4638, 5709, 1446, 8024, 3299, 5310, 1139, 3341, 6432, 6375, 2769, 6820, 115, 115, 115, 1039, 8024, 2769, 5632, 2346, 5050, 749, 671, 678, 6422, 5301, 1399, 1296, 2769, 2418, 6421, 6820, 115, 115, 115, 1039, 102]\n",
            "59\n",
            "[101, 6010, 6009, 955, 1446, 4638, 7583, 2428, 1377, 809, 794, 4509, 6435, 679, 102, 6010, 6009, 955, 1446, 5688, 969, 3189, 1377, 809, 955, 3621, 1408, 102]\n",
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bucket_sampler import SortedSampler\n",
        "random_sampler = torch.utils.data.RandomSampler(mini_data, replacement=False)\n",
        "# print(list(random_sampler))\n",
        "# 关于dataset的随机索引 [3, 5, 4, 1, 0, 2]\n",
        "\n",
        "batch_sampler = torch.utils.data.BatchSampler(random_sampler, 4, drop_last=True)\n",
        "# [0, 5, 2, 4] 【还有[1, 3] 但是丢弃了】\n",
        "\n",
        "for samp in batch_sampler:\n",
        "    print('samp:',samp)\n",
        "    sorted_sampler = SortedSampler(samp, sort_key=lambda x:len(mini_data[x][0]))\n",
        "    print('list_sorted_sampler:',list(sorted_sampler))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yyjconvz6TO",
        "outputId": "0e468aa9-c2fc-46d6-9fdf-8b6cc620c694"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "samp: [0, 3, 1, 4]\n",
            "list_sorted_sampler: [1, 2, 0, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "[0, 5, 2, 4]分别对应mini_data中的长度[30, 28, 25, 59]\n",
        "\n",
        "[2, 1, 0, 3] \n",
        "\n",
        "2（位置2的数据len最小） -> 2 -> 25 \n",
        "\n",
        "1 -> 5 -> 28 \n",
        "\n",
        "0 -> 0 -> 30 \n",
        "\n",
        "3（位置3的数据len最大） -> 4 -> 59\n",
        "```"
      ],
      "metadata": {
        "id": "-A6oFU9B1RXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 得到‘大桶'的排序索引 -> 返回‘小桶'在‘大桶'中的位置\n",
        "c = list(torch.utils.data.BatchSampler(sorted_sampler, 2, drop_last=True))\n",
        "print(c)\n",
        "# c 把大桶 分成 batch_size大小的小桶"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az7SLVSs2idz",
        "outputId": "f542674d-48b0-4bb1-c01b-125c2be83c04"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2], [0, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "[[2, 1], [0, 3]]\n",
        "```"
      ],
      "metadata": {
        "id": "ABvwvlmZ5_r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in torch.utils.data.SubsetRandomSampler(c):\n",
        "    print('从给定的索引列表中随机采样元素')\n",
        "    print(batch)\n",
        "    print('所对应的原序列是什么：')\n",
        "    print([samp[i] for i in batch])\n",
        "    # 参考上面 from bucket_sampler import SortedSampler 单元格对应法则"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGcdBAPo3Za7",
        "outputId": "af670e2d-c1c3-4501-aa3c-4de78a7be505"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "从给定的索引列表中随机采样元素\n",
            "[1, 2]\n",
            "所对应的原序列是什么：\n",
            "[3, 1]\n",
            "从给定的索引列表中随机采样元素\n",
            "[0, 3]\n",
            "所对应的原序列是什么：\n",
            "[0, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "从给定的索引列表中随机采样元素\n",
        "[2, 1]\n",
        "所对应的原序列是什么：\n",
        "[2, 5]\n",
        "从给定的索引列表中随机采样元素\n",
        "[0, 3]\n",
        "所对应的原序列是什么：\n",
        "[0, 4]\n",
        "```"
      ],
      "metadata": {
        "id": "l_W55Rvx5WxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 采样在dataloader中使用\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from bucket_sampler import BucketBatchSampler\n",
        "\n",
        "def build_dataloader(config, data, collate_fn):\n",
        "  train_dataset = AFQMCDataset(data['train'])\n",
        "  dev_dataset = AFQMCDataset(data['dev'])\n",
        "  test_dataset = AFQMCDataset(data['test'])\n",
        "\n",
        "  if config['use_bucket']:\n",
        "    # 先放一个基采样器\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    # sort_key 以input_ids 的len排序\n",
        "    bucket_sampler = BucketBatchSampler(train_sampler, \n",
        "                       batch_size = config['batch_size'],\n",
        "                       drop_last = False,\n",
        "                       sort_key = lambda x:len(train_dataset[x][0]),\n",
        "                       bucket_size_multiplier = config['bucket_multiplier'])\n",
        "    train_dataloader = DataLoader(dataset = train_dataset, batch_sampler = bucket_sampler,\n",
        "                    num_workers = 4, collate_fn = collate_fn)\n",
        "    \n",
        "  else:\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = config['batch_size'],\n",
        "                    shuffle = True, num_workers = 4, collate_fn = collate_fn)\n",
        "  dev_dataloader = DataLoader(dev_dataset, batch_size = config['batch_size'],\n",
        "                  shuffle = False, num_workers = 4, collate_fn = collate_fn)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size = config['batch_size'],\n",
        "                  shuffle = False, num_workers = 4, collate_fn = collate_fn)\n",
        "  return train_dataloader, dev_dataloader, test_dataloader  \n"
      ],
      "metadata": {
        "id": "UwqaHWKQ_qJV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, dev_dataloader, test_dataloader = build_dataloader(config, dt, collate_fn)"
      ],
      "metadata": {
        "id": "ck6rtFpEAQ70"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataloader:\n",
        "    print('train_dataloader一个batch:',i)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5epRv3qR2mC",
        "outputId": "1ec8232a-092f-4679-a67f-21e046778206"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader一个batch: {'input_ids': tensor([[ 101, 2769,  671,  ...,  955, 1446,  102],\n",
            "        [ 101,  671, 2476,  ..., 1126,  702,  102],\n",
            "        [ 101, 2769, 4500,  ..., 1168, 6572,  102],\n",
            "        ...,\n",
            "        [ 101, 2769, 4638,  ..., 1921, 6820,  102],\n",
            "        [ 101, 2769, 4638,  ..., 5709, 1446,  102],\n",
            "        [ 101, 2769,  955,  ...,  955, 1446,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
            "        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自动混合精度（混合精度训练）\n",
        "\n",
        "![amp](https://img-blog.csdnimg.cn/e4226734b82f462e983aa905de50891a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)\n"
      ],
      "metadata": {
        "id": "fDXEWnmmAQlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 混合精度训练\n",
        "作用：训练时，尽量不降低性能，并提升速度 \n",
        "\n",
        "Float16优点:\n",
        "\n",
        "* 减少内存的使用\n",
        "* 加快训练和推断的计算，能带来多一倍速的体验\n",
        "\n",
        "Float16缺点:\n",
        "* 溢出错误\n",
        "* 舍入误差"
      ],
      "metadata": {
        "id": "bIV8wdDKpjpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.FloatTensor 32位\n",
        "a = torch.zeros(2,3)\n",
        "print(a.type())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrzjvFGxDNqo",
        "outputId": "dee3f550-0ebb-4a2f-a932-43a2d3c63641"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.FloatTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "混合精度将 ***autocast*** 和 ***GradScaler*** 一起使用"
      ],
      "metadata": {
        "id": "u0K13AmcEtwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***当进入autocast()时， 系统自动切换为float16, autocast上下文只包含前向传播，建议不用反向传播***"
      ],
      "metadata": {
        "id": "61kZsJEeEB0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![amp2](https://img-blog.csdnimg.cn/72b642b508024cc2a6207c308347c7e7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "WJZyS-0hwR9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient Scaling\n",
        "* scaler.scale(loss) 将给定的损失乘以缩放器的当前比例因子，进行反向传播\n",
        "* scaler.step(optimizer) 取消缩放梯度并调用optimizer.step()\n",
        "* scaler.update() 更新缩放器的比例因子"
      ],
      "metadata": {
        "id": "sn1VIDX5ETYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![scaling](https://img-blog.csdnimg.cn/bbae5cdd360748ecb59cee8dc6f728f2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "J8rDZvQnwXVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![GradScaler](https://img-blog.csdnimg.cn/2c0fdf08602748ea8a7655f2d5bb1829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "lrT4nqUiway0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![example](https://img-blog.csdnimg.cn/dfeebde4d34b496096062bb7dbbee7b6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "eSAb4I01wdOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def evaluation(config, model, val_dataloader):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  labels = []\n",
        "  val_loss = 0.\n",
        "  val_iterator = tqdm(val_dataloader, desc = 'Evaluation', total = len(val_dataloader))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_iterator:\n",
        "      labels.append(batch['labels'])\n",
        "      batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
        "      loss, logits = model(**batch_cuda)[:2]\n",
        "\n",
        "      if config['n_gpus'] > 1:\n",
        "        loss = loss.mean()\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      preds.append(logits.argmax(dim = -1).detach().cpu())\n",
        "\n",
        "  avg_val_loss = val_loss / len(val_dataloader)\n",
        "  labels = torch.cat(labels, dim = 0).numpy()\n",
        "  preds = torch.cat(preds, dim = 0).numpy()\n",
        "  f1 = f1_score(labels, preds)\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return avg_val_loss, f1, acc"
      ],
      "metadata": {
        "id": "gG3kyfNQ7Xcu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EMA:\n",
        "  def __init__(self, model, decay):\n",
        "    self.model = model\n",
        "    self.decay = decay\n",
        "    self.shadow = {}\n",
        "    self.backup = {}\n",
        "    self.register()\n",
        "\n",
        "  def register(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        self.shadow[name] = param.data.clone()\n",
        "\n",
        "  def update(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        # 如果 name in self.shadow 则运行下面两行代码， 否则报错\n",
        "        assert name in self.shadow\n",
        "        new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
        "        self.shadow[name] = new_average.clone()\n",
        "\n",
        "  def apply_shadow(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        assert name in self.shadow\n",
        "        self.backup[name] = param.data\n",
        "        param.data = self.shadow[name]\n",
        " \n",
        "  def restore(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        assert name in self.backup\n",
        "        param.data = self.backup[name]\n",
        "    self.backup = {}\n",
        "  "
      ],
      "metadata": {
        "id": "j2yDMxscHcem"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from torch.cuda import amp\n",
        "from transformers import AdamW\n",
        "from extra_pgd import *\n",
        "from extra_loss import *\n",
        "from extra_fgm import *\n",
        "from extra_optim import *\n",
        "from tqdm import trange\n",
        "def train(config, train_dataloader, dev_dataloader):\n",
        "  # 封装好 BertForSequenceClassification\n",
        "  model = BertForSequenceClassification.from_pretrained(config['model_path'])\n",
        "\n",
        "  # param_optimizer = model.named_parameters()\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "\n",
        "  # 实例化scaler对象 enabled=True 可以使用梯度缩放\n",
        "  scaler = amp.GradScaler(enabled = config['use_amp'])\n",
        "\n",
        "  # 权重缩减\n",
        "  no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "  # 名称包含 ['bias', 'LayerNorm.weight']的权重， 其权重衰减因子为0\n",
        "  # 名称不包含 ['bias', 'LayerNorm.weight']的权重， 其权重衰减因子为 0.01\n",
        "  # any() 理解成any True的意思，是否存在True，只要有一个是True，结果就是True\n",
        "  optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay': config['weight_decay']},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay': 0.0}\n",
        "  ]\n",
        "\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr = config['learning_rate'],\n",
        "            eps = 1e-8)\n",
        "  \n",
        "  # lookahead 预先查看由 AdamW 生成的快速权重 来选择搜索方向\n",
        "  optimizer = Lookahead(optimizer, 5, 1)\n",
        "  total_steps = config['num_epochs'] * len(train_dataloader)\n",
        "\n",
        "  # 使用Warmup来调整学习率，每调用warmup_steps次，对应的学习率就会调整一次。\n",
        "  lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps = int(config['warmup_ratio'] * total_steps),\n",
        "                     t_total = total_steps)\n",
        "  \n",
        "\n",
        "                                  \n",
        "  model.to(config['device'])\n",
        "\n",
        "  if config['adv'] == 'fgm':\n",
        "    fgm = FGM(model)\n",
        "  else:\n",
        "    pgd = PGD(model)\n",
        "    K = 3\n",
        "\n",
        "  epoch_iterator = trange(config['num_epochs'])\n",
        "  global_steps = 0\n",
        "  train_loss = 0.\n",
        "  logging_loss = 0.\n",
        "  best_acc = 0.\n",
        "  best_model_path = ''\n",
        "\n",
        "  # 多卡情况\n",
        "  if config['n_gpus'] > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "  for _ in epoch_iterator:\n",
        "    train_iterator = tqdm(train_dataloader, desc = 'Trainging', total = len(train_dataloader))\n",
        "    model.train()\n",
        "    for batch in train_iterator:\n",
        "      batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
        "\n",
        "      # 前向过程（前向传播 + loss）\n",
        "      with amp.autocast(enabled = config['use_amp']):\n",
        "        loss = model(**batch_cuda)[0]\n",
        "        # 多卡 取平均\n",
        "        if config['n_gpus'] > 1:\n",
        "          loss = loss.mean()\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "      if config['adv'] == 'fgm':\n",
        "          # 在embedding上加扰动\n",
        "        fgm.attack(epsilon = config['eps'])\n",
        "\n",
        "          # autocast\n",
        "        with amp.autocast(enabled = config['use_amp']):\n",
        "          loss_adv = model(**batch_cuda)[0]\n",
        "\n",
        "          if config['n_gpus'] > 1:\n",
        "            loss_adv =loss_adv.mean()\n",
        "\n",
        "        scaler.scale(loss_adv).backward()\n",
        "        # 恢复embedding参数\n",
        "        fgm.restore()\n",
        "      else:\n",
        "        pgd.backup_grad()\n",
        "        for t in range(K):\n",
        "          pgd.attack(epsilon = config['eps'], alpha = config['alpha'], is_first_attack= ( t == 0))\n",
        "          if t != K - 1:\n",
        "            model.zero_grad()\n",
        "          else:\n",
        "            pgd.restore_grad()\n",
        "          with amp.autocast(enabled = config['use_amp']):\n",
        "            loss_adv = model(**batch_cuda)[0]\n",
        "            if config['n_gpus'] > 1:\n",
        "              loss_adv = loss_adv.mean()\n",
        "\n",
        "          scaler.scale(loss_adv).backward()\n",
        "        pgd.restore()\n",
        "        \n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "      lr_scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if config['ema_start']:\n",
        "        ema.update()\n",
        "        \n",
        "      train_loss += loss.item()\n",
        "      global_steps += 1\n",
        "\n",
        "      train_iterator.set_postfix_str(f'running train loss: {loss.item():.5f}')\n",
        "\n",
        "      if global_steps % config['logging_step'] == 0:\n",
        "        if global_steps >= config['ema_start_step'] and not config['ema_start']:\n",
        "          print('\\n>>> EMA starting .....')\n",
        "          config['ema_start'] = True\n",
        "\n",
        "          ema = EMA(model.module if hasattr(model, 'module') else model, decay = 0.99)\n",
        "\n",
        "        print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
        "        logging_loss = train_loss\n",
        "\n",
        "        if config['ema_start']:\n",
        "          ema.apply_shadow()\n",
        "        val_loss, f1, acc = evaluation(config, model, dev_dataloader)\n",
        "\n",
        "        print_log = f'\\n>>> training loss: {print_train_loss:.6f}, valid loss: {val_loss:.6f},' \n",
        "\n",
        "        if acc > best_acc:\n",
        "          model_save_path = os.path.join(config['output_path'],\n",
        "                          f'checkpoint- {global_steps} - {acc:.6f}')\n",
        "          model_to_save = model.module if hasattr(model, 'module') else model\n",
        "          model_to_save.save_pretrained(model_save_path)\n",
        "          best_acc = acc\n",
        "          best_model_path = model_save_path\n",
        "        print_log += f'valid f1: {f1:.6f}, valid acc:{acc:.6f}'\n",
        "\n",
        "        print(print_log)\n",
        "        model.train()\n",
        "\n",
        "        if config['ema_start']:\n",
        "          ema.restore()\n",
        "\n",
        "\n",
        "  return model, best_model_path        \n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "91xvXsDETp7m"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(config, train_dataloader, dev_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpCUDSWDk2hk",
        "outputId": "b2b32cb2-981d-43ac-fad9-31b10e9af5c9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Trainging:   0%|          | 0/537 [00:00<?, ?it/s]\u001b[A\n",
            "Trainging:   0%|          | 0/537 [00:05<?, ?it/s, running train loss: 0.62231]\u001b[A\n",
            "Trainging:   0%|          | 1/537 [00:05<47:32,  5.32s/it, running train loss: 0.62231]\u001b[A\n",
            "Trainging:   0%|          | 1/537 [00:09<47:32,  5.32s/it, running train loss: 0.61432]\u001b[A\n",
            "Trainging:   0%|          | 2/537 [00:09<43:34,  4.89s/it, running train loss: 0.61432]\u001b[A\n",
            "Trainging:   0%|          | 2/537 [00:13<43:34,  4.89s/it, running train loss: 0.61790]\u001b[A\n",
            "Trainging:   1%|          | 3/537 [00:13<39:32,  4.44s/it, running train loss: 0.61790]\u001b[A\n",
            "Trainging:   1%|          | 3/537 [00:17<39:32,  4.44s/it, running train loss: 0.54319]\u001b[A\n",
            "Trainging:   1%|          | 4/537 [00:17<35:36,  4.01s/it, running train loss: 0.54319]\u001b[A\n",
            "Trainging:   1%|          | 4/537 [00:20<35:36,  4.01s/it, running train loss: 0.59943]\u001b[A\n",
            "Trainging:   1%|          | 5/537 [00:20<32:54,  3.71s/it, running train loss: 0.59943]\u001b[A\n",
            "Trainging:   1%|          | 5/537 [00:23<32:54,  3.71s/it, running train loss: 0.66518]\u001b[A\n",
            "Trainging:   1%|          | 6/537 [00:23<31:54,  3.60s/it, running train loss: 0.66518]\u001b[A\n",
            "Trainging:   1%|          | 6/537 [00:27<31:54,  3.60s/it, running train loss: 0.58813]\u001b[A\n",
            "Trainging:   1%|▏         | 7/537 [00:27<31:34,  3.57s/it, running train loss: 0.58813]\u001b[A\n",
            "Trainging:   1%|▏         | 7/537 [00:30<31:34,  3.57s/it, running train loss: 0.65023]\u001b[A\n",
            "Trainging:   1%|▏         | 8/537 [00:30<31:38,  3.59s/it, running train loss: 0.65023]\u001b[A\n",
            "Trainging:   1%|▏         | 8/537 [00:35<31:38,  3.59s/it, running train loss: 0.56604]\u001b[A\n",
            "Trainging:   2%|▏         | 9/537 [00:35<34:04,  3.87s/it, running train loss: 0.56604]\u001b[A\n",
            "Trainging:   2%|▏         | 9/537 [00:38<34:04,  3.87s/it, running train loss: 0.59254]\u001b[A\n",
            "Trainging:   2%|▏         | 10/537 [00:38<33:08,  3.77s/it, running train loss: 0.59254]\u001b[A\n",
            "Trainging:   2%|▏         | 10/537 [00:42<33:08,  3.77s/it, running train loss: 0.55515]\u001b[A\n",
            "Trainging:   2%|▏         | 11/537 [00:42<33:40,  3.84s/it, running train loss: 0.55515]\u001b[A\n",
            "Trainging:   2%|▏         | 11/537 [00:46<33:40,  3.84s/it, running train loss: 0.63968]\u001b[A\n",
            "Trainging:   2%|▏         | 12/537 [00:46<32:38,  3.73s/it, running train loss: 0.63968]\u001b[A\n",
            "Trainging:   2%|▏         | 12/537 [00:50<32:38,  3.73s/it, running train loss: 0.56992]\u001b[A\n",
            "Trainging:   2%|▏         | 13/537 [00:50<34:44,  3.98s/it, running train loss: 0.56992]\u001b[A\n",
            "Trainging:   2%|▏         | 13/537 [00:55<34:44,  3.98s/it, running train loss: 0.63720]\u001b[A\n",
            "Trainging:   3%|▎         | 14/537 [00:55<36:55,  4.24s/it, running train loss: 0.63720]\u001b[A\n",
            "Trainging:   3%|▎         | 14/537 [01:00<36:55,  4.24s/it, running train loss: 0.73981]\u001b[A\n",
            "Trainging:   3%|▎         | 15/537 [01:00<37:57,  4.36s/it, running train loss: 0.73981]\u001b[A\n",
            "Trainging:   3%|▎         | 15/537 [01:04<37:57,  4.36s/it, running train loss: 0.65505]\u001b[A\n",
            "Trainging:   3%|▎         | 16/537 [01:04<35:52,  4.13s/it, running train loss: 0.65505]\u001b[A\n",
            "Trainging:   3%|▎         | 16/537 [01:07<35:52,  4.13s/it, running train loss: 0.55137]\u001b[A\n",
            "Trainging:   3%|▎         | 17/537 [01:07<33:51,  3.91s/it, running train loss: 0.55137]\u001b[A\n",
            "Trainging:   3%|▎         | 17/537 [01:11<33:51,  3.91s/it, running train loss: 0.58615]\u001b[A\n",
            "Trainging:   3%|▎         | 18/537 [01:11<33:11,  3.84s/it, running train loss: 0.58615]\u001b[A\n",
            "Trainging:   3%|▎         | 18/537 [01:14<33:11,  3.84s/it, running train loss: 0.55769]\u001b[A\n",
            "Trainging:   4%|▎         | 19/537 [01:14<33:07,  3.84s/it, running train loss: 0.55769]\u001b[A\n",
            "Trainging:   4%|▎         | 19/537 [01:19<33:07,  3.84s/it, running train loss: 0.66329]\u001b[A\n",
            "Trainging:   4%|▎         | 20/537 [01:19<35:33,  4.13s/it, running train loss: 0.66329]\u001b[A\n",
            "Trainging:   4%|▎         | 20/537 [01:25<35:33,  4.13s/it, running train loss: 0.55482]\u001b[A\n",
            "Trainging:   4%|▍         | 21/537 [01:25<39:01,  4.54s/it, running train loss: 0.55482]\u001b[A\n",
            "Trainging:   4%|▍         | 21/537 [01:28<39:01,  4.54s/it, running train loss: 0.59358]\u001b[A\n",
            "Trainging:   4%|▍         | 22/537 [01:28<36:27,  4.25s/it, running train loss: 0.59358]\u001b[A\n",
            "Trainging:   4%|▍         | 22/537 [01:32<36:27,  4.25s/it, running train loss: 0.60292]\u001b[A\n",
            "Trainging:   4%|▍         | 23/537 [01:32<33:56,  3.96s/it, running train loss: 0.60292]\u001b[A\n",
            "Trainging:   4%|▍         | 23/537 [01:36<33:56,  3.96s/it, running train loss: 0.64755]\u001b[A\n",
            "Trainging:   4%|▍         | 24/537 [01:36<35:32,  4.16s/it, running train loss: 0.64755]\u001b[A\n",
            "Trainging:   4%|▍         | 24/537 [01:41<35:32,  4.16s/it, running train loss: 0.63174]\u001b[A\n",
            "Trainging:   5%|▍         | 25/537 [01:41<36:30,  4.28s/it, running train loss: 0.63174]\u001b[A\n",
            "Trainging:   5%|▍         | 25/537 [01:45<36:30,  4.28s/it, running train loss: 0.65112]\u001b[A\n",
            "Trainging:   5%|▍         | 26/537 [01:45<35:29,  4.17s/it, running train loss: 0.65112]\u001b[A\n",
            "Trainging:   5%|▍         | 26/537 [01:50<35:29,  4.17s/it, running train loss: 0.60633]\u001b[A\n",
            "Trainging:   5%|▌         | 27/537 [01:50<37:36,  4.42s/it, running train loss: 0.60633]\u001b[A\n",
            "Trainging:   5%|▌         | 27/537 [01:54<37:36,  4.42s/it, running train loss: 0.71202]\u001b[A\n",
            "Trainging:   5%|▌         | 28/537 [01:54<36:01,  4.25s/it, running train loss: 0.71202]\u001b[A\n",
            "Trainging:   5%|▌         | 28/537 [01:57<36:01,  4.25s/it, running train loss: 0.67540]\u001b[A\n",
            "Trainging:   5%|▌         | 29/537 [01:57<34:10,  4.04s/it, running train loss: 0.67540]\u001b[A\n",
            "Trainging:   5%|▌         | 29/537 [02:01<34:10,  4.04s/it, running train loss: 0.60605]\u001b[A\n",
            "Trainging:   6%|▌         | 30/537 [02:01<33:12,  3.93s/it, running train loss: 0.60605]\u001b[A\n",
            "Trainging:   6%|▌         | 30/537 [02:06<33:12,  3.93s/it, running train loss: 0.63858]\u001b[A\n",
            "Trainging:   6%|▌         | 31/537 [02:06<35:46,  4.24s/it, running train loss: 0.63858]\u001b[A\n",
            "Trainging:   6%|▌         | 31/537 [02:09<35:46,  4.24s/it, running train loss: 0.68499]\u001b[A\n",
            "Trainging:   6%|▌         | 32/537 [02:09<33:55,  4.03s/it, running train loss: 0.68499]\u001b[A\n",
            "Trainging:   6%|▌         | 32/537 [02:13<33:55,  4.03s/it, running train loss: 0.66573]\u001b[A\n",
            "Trainging:   6%|▌         | 33/537 [02:13<33:02,  3.93s/it, running train loss: 0.66573]\u001b[A\n",
            "Trainging:   6%|▌         | 33/537 [02:16<33:02,  3.93s/it, running train loss: 0.70958]\u001b[A\n",
            "Trainging:   6%|▋         | 34/537 [02:16<31:51,  3.80s/it, running train loss: 0.70958]\u001b[A\n",
            "Trainging:   6%|▋         | 34/537 [02:20<31:51,  3.80s/it, running train loss: 0.54714]\u001b[A\n",
            "Trainging:   7%|▋         | 35/537 [02:20<31:25,  3.76s/it, running train loss: 0.54714]\u001b[A\n",
            "Trainging:   7%|▋         | 35/537 [02:24<31:25,  3.76s/it, running train loss: 0.59900]\u001b[A\n",
            "Trainging:   7%|▋         | 36/537 [02:24<32:01,  3.83s/it, running train loss: 0.59900]\u001b[A\n",
            "Trainging:   7%|▋         | 36/537 [02:27<32:01,  3.83s/it, running train loss: 0.63428]\u001b[A\n",
            "Trainging:   7%|▋         | 37/537 [02:27<30:45,  3.69s/it, running train loss: 0.63428]\u001b[A\n",
            "Trainging:   7%|▋         | 37/537 [02:31<30:45,  3.69s/it, running train loss: 0.68162]\u001b[A\n",
            "Trainging:   7%|▋         | 38/537 [02:31<30:41,  3.69s/it, running train loss: 0.68162]\u001b[A\n",
            "Trainging:   7%|▋         | 38/537 [02:35<30:41,  3.69s/it, running train loss: 0.63205]\u001b[A\n",
            "Trainging:   7%|▋         | 39/537 [02:35<30:02,  3.62s/it, running train loss: 0.63205]\u001b[A\n",
            "Trainging:   7%|▋         | 39/537 [02:38<30:02,  3.62s/it, running train loss: 0.59141]\u001b[A\n",
            "Trainging:   7%|▋         | 40/537 [02:38<30:24,  3.67s/it, running train loss: 0.59141]\u001b[A\n",
            "Trainging:   7%|▋         | 40/537 [02:42<30:24,  3.67s/it, running train loss: 0.62551]\u001b[A\n",
            "Trainging:   8%|▊         | 41/537 [02:42<31:15,  3.78s/it, running train loss: 0.62551]\u001b[A\n",
            "Trainging:   8%|▊         | 41/537 [02:46<31:15,  3.78s/it, running train loss: 0.59446]\u001b[A\n",
            "Trainging:   8%|▊         | 42/537 [02:46<31:44,  3.85s/it, running train loss: 0.59446]\u001b[A\n",
            "Trainging:   8%|▊         | 42/537 [02:50<31:44,  3.85s/it, running train loss: 0.58179]\u001b[A\n",
            "Trainging:   8%|▊         | 43/537 [02:50<30:49,  3.74s/it, running train loss: 0.58179]\u001b[A\n",
            "Trainging:   8%|▊         | 43/537 [02:54<30:49,  3.74s/it, running train loss: 0.63645]\u001b[A\n",
            "Trainging:   8%|▊         | 44/537 [02:54<31:17,  3.81s/it, running train loss: 0.63645]\u001b[A\n",
            "Trainging:   8%|▊         | 44/537 [02:58<31:17,  3.81s/it, running train loss: 0.67419]\u001b[A\n",
            "Trainging:   8%|▊         | 45/537 [02:58<30:50,  3.76s/it, running train loss: 0.67419]\u001b[A\n",
            "Trainging:   8%|▊         | 45/537 [03:01<30:50,  3.76s/it, running train loss: 0.59074]\u001b[A\n",
            "Trainging:   9%|▊         | 46/537 [03:01<31:10,  3.81s/it, running train loss: 0.59074]\u001b[A\n",
            "Trainging:   9%|▊         | 46/537 [03:05<31:10,  3.81s/it, running train loss: 0.52917]\u001b[A\n",
            "Trainging:   9%|▉         | 47/537 [03:05<30:45,  3.77s/it, running train loss: 0.52917]\u001b[A\n",
            "Trainging:   9%|▉         | 47/537 [03:07<30:45,  3.77s/it, running train loss: 0.72024]\u001b[A\n",
            "Trainging:   9%|▉         | 48/537 [03:07<26:42,  3.28s/it, running train loss: 0.72024]\u001b[A\n",
            "Trainging:   9%|▉         | 48/537 [03:11<26:42,  3.28s/it, running train loss: 0.47854]\u001b[A\n",
            "Trainging:   9%|▉         | 49/537 [03:11<27:40,  3.40s/it, running train loss: 0.47854]\u001b[A\n",
            "Trainging:   9%|▉         | 49/537 [03:15<27:40,  3.40s/it, running train loss: 0.70298]\u001b[A\n",
            "Trainging:   9%|▉         | 50/537 [03:15<28:51,  3.55s/it, running train loss: 0.70298]\u001b[A\n",
            "Trainging:   9%|▉         | 50/537 [03:18<28:51,  3.55s/it, running train loss: 0.65933]\u001b[A\n",
            "Trainging:   9%|▉         | 51/537 [03:18<28:38,  3.54s/it, running train loss: 0.65933]\u001b[A\n",
            "Trainging:   9%|▉         | 51/537 [03:22<28:38,  3.54s/it, running train loss: 0.54744]\u001b[A\n",
            "Trainging:  10%|▉         | 52/537 [03:22<29:23,  3.64s/it, running train loss: 0.54744]\u001b[A\n",
            "Trainging:  10%|▉         | 52/537 [03:26<29:23,  3.64s/it, running train loss: 0.69047]\u001b[A\n",
            "Trainging:  10%|▉         | 53/537 [03:26<30:04,  3.73s/it, running train loss: 0.69047]\u001b[A\n",
            "Trainging:  10%|▉         | 53/537 [03:30<30:04,  3.73s/it, running train loss: 0.65854]\u001b[A\n",
            "Trainging:  10%|█         | 54/537 [03:30<30:25,  3.78s/it, running train loss: 0.65854]\u001b[A\n",
            "Trainging:  10%|█         | 54/537 [03:34<30:25,  3.78s/it, running train loss: 0.56422]\u001b[A\n",
            "Trainging:  10%|█         | 55/537 [03:34<30:52,  3.84s/it, running train loss: 0.56422]\u001b[A\n",
            "Trainging:  10%|█         | 55/537 [03:38<30:52,  3.84s/it, running train loss: 0.64325]\u001b[A\n",
            "Trainging:  10%|█         | 56/537 [03:38<30:03,  3.75s/it, running train loss: 0.64325]\u001b[A\n",
            "Trainging:  10%|█         | 56/537 [03:41<30:03,  3.75s/it, running train loss: 0.56751]\u001b[A\n",
            "Trainging:  11%|█         | 57/537 [03:41<30:15,  3.78s/it, running train loss: 0.56751]\u001b[A\n",
            "Trainging:  11%|█         | 57/537 [03:45<30:15,  3.78s/it, running train loss: 0.61280]\u001b[A\n",
            "Trainging:  11%|█         | 58/537 [03:45<29:43,  3.72s/it, running train loss: 0.61280]\u001b[A\n",
            "Trainging:  11%|█         | 58/537 [03:49<29:43,  3.72s/it, running train loss: 0.56103]\u001b[A\n",
            "Trainging:  11%|█         | 59/537 [03:49<30:04,  3.78s/it, running train loss: 0.56103]\u001b[A\n",
            "Trainging:  11%|█         | 59/537 [03:53<30:04,  3.78s/it, running train loss: 0.57946]\u001b[A\n",
            "Trainging:  11%|█         | 60/537 [03:53<30:25,  3.83s/it, running train loss: 0.57946]\u001b[A\n",
            "Trainging:  11%|█         | 60/537 [03:57<30:25,  3.83s/it, running train loss: 0.55780]\u001b[A\n",
            "Trainging:  11%|█▏        | 61/537 [03:57<29:51,  3.76s/it, running train loss: 0.55780]\u001b[A\n",
            "Trainging:  11%|█▏        | 61/537 [04:00<29:51,  3.76s/it, running train loss: 0.74163]\u001b[A\n",
            "Trainging:  12%|█▏        | 62/537 [04:00<28:28,  3.60s/it, running train loss: 0.74163]\u001b[A\n",
            "Trainging:  12%|█▏        | 62/537 [04:03<28:28,  3.60s/it, running train loss: 0.60884]\u001b[A\n",
            "Trainging:  12%|█▏        | 63/537 [04:03<28:37,  3.62s/it, running train loss: 0.60884]\u001b[A\n",
            "Trainging:  12%|█▏        | 63/537 [04:07<28:37,  3.62s/it, running train loss: 0.50894]\u001b[A\n",
            "Trainging:  12%|█▏        | 64/537 [04:07<28:27,  3.61s/it, running train loss: 0.50894]\u001b[A\n",
            "Trainging:  12%|█▏        | 64/537 [04:10<28:27,  3.61s/it, running train loss: 0.52933]\u001b[A\n",
            "Trainging:  12%|█▏        | 65/537 [04:10<27:43,  3.53s/it, running train loss: 0.52933]\u001b[A\n",
            "Trainging:  12%|█▏        | 65/537 [04:14<27:43,  3.53s/it, running train loss: 0.63954]\u001b[A\n",
            "Trainging:  12%|█▏        | 66/537 [04:14<27:33,  3.51s/it, running train loss: 0.63954]\u001b[A\n",
            "Trainging:  12%|█▏        | 66/537 [04:18<27:33,  3.51s/it, running train loss: 0.64987]\u001b[A\n",
            "Trainging:  12%|█▏        | 67/537 [04:18<28:08,  3.59s/it, running train loss: 0.64987]\u001b[A\n",
            "Trainging:  12%|█▏        | 67/537 [04:22<28:08,  3.59s/it, running train loss: 0.65335]\u001b[A\n",
            "Trainging:  13%|█▎        | 68/537 [04:22<31:00,  3.97s/it, running train loss: 0.65335]\u001b[A\n",
            "Trainging:  13%|█▎        | 68/537 [04:27<31:00,  3.97s/it, running train loss: 0.55463]\u001b[A\n",
            "Trainging:  13%|█▎        | 69/537 [04:27<33:19,  4.27s/it, running train loss: 0.55463]\u001b[A\n",
            "Trainging:  13%|█▎        | 69/537 [04:31<33:19,  4.27s/it, running train loss: 0.61327]\u001b[A\n",
            "Trainging:  13%|█▎        | 70/537 [04:31<31:45,  4.08s/it, running train loss: 0.61327]\u001b[A\n",
            "Trainging:  13%|█▎        | 70/537 [04:34<31:45,  4.08s/it, running train loss: 0.61592]\u001b[A\n",
            "Trainging:  13%|█▎        | 71/537 [04:34<29:37,  3.82s/it, running train loss: 0.61592]\u001b[A\n",
            "Trainging:  13%|█▎        | 71/537 [04:39<29:37,  3.82s/it, running train loss: 0.63029]\u001b[A\n",
            "Trainging:  13%|█▎        | 72/537 [04:39<30:36,  3.95s/it, running train loss: 0.63029]\u001b[A\n",
            "Trainging:  13%|█▎        | 72/537 [04:45<30:36,  3.95s/it, running train loss: 0.61918]\u001b[A\n",
            "Trainging:  14%|█▎        | 73/537 [04:45<35:36,  4.60s/it, running train loss: 0.61918]\u001b[A\n",
            "Trainging:  14%|█▎        | 73/537 [04:48<35:36,  4.60s/it, running train loss: 0.66470]\u001b[A\n",
            "Trainging:  14%|█▍        | 74/537 [04:48<33:39,  4.36s/it, running train loss: 0.66470]\u001b[A\n",
            "Trainging:  14%|█▍        | 74/537 [04:53<33:39,  4.36s/it, running train loss: 0.64488]\u001b[A\n",
            "Trainging:  14%|█▍        | 75/537 [04:53<33:43,  4.38s/it, running train loss: 0.64488]\u001b[A\n",
            "Trainging:  14%|█▍        | 75/537 [04:56<33:43,  4.38s/it, running train loss: 0.66065]\u001b[A\n",
            "Trainging:  14%|█▍        | 76/537 [04:56<31:28,  4.10s/it, running train loss: 0.66065]\u001b[A\n",
            "Trainging:  14%|█▍        | 76/537 [05:00<31:28,  4.10s/it, running train loss: 0.56504]\u001b[A\n",
            "Trainging:  14%|█▍        | 77/537 [05:00<30:22,  3.96s/it, running train loss: 0.56504]\u001b[A\n",
            "Trainging:  14%|█▍        | 77/537 [05:06<30:22,  3.96s/it, running train loss: 0.55815]\u001b[A\n",
            "Trainging:  15%|█▍        | 78/537 [05:06<35:14,  4.61s/it, running train loss: 0.55815]\u001b[A\n",
            "Trainging:  15%|█▍        | 78/537 [05:10<35:14,  4.61s/it, running train loss: 0.53933]\u001b[A\n",
            "Trainging:  15%|█▍        | 79/537 [05:10<34:32,  4.53s/it, running train loss: 0.53933]\u001b[A\n",
            "Trainging:  15%|█▍        | 79/537 [05:14<34:32,  4.53s/it, running train loss: 0.53327]\u001b[A\n",
            "Trainging:  15%|█▍        | 80/537 [05:14<33:03,  4.34s/it, running train loss: 0.53327]\u001b[A\n",
            "Trainging:  15%|█▍        | 80/537 [05:18<33:03,  4.34s/it, running train loss: 0.63678]\u001b[A\n",
            "Trainging:  15%|█▌        | 81/537 [05:18<31:43,  4.18s/it, running train loss: 0.63678]\u001b[A\n",
            "Trainging:  15%|█▌        | 81/537 [05:22<31:43,  4.18s/it, running train loss: 0.60600]\u001b[A\n",
            "Trainging:  15%|█▌        | 82/537 [05:22<30:06,  3.97s/it, running train loss: 0.60600]\u001b[A\n",
            "Trainging:  15%|█▌        | 82/537 [05:25<30:06,  3.97s/it, running train loss: 0.50913]\u001b[A\n",
            "Trainging:  15%|█▌        | 83/537 [05:25<29:49,  3.94s/it, running train loss: 0.50913]\u001b[A\n",
            "Trainging:  15%|█▌        | 83/537 [05:30<29:49,  3.94s/it, running train loss: 0.55747]\u001b[A\n",
            "Trainging:  16%|█▌        | 84/537 [05:30<30:50,  4.08s/it, running train loss: 0.55747]\u001b[A\n",
            "Trainging:  16%|█▌        | 84/537 [05:34<30:50,  4.08s/it, running train loss: 0.53358]\u001b[A\n",
            "Trainging:  16%|█▌        | 85/537 [05:34<30:31,  4.05s/it, running train loss: 0.53358]\u001b[A\n",
            "Trainging:  16%|█▌        | 85/537 [05:37<30:31,  4.05s/it, running train loss: 0.57526]\u001b[A\n",
            "Trainging:  16%|█▌        | 86/537 [05:37<28:59,  3.86s/it, running train loss: 0.57526]\u001b[A\n",
            "Trainging:  16%|█▌        | 86/537 [05:41<28:59,  3.86s/it, running train loss: 0.51626]\u001b[A\n",
            "Trainging:  16%|█▌        | 87/537 [05:41<27:40,  3.69s/it, running train loss: 0.51626]\u001b[A\n",
            "Trainging:  16%|█▌        | 87/537 [05:44<27:40,  3.69s/it, running train loss: 0.55004]\u001b[A\n",
            "Trainging:  16%|█▋        | 88/537 [05:44<27:54,  3.73s/it, running train loss: 0.55004]\u001b[A\n",
            "Trainging:  16%|█▋        | 88/537 [05:48<27:54,  3.73s/it, running train loss: 0.68524]\u001b[A\n",
            "Trainging:  17%|█▋        | 89/537 [05:48<27:26,  3.67s/it, running train loss: 0.68524]\u001b[A\n",
            "Trainging:  17%|█▋        | 89/537 [05:51<27:26,  3.67s/it, running train loss: 0.50318]\u001b[A\n",
            "Trainging:  17%|█▋        | 90/537 [05:51<26:45,  3.59s/it, running train loss: 0.50318]\u001b[A\n",
            "Trainging:  17%|█▋        | 90/537 [05:56<26:45,  3.59s/it, running train loss: 0.57749]\u001b[A\n",
            "Trainging:  17%|█▋        | 91/537 [05:56<28:13,  3.80s/it, running train loss: 0.57749]\u001b[A\n",
            "Trainging:  17%|█▋        | 91/537 [05:59<28:13,  3.80s/it, running train loss: 0.54518]\u001b[A\n",
            "Trainging:  17%|█▋        | 92/537 [05:59<27:54,  3.76s/it, running train loss: 0.54518]\u001b[A\n",
            "Trainging:  17%|█▋        | 92/537 [06:05<27:54,  3.76s/it, running train loss: 0.58960]\u001b[A\n",
            "Trainging:  17%|█▋        | 93/537 [06:05<31:59,  4.32s/it, running train loss: 0.58960]\u001b[A\n",
            "Trainging:  17%|█▋        | 93/537 [06:09<31:59,  4.32s/it, running train loss: 0.55663]\u001b[A\n",
            "Trainging:  18%|█▊        | 94/537 [06:09<32:26,  4.39s/it, running train loss: 0.55663]\u001b[A\n",
            "Trainging:  18%|█▊        | 94/537 [06:13<32:26,  4.39s/it, running train loss: 0.74626]\u001b[A\n",
            "Trainging:  18%|█▊        | 95/537 [06:13<30:22,  4.12s/it, running train loss: 0.74626]\u001b[A\n",
            "Trainging:  18%|█▊        | 95/537 [06:18<30:22,  4.12s/it, running train loss: 0.61724]\u001b[A\n",
            "Trainging:  18%|█▊        | 96/537 [06:18<31:30,  4.29s/it, running train loss: 0.61724]\u001b[A\n",
            "Trainging:  18%|█▊        | 96/537 [06:20<31:30,  4.29s/it, running train loss: 0.58628]\u001b[A\n",
            "Trainging:  18%|█▊        | 97/537 [06:20<26:50,  3.66s/it, running train loss: 0.58628]\u001b[A\n",
            "Trainging:  18%|█▊        | 97/537 [06:24<26:50,  3.66s/it, running train loss: 0.58650]\u001b[A\n",
            "Trainging:  18%|█▊        | 98/537 [06:24<27:14,  3.72s/it, running train loss: 0.58650]\u001b[A\n",
            "Trainging:  18%|█▊        | 98/537 [06:27<27:14,  3.72s/it, running train loss: 0.59557]\u001b[A\n",
            "Trainging:  18%|█▊        | 99/537 [06:27<26:20,  3.61s/it, running train loss: 0.59557]\u001b[A\n",
            "Trainging:  18%|█▊        | 99/537 [06:32<26:20,  3.61s/it, running train loss: 0.55185]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|▏         | 1/68 [00:00<00:41,  1.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 2/68 [00:01<00:33,  1.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 3/68 [00:01<00:27,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 4/68 [00:01<00:27,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 5/68 [00:02<00:27,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 6/68 [00:02<00:24,  2.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|█         | 7/68 [00:02<00:24,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 8/68 [00:03<00:24,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 9/68 [00:03<00:24,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 10/68 [00:04<00:23,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 11/68 [00:04<00:23,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 12/68 [00:05<00:23,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 13/68 [00:05<00:23,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 14/68 [00:05<00:22,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 15/68 [00:06<00:22,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▎       | 16/68 [00:06<00:22,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▌       | 17/68 [00:07<00:20,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 18/68 [00:07<00:20,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 19/68 [00:07<00:18,  2.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 20/68 [00:08<00:18,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 21/68 [00:08<00:18,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 22/68 [00:09<00:17,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 23/68 [00:09<00:18,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▌      | 24/68 [00:09<00:16,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 25/68 [00:10<00:16,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 26/68 [00:10<00:17,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 27/68 [00:11<00:16,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████      | 28/68 [00:11<00:15,  2.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 29/68 [00:11<00:15,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 30/68 [00:12<00:15,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▌     | 31/68 [00:12<00:14,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 32/68 [00:13<00:14,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▊     | 33/68 [00:13<00:14,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 34/68 [00:13<00:13,  2.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████▏    | 35/68 [00:14<00:12,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 36/68 [00:14<00:12,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▍    | 37/68 [00:15<00:12,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 38/68 [00:15<00:12,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 39/68 [00:15<00:11,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▉    | 40/68 [00:16<00:11,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 41/68 [00:16<00:10,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 42/68 [00:17<00:10,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 43/68 [00:17<00:10,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▍   | 44/68 [00:18<00:10,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 45/68 [00:18<00:09,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 46/68 [00:18<00:09,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 47/68 [00:19<00:09,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 48/68 [00:19<00:08,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 49/68 [00:20<00:08,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 50/68 [00:20<00:07,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 51/68 [00:21<00:07,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▋  | 52/68 [00:21<00:06,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 53/68 [00:21<00:06,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 54/68 [00:22<00:06,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 55/68 [00:22<00:05,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 56/68 [00:23<00:04,  2.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 57/68 [00:23<00:04,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 58/68 [00:23<00:04,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 59/68 [00:24<00:03,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 60/68 [00:24<00:03,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|████████▉ | 61/68 [00:25<00:02,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 62/68 [00:25<00:02,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 63/68 [00:25<00:02,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 64/68 [00:26<00:01,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 65/68 [00:26<00:01,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 66/68 [00:27<00:00,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▊| 67/68 [00:27<00:00,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 68/68 [00:27<00:00,  2.44it/s]\n",
            "\n",
            "Trainging:  19%|█▊        | 100/537 [07:02<1:33:45, 12.87s/it, running train loss: 0.55185]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> training loss: 0.608127, valid loss: 0.581218,valid f1: 0.000000, valid acc:0.689991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Trainging:  19%|█▊        | 100/537 [07:06<1:33:45, 12.87s/it, running train loss: 0.67692]\u001b[A\n",
            "Trainging:  19%|█▉        | 101/537 [07:06<1:15:10, 10.35s/it, running train loss: 0.67692]\u001b[A\n",
            "Trainging:  19%|█▉        | 101/537 [07:10<1:15:10, 10.35s/it, running train loss: 0.57110]\u001b[A\n",
            "Trainging:  19%|█▉        | 102/537 [07:10<1:01:56,  8.54s/it, running train loss: 0.57110]\u001b[A\n",
            "Trainging:  19%|█▉        | 102/537 [07:14<1:01:56,  8.54s/it, running train loss: 0.53864]\u001b[A\n",
            "Trainging:  19%|█▉        | 103/537 [07:14<50:34,  6.99s/it, running train loss: 0.53864]  \u001b[A\n",
            "Trainging:  19%|█▉        | 103/537 [07:19<50:34,  6.99s/it, running train loss: 0.58431]\u001b[A\n",
            "Trainging:  19%|█▉        | 104/537 [07:19<46:18,  6.42s/it, running train loss: 0.58431]\u001b[A\n",
            "Trainging:  19%|█▉        | 104/537 [07:23<46:18,  6.42s/it, running train loss: 0.54549]\u001b[A\n",
            "Trainging:  20%|█▉        | 105/537 [07:23<41:52,  5.82s/it, running train loss: 0.54549]\u001b[A\n",
            "Trainging:  20%|█▉        | 105/537 [07:27<41:52,  5.82s/it, running train loss: 0.47662]\u001b[A\n",
            "Trainging:  20%|█▉        | 106/537 [07:27<37:30,  5.22s/it, running train loss: 0.47662]\u001b[A\n",
            "Trainging:  20%|█▉        | 106/537 [07:31<37:30,  5.22s/it, running train loss: 0.64241]\u001b[A\n",
            "Trainging:  20%|█▉        | 107/537 [07:31<34:46,  4.85s/it, running train loss: 0.64241]\u001b[A\n",
            "Trainging:  20%|█▉        | 107/537 [07:35<34:46,  4.85s/it, running train loss: 0.52789]\u001b[A\n",
            "Trainging:  20%|██        | 108/537 [07:35<32:27,  4.54s/it, running train loss: 0.52789]\u001b[A\n",
            "Trainging:  20%|██        | 108/537 [07:39<32:27,  4.54s/it, running train loss: 0.64398]\u001b[A\n",
            "Trainging:  20%|██        | 109/537 [07:39<31:48,  4.46s/it, running train loss: 0.64398]\u001b[A\n",
            "Trainging:  20%|██        | 109/537 [07:44<31:48,  4.46s/it, running train loss: 0.56635]\u001b[A\n",
            "Trainging:  20%|██        | 110/537 [07:44<31:46,  4.47s/it, running train loss: 0.56635]\u001b[A\n",
            "Trainging:  20%|██        | 110/537 [07:48<31:46,  4.47s/it, running train loss: 0.60863]\u001b[A\n",
            "Trainging:  21%|██        | 111/537 [07:48<30:39,  4.32s/it, running train loss: 0.60863]\u001b[A\n",
            "Trainging:  21%|██        | 111/537 [07:52<30:39,  4.32s/it, running train loss: 0.72356]\u001b[A\n",
            "Trainging:  21%|██        | 112/537 [07:52<30:57,  4.37s/it, running train loss: 0.72356]\u001b[A\n",
            "Trainging:  21%|██        | 112/537 [07:56<30:57,  4.37s/it, running train loss: 0.59658]\u001b[A\n",
            "Trainging:  21%|██        | 113/537 [07:56<29:42,  4.20s/it, running train loss: 0.59658]\u001b[A\n",
            "Trainging:  21%|██        | 113/537 [08:01<29:42,  4.20s/it, running train loss: 0.57366]\u001b[A\n",
            "Trainging:  21%|██        | 114/537 [08:01<31:18,  4.44s/it, running train loss: 0.57366]\u001b[A\n",
            "Trainging:  21%|██        | 114/537 [08:05<31:18,  4.44s/it, running train loss: 0.52382]\u001b[A\n",
            "Trainging:  21%|██▏       | 115/537 [08:05<30:09,  4.29s/it, running train loss: 0.52382]\u001b[A\n",
            "Trainging:  21%|██▏       | 115/537 [08:09<30:09,  4.29s/it, running train loss: 0.53027]\u001b[A\n",
            "Trainging:  22%|██▏       | 116/537 [08:09<30:36,  4.36s/it, running train loss: 0.53027]\u001b[A\n",
            "Trainging:  22%|██▏       | 116/537 [08:13<30:36,  4.36s/it, running train loss: 0.50848]\u001b[A\n",
            "Trainging:  22%|██▏       | 117/537 [08:13<28:25,  4.06s/it, running train loss: 0.50848]\u001b[A\n",
            "Trainging:  22%|██▏       | 117/537 [08:17<28:25,  4.06s/it, running train loss: 0.56866]\u001b[A\n",
            "Trainging:  22%|██▏       | 118/537 [08:17<29:00,  4.15s/it, running train loss: 0.56866]\u001b[A\n",
            "Trainging:  22%|██▏       | 118/537 [08:21<29:00,  4.15s/it, running train loss: 0.59035]\u001b[A\n",
            "Trainging:  22%|██▏       | 119/537 [08:21<27:43,  3.98s/it, running train loss: 0.59035]\u001b[A\n",
            "Trainging:  22%|██▏       | 119/537 [08:25<27:43,  3.98s/it, running train loss: 0.61634]\u001b[A\n",
            "Trainging:  22%|██▏       | 120/537 [08:25<28:52,  4.15s/it, running train loss: 0.61634]\u001b[A\n",
            "Trainging:  22%|██▏       | 120/537 [08:29<28:52,  4.15s/it, running train loss: 0.47614]\u001b[A\n",
            "Trainging:  23%|██▎       | 121/537 [08:29<27:12,  3.92s/it, running train loss: 0.47614]\u001b[A\n",
            "Trainging:  23%|██▎       | 121/537 [08:33<27:12,  3.92s/it, running train loss: 0.54330]\u001b[A\n",
            "Trainging:  23%|██▎       | 122/537 [08:33<27:15,  3.94s/it, running train loss: 0.54330]\u001b[A\n",
            "Trainging:  23%|██▎       | 122/537 [08:36<27:15,  3.94s/it, running train loss: 0.62758]\u001b[A\n",
            "Trainging:  23%|██▎       | 123/537 [08:36<26:36,  3.86s/it, running train loss: 0.62758]\u001b[A\n",
            "Trainging:  23%|██▎       | 123/537 [08:40<26:36,  3.86s/it, running train loss: 0.46256]\u001b[A\n",
            "Trainging:  23%|██▎       | 124/537 [08:40<25:38,  3.72s/it, running train loss: 0.46256]\u001b[A\n",
            "Trainging:  23%|██▎       | 124/537 [08:43<25:38,  3.72s/it, running train loss: 0.55726]\u001b[A\n",
            "Trainging:  23%|██▎       | 125/537 [08:43<25:48,  3.76s/it, running train loss: 0.55726]\u001b[A\n",
            "Trainging:  23%|██▎       | 125/537 [08:47<25:48,  3.76s/it, running train loss: 0.62924]\u001b[A\n",
            "Trainging:  23%|██▎       | 126/537 [08:47<25:30,  3.72s/it, running train loss: 0.62924]\u001b[A\n",
            "Trainging:  23%|██▎       | 126/537 [08:49<25:30,  3.72s/it, running train loss: 0.54638]\u001b[A\n",
            "Trainging:  24%|██▎       | 127/537 [08:49<22:12,  3.25s/it, running train loss: 0.54638]\u001b[A\n",
            "Trainging:  24%|██▎       | 127/537 [08:53<22:12,  3.25s/it, running train loss: 0.51477]\u001b[A\n",
            "Trainging:  24%|██▍       | 128/537 [08:53<22:25,  3.29s/it, running train loss: 0.51477]\u001b[A\n",
            "Trainging:  24%|██▍       | 128/537 [08:56<22:25,  3.29s/it, running train loss: 0.68431]\u001b[A\n",
            "Trainging:  24%|██▍       | 129/537 [08:56<23:12,  3.41s/it, running train loss: 0.68431]\u001b[A\n",
            "Trainging:  24%|██▍       | 129/537 [09:00<23:12,  3.41s/it, running train loss: 0.55130]\u001b[A\n",
            "Trainging:  24%|██▍       | 130/537 [09:00<24:14,  3.57s/it, running train loss: 0.55130]\u001b[A\n",
            "Trainging:  24%|██▍       | 130/537 [09:04<24:14,  3.57s/it, running train loss: 0.52855]\u001b[A\n",
            "Trainging:  24%|██▍       | 131/537 [09:04<24:11,  3.58s/it, running train loss: 0.52855]\u001b[A\n",
            "Trainging:  24%|██▍       | 131/537 [09:08<24:11,  3.58s/it, running train loss: 0.55400]\u001b[A\n",
            "Trainging:  25%|██▍       | 132/537 [09:08<24:33,  3.64s/it, running train loss: 0.55400]\u001b[A\n",
            "Trainging:  25%|██▍       | 132/537 [09:11<24:33,  3.64s/it, running train loss: 0.66429]\u001b[A\n",
            "Trainging:  25%|██▍       | 133/537 [09:11<24:15,  3.60s/it, running train loss: 0.66429]\u001b[A\n",
            "Trainging:  25%|██▍       | 133/537 [09:14<24:15,  3.60s/it, running train loss: 0.67925]\u001b[A\n",
            "Trainging:  25%|██▍       | 134/537 [09:14<23:35,  3.51s/it, running train loss: 0.67925]\u001b[A\n",
            "Trainging:  25%|██▍       | 134/537 [09:19<23:35,  3.51s/it, running train loss: 0.65358]\u001b[A\n",
            "Trainging:  25%|██▌       | 135/537 [09:19<25:10,  3.76s/it, running train loss: 0.65358]\u001b[A\n",
            "Trainging:  25%|██▌       | 135/537 [09:22<25:10,  3.76s/it, running train loss: 0.54564]\u001b[A\n",
            "Trainging:  25%|██▌       | 136/537 [09:22<24:46,  3.71s/it, running train loss: 0.54564]\u001b[A\n",
            "Trainging:  25%|██▌       | 136/537 [09:26<24:46,  3.71s/it, running train loss: 0.59897]\u001b[A\n",
            "Trainging:  26%|██▌       | 137/537 [09:26<23:54,  3.59s/it, running train loss: 0.59897]\u001b[A\n",
            "Trainging:  26%|██▌       | 137/537 [09:32<23:54,  3.59s/it, running train loss: 0.62508]\u001b[A\n",
            "Trainging:  26%|██▌       | 138/537 [09:32<28:47,  4.33s/it, running train loss: 0.62508]\u001b[A\n",
            "Trainging:  26%|██▌       | 138/537 [09:36<28:47,  4.33s/it, running train loss: 0.51550]\u001b[A\n",
            "Trainging:  26%|██▌       | 139/537 [09:36<28:02,  4.23s/it, running train loss: 0.51550]\u001b[A\n",
            "Trainging:  26%|██▌       | 139/537 [09:40<28:02,  4.23s/it, running train loss: 0.56875]\u001b[A\n",
            "Trainging:  26%|██▌       | 140/537 [09:40<28:29,  4.31s/it, running train loss: 0.56875]\u001b[A\n",
            "Trainging:  26%|██▌       | 140/537 [09:45<28:29,  4.31s/it, running train loss: 0.53700]\u001b[A\n",
            "Trainging:  26%|██▋       | 141/537 [09:45<29:16,  4.44s/it, running train loss: 0.53700]\u001b[A\n",
            "Trainging:  26%|██▋       | 141/537 [09:48<29:16,  4.44s/it, running train loss: 0.51579]\u001b[A\n",
            "Trainging:  26%|██▋       | 142/537 [09:48<27:24,  4.16s/it, running train loss: 0.51579]\u001b[A\n",
            "Trainging:  26%|██▋       | 142/537 [09:52<27:24,  4.16s/it, running train loss: 0.57816]\u001b[A\n",
            "Trainging:  27%|██▋       | 143/537 [09:52<26:15,  4.00s/it, running train loss: 0.57816]\u001b[A\n",
            "Trainging:  27%|██▋       | 143/537 [09:56<26:15,  4.00s/it, running train loss: 0.54613]\u001b[A\n",
            "Trainging:  27%|██▋       | 144/537 [09:56<25:21,  3.87s/it, running train loss: 0.54613]\u001b[A\n",
            "Trainging:  27%|██▋       | 144/537 [09:59<25:21,  3.87s/it, running train loss: 0.57638]\u001b[A\n",
            "Trainging:  27%|██▋       | 145/537 [09:59<24:39,  3.77s/it, running train loss: 0.57638]\u001b[A\n",
            "Trainging:  27%|██▋       | 145/537 [10:03<24:39,  3.77s/it, running train loss: 0.59712]\u001b[A\n",
            "Trainging:  27%|██▋       | 146/537 [10:03<24:43,  3.80s/it, running train loss: 0.59712]\u001b[A\n",
            "Trainging:  27%|██▋       | 146/537 [10:07<24:43,  3.80s/it, running train loss: 0.59399]\u001b[A\n",
            "Trainging:  27%|██▋       | 147/537 [10:07<24:52,  3.83s/it, running train loss: 0.59399]\u001b[A\n",
            "Trainging:  27%|██▋       | 147/537 [10:12<24:52,  3.83s/it, running train loss: 0.61446]\u001b[A\n",
            "Trainging:  28%|██▊       | 148/537 [10:12<27:52,  4.30s/it, running train loss: 0.61446]\u001b[A\n",
            "Trainging:  28%|██▊       | 148/537 [10:16<27:52,  4.30s/it, running train loss: 0.57428]\u001b[A\n",
            "Trainging:  28%|██▊       | 149/537 [10:16<26:31,  4.10s/it, running train loss: 0.57428]\u001b[A\n",
            "Trainging:  28%|██▊       | 149/537 [10:20<26:31,  4.10s/it, running train loss: 0.45245]\u001b[A\n",
            "Trainging:  28%|██▊       | 150/537 [10:20<26:02,  4.04s/it, running train loss: 0.45245]\u001b[A\n",
            "Trainging:  28%|██▊       | 150/537 [10:23<26:02,  4.04s/it, running train loss: 0.52694]\u001b[A\n",
            "Trainging:  28%|██▊       | 151/537 [10:23<24:54,  3.87s/it, running train loss: 0.52694]\u001b[A\n",
            "Trainging:  28%|██▊       | 151/537 [10:28<24:54,  3.87s/it, running train loss: 0.45722]\u001b[A\n",
            "Trainging:  28%|██▊       | 152/537 [10:28<25:36,  3.99s/it, running train loss: 0.45722]\u001b[A\n",
            "Trainging:  28%|██▊       | 152/537 [10:32<25:36,  3.99s/it, running train loss: 0.59440]\u001b[A\n",
            "Trainging:  28%|██▊       | 153/537 [10:32<25:25,  3.97s/it, running train loss: 0.59440]\u001b[A\n",
            "Trainging:  28%|██▊       | 153/537 [10:35<25:25,  3.97s/it, running train loss: 0.57010]\u001b[A\n",
            "Trainging:  29%|██▊       | 154/537 [10:35<24:32,  3.85s/it, running train loss: 0.57010]\u001b[A\n",
            "Trainging:  29%|██▊       | 154/537 [10:39<24:32,  3.85s/it, running train loss: 0.65090]\u001b[A\n",
            "Trainging:  29%|██▉       | 155/537 [10:39<23:43,  3.73s/it, running train loss: 0.65090]\u001b[A\n",
            "Trainging:  29%|██▉       | 155/537 [10:42<23:43,  3.73s/it, running train loss: 0.62600]\u001b[A\n",
            "Trainging:  29%|██▉       | 156/537 [10:42<23:08,  3.64s/it, running train loss: 0.62600]\u001b[A\n",
            "Trainging:  29%|██▉       | 156/537 [10:46<23:08,  3.64s/it, running train loss: 0.53213]\u001b[A\n",
            "Trainging:  29%|██▉       | 157/537 [10:46<23:05,  3.65s/it, running train loss: 0.53213]\u001b[A\n",
            "Trainging:  29%|██▉       | 157/537 [10:50<23:05,  3.65s/it, running train loss: 0.39908]\u001b[A\n",
            "Trainging:  29%|██▉       | 158/537 [10:50<23:38,  3.74s/it, running train loss: 0.39908]\u001b[A\n",
            "Trainging:  29%|██▉       | 158/537 [10:53<23:38,  3.74s/it, running train loss: 0.66637]\u001b[A\n",
            "Trainging:  30%|██▉       | 159/537 [10:53<23:05,  3.66s/it, running train loss: 0.66637]\u001b[A\n",
            "Trainging:  30%|██▉       | 159/537 [10:57<23:05,  3.66s/it, running train loss: 0.64026]\u001b[A\n",
            "Trainging:  30%|██▉       | 160/537 [10:57<23:37,  3.76s/it, running train loss: 0.64026]\u001b[A\n",
            "Trainging:  30%|██▉       | 160/537 [11:01<23:37,  3.76s/it, running train loss: 0.51049]\u001b[A\n",
            "Trainging:  30%|██▉       | 161/537 [11:01<23:49,  3.80s/it, running train loss: 0.51049]\u001b[A\n",
            "Trainging:  30%|██▉       | 161/537 [11:06<23:49,  3.80s/it, running train loss: 0.60485]\u001b[A\n",
            "Trainging:  30%|███       | 162/537 [11:06<25:07,  4.02s/it, running train loss: 0.60485]\u001b[A\n",
            "Trainging:  30%|███       | 162/537 [11:09<25:07,  4.02s/it, running train loss: 0.54971]\u001b[A\n",
            "Trainging:  30%|███       | 163/537 [11:09<24:39,  3.96s/it, running train loss: 0.54971]\u001b[A\n",
            "Trainging:  30%|███       | 163/537 [11:13<24:39,  3.96s/it, running train loss: 0.58819]\u001b[A\n",
            "Trainging:  31%|███       | 164/537 [11:13<24:06,  3.88s/it, running train loss: 0.58819]\u001b[A\n",
            "Trainging:  31%|███       | 164/537 [11:17<24:06,  3.88s/it, running train loss: 0.53630]\u001b[A\n",
            "Trainging:  31%|███       | 165/537 [11:17<23:25,  3.78s/it, running train loss: 0.53630]\u001b[A\n",
            "Trainging:  31%|███       | 165/537 [11:20<23:25,  3.78s/it, running train loss: 0.53293]\u001b[A\n",
            "Trainging:  31%|███       | 166/537 [11:20<22:44,  3.68s/it, running train loss: 0.53293]\u001b[A\n",
            "Trainging:  31%|███       | 166/537 [11:24<22:44,  3.68s/it, running train loss: 0.50455]\u001b[A\n",
            "Trainging:  31%|███       | 167/537 [11:24<22:41,  3.68s/it, running train loss: 0.50455]\u001b[A\n",
            "Trainging:  31%|███       | 167/537 [11:27<22:41,  3.68s/it, running train loss: 0.51482]\u001b[A\n",
            "Trainging:  31%|███▏      | 168/537 [11:27<22:42,  3.69s/it, running train loss: 0.51482]\u001b[A\n",
            "Trainging:  31%|███▏      | 168/537 [11:31<22:42,  3.69s/it, running train loss: 0.46498]\u001b[A\n",
            "Trainging:  31%|███▏      | 169/537 [11:31<22:07,  3.61s/it, running train loss: 0.46498]\u001b[A\n",
            "Trainging:  31%|███▏      | 169/537 [11:37<22:07,  3.61s/it, running train loss: 0.68251]\u001b[A\n",
            "Trainging:  32%|███▏      | 170/537 [11:37<26:10,  4.28s/it, running train loss: 0.68251]\u001b[A\n",
            "Trainging:  32%|███▏      | 170/537 [11:41<26:10,  4.28s/it, running train loss: 0.58385]\u001b[A\n",
            "Trainging:  32%|███▏      | 171/537 [11:41<25:33,  4.19s/it, running train loss: 0.58385]\u001b[A\n",
            "Trainging:  32%|███▏      | 171/537 [11:45<25:33,  4.19s/it, running train loss: 0.58950]\u001b[A\n",
            "Trainging:  32%|███▏      | 172/537 [11:45<26:01,  4.28s/it, running train loss: 0.58950]\u001b[A\n",
            "Trainging:  32%|███▏      | 172/537 [11:49<26:01,  4.28s/it, running train loss: 0.51311]\u001b[A\n",
            "Trainging:  32%|███▏      | 173/537 [11:49<24:46,  4.08s/it, running train loss: 0.51311]\u001b[A\n",
            "Trainging:  32%|███▏      | 173/537 [11:52<24:46,  4.08s/it, running train loss: 0.53718]\u001b[A\n",
            "Trainging:  32%|███▏      | 174/537 [11:52<23:38,  3.91s/it, running train loss: 0.53718]\u001b[A\n",
            "Trainging:  32%|███▏      | 174/537 [11:57<23:38,  3.91s/it, running train loss: 0.55749]\u001b[A\n",
            "Trainging:  33%|███▎      | 175/537 [11:57<24:14,  4.02s/it, running train loss: 0.55749]\u001b[A\n",
            "Trainging:  33%|███▎      | 175/537 [12:00<24:14,  4.02s/it, running train loss: 0.49639]\u001b[A\n",
            "Trainging:  33%|███▎      | 176/537 [12:00<23:09,  3.85s/it, running train loss: 0.49639]\u001b[A\n",
            "Trainging:  33%|███▎      | 176/537 [12:04<23:09,  3.85s/it, running train loss: 0.52997]\u001b[A\n",
            "Trainging:  33%|███▎      | 177/537 [12:04<23:58,  4.00s/it, running train loss: 0.52997]\u001b[A\n",
            "Trainging:  33%|███▎      | 177/537 [12:08<23:58,  4.00s/it, running train loss: 0.48725]\u001b[A\n",
            "Trainging:  33%|███▎      | 178/537 [12:08<23:36,  3.95s/it, running train loss: 0.48725]\u001b[A\n",
            "Trainging:  33%|███▎      | 178/537 [12:13<23:36,  3.95s/it, running train loss: 0.61684]\u001b[A\n",
            "Trainging:  33%|███▎      | 179/537 [12:13<25:16,  4.24s/it, running train loss: 0.61684]\u001b[A\n",
            "Trainging:  33%|███▎      | 179/537 [12:18<25:16,  4.24s/it, running train loss: 0.59739]\u001b[A\n",
            "Trainging:  34%|███▎      | 180/537 [12:18<25:53,  4.35s/it, running train loss: 0.59739]\u001b[A\n",
            "Trainging:  34%|███▎      | 180/537 [12:22<25:53,  4.35s/it, running train loss: 0.45736]\u001b[A\n",
            "Trainging:  34%|███▎      | 181/537 [12:22<24:54,  4.20s/it, running train loss: 0.45736]\u001b[A\n",
            "Trainging:  34%|███▎      | 181/537 [12:25<24:54,  4.20s/it, running train loss: 0.61877]\u001b[A\n",
            "Trainging:  34%|███▍      | 182/537 [12:25<24:06,  4.08s/it, running train loss: 0.61877]\u001b[A\n",
            "Trainging:  34%|███▍      | 182/537 [12:29<24:06,  4.08s/it, running train loss: 0.56133]\u001b[A\n",
            "Trainging:  34%|███▍      | 183/537 [12:29<23:34,  4.00s/it, running train loss: 0.56133]\u001b[A\n",
            "Trainging:  34%|███▍      | 183/537 [12:33<23:34,  4.00s/it, running train loss: 0.58876]\u001b[A\n",
            "Trainging:  34%|███▍      | 184/537 [12:33<22:37,  3.84s/it, running train loss: 0.58876]\u001b[A\n",
            "Trainging:  34%|███▍      | 184/537 [12:38<22:37,  3.84s/it, running train loss: 0.47648]\u001b[A\n",
            "Trainging:  34%|███▍      | 185/537 [12:38<24:45,  4.22s/it, running train loss: 0.47648]\u001b[A\n",
            "Trainging:  34%|███▍      | 185/537 [12:41<24:45,  4.22s/it, running train loss: 0.56303]\u001b[A\n",
            "Trainging:  35%|███▍      | 186/537 [12:41<23:30,  4.02s/it, running train loss: 0.56303]\u001b[A\n",
            "Trainging:  35%|███▍      | 186/537 [12:45<23:30,  4.02s/it, running train loss: 0.57499]\u001b[A\n",
            "Trainging:  35%|███▍      | 187/537 [12:45<23:24,  4.01s/it, running train loss: 0.57499]\u001b[A\n",
            "Trainging:  35%|███▍      | 187/537 [12:49<23:24,  4.01s/it, running train loss: 0.51828]\u001b[A\n",
            "Trainging:  35%|███▌      | 188/537 [12:49<23:16,  4.00s/it, running train loss: 0.51828]\u001b[A\n",
            "Trainging:  35%|███▌      | 188/537 [12:54<23:16,  4.00s/it, running train loss: 0.50975]\u001b[A\n",
            "Trainging:  35%|███▌      | 189/537 [12:54<24:32,  4.23s/it, running train loss: 0.50975]\u001b[A\n",
            "Trainging:  35%|███▌      | 189/537 [13:00<24:32,  4.23s/it, running train loss: 0.64016]\u001b[A\n",
            "Trainging:  35%|███▌      | 190/537 [13:00<27:41,  4.79s/it, running train loss: 0.64016]\u001b[A\n",
            "Trainging:  35%|███▌      | 190/537 [13:06<27:41,  4.79s/it, running train loss: 0.56470]\u001b[A\n",
            "Trainging:  36%|███▌      | 191/537 [13:06<29:13,  5.07s/it, running train loss: 0.56470]\u001b[A\n",
            "Trainging:  36%|███▌      | 191/537 [13:10<29:13,  5.07s/it, running train loss: 0.55084]\u001b[A\n",
            "Trainging:  36%|███▌      | 192/537 [13:10<27:17,  4.75s/it, running train loss: 0.55084]\u001b[A\n",
            "Trainging:  36%|███▌      | 192/537 [13:13<27:17,  4.75s/it, running train loss: 0.52823]\u001b[A\n",
            "Trainging:  36%|███▌      | 193/537 [13:13<25:06,  4.38s/it, running train loss: 0.52823]\u001b[A\n",
            "Trainging:  36%|███▌      | 193/537 [13:19<25:06,  4.38s/it, running train loss: 0.55915]\u001b[A\n",
            "Trainging:  36%|███▌      | 194/537 [13:19<26:41,  4.67s/it, running train loss: 0.55915]\u001b[A\n",
            "Trainging:  36%|███▌      | 194/537 [13:22<26:41,  4.67s/it, running train loss: 0.60830]\u001b[A\n",
            "Trainging:  36%|███▋      | 195/537 [13:22<24:28,  4.29s/it, running train loss: 0.60830]\u001b[A\n",
            "Trainging:  36%|███▋      | 195/537 [13:26<24:28,  4.29s/it, running train loss: 0.52755]\u001b[A\n",
            "Trainging:  36%|███▋      | 196/537 [13:26<23:18,  4.10s/it, running train loss: 0.52755]\u001b[A\n",
            "Trainging:  36%|███▋      | 196/537 [13:30<23:18,  4.10s/it, running train loss: 0.47792]\u001b[A\n",
            "Trainging:  37%|███▋      | 197/537 [13:30<22:47,  4.02s/it, running train loss: 0.47792]\u001b[A\n",
            "Trainging:  37%|███▋      | 197/537 [13:33<22:47,  4.02s/it, running train loss: 0.55694]\u001b[A\n",
            "Trainging:  37%|███▋      | 198/537 [13:33<22:01,  3.90s/it, running train loss: 0.55694]\u001b[A\n",
            "Trainging:  37%|███▋      | 198/537 [13:37<22:01,  3.90s/it, running train loss: 0.53899]\u001b[A\n",
            "Trainging:  37%|███▋      | 199/537 [13:37<22:06,  3.93s/it, running train loss: 0.53899]\u001b[A\n",
            "Trainging:  37%|███▋      | 199/537 [13:41<22:06,  3.93s/it, running train loss: 0.58712]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|▏         | 1/68 [00:00<00:39,  1.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 2/68 [00:01<00:33,  1.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 3/68 [00:01<00:28,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 4/68 [00:01<00:27,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 5/68 [00:02<00:27,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 6/68 [00:02<00:24,  2.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|█         | 7/68 [00:02<00:24,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 8/68 [00:03<00:24,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 9/68 [00:03<00:24,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 10/68 [00:04<00:23,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 11/68 [00:04<00:23,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 12/68 [00:05<00:23,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 13/68 [00:05<00:23,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 14/68 [00:05<00:23,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 15/68 [00:06<00:22,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▎       | 16/68 [00:06<00:22,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▌       | 17/68 [00:07<00:20,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 18/68 [00:07<00:20,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 19/68 [00:07<00:18,  2.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 20/68 [00:08<00:18,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 21/68 [00:08<00:18,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 22/68 [00:09<00:17,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 23/68 [00:09<00:18,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▌      | 24/68 [00:09<00:16,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 25/68 [00:10<00:17,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 26/68 [00:10<00:17,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 27/68 [00:11<00:16,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████      | 28/68 [00:11<00:15,  2.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 29/68 [00:11<00:15,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 30/68 [00:12<00:15,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▌     | 31/68 [00:12<00:14,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 32/68 [00:13<00:14,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▊     | 33/68 [00:13<00:14,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 34/68 [00:13<00:13,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████▏    | 35/68 [00:14<00:12,  2.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 36/68 [00:14<00:12,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▍    | 37/68 [00:15<00:12,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 38/68 [00:15<00:12,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 39/68 [00:15<00:11,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▉    | 40/68 [00:16<00:11,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 41/68 [00:16<00:10,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 42/68 [00:17<00:10,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 43/68 [00:17<00:10,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▍   | 44/68 [00:18<00:10,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 45/68 [00:18<00:09,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 46/68 [00:18<00:09,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 47/68 [00:19<00:09,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 48/68 [00:19<00:08,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 49/68 [00:20<00:08,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 50/68 [00:20<00:07,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 51/68 [00:21<00:07,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▋  | 52/68 [00:21<00:06,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 53/68 [00:21<00:06,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 54/68 [00:22<00:06,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 55/68 [00:22<00:05,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 56/68 [00:23<00:04,  2.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 57/68 [00:23<00:04,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 58/68 [00:23<00:04,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 59/68 [00:24<00:03,  2.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 60/68 [00:24<00:03,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|████████▉ | 61/68 [00:25<00:02,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 62/68 [00:25<00:02,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 63/68 [00:25<00:02,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 64/68 [00:26<00:01,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 65/68 [00:26<00:01,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 66/68 [00:27<00:00,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▊| 67/68 [00:27<00:00,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 68/68 [00:27<00:00,  2.44it/s]\n",
            "\n",
            "Trainging:  37%|███▋      | 200/537 [14:09<1:09:40, 12.40s/it, running train loss: 0.58712]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> training loss: 0.563966, valid loss: 0.538982,valid f1: 0.000000, valid acc:0.689991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Trainging:  37%|███▋      | 200/537 [14:13<1:09:40, 12.40s/it, running train loss: 0.46188]\u001b[A\n",
            "Trainging:  37%|███▋      | 201/537 [14:13<55:11,  9.86s/it, running train loss: 0.46188]  \u001b[A\n",
            "Trainging:  37%|███▋      | 201/537 [14:17<55:11,  9.86s/it, running train loss: 0.42793]\u001b[A\n",
            "Trainging:  38%|███▊      | 202/537 [14:17<45:11,  8.09s/it, running train loss: 0.42793]\u001b[A\n",
            "Trainging:  38%|███▊      | 202/537 [14:21<45:11,  8.09s/it, running train loss: 0.55234]\u001b[A\n",
            "Trainging:  38%|███▊      | 203/537 [14:21<38:09,  6.86s/it, running train loss: 0.55234]\u001b[A\n",
            "Trainging:  38%|███▊      | 203/537 [14:25<38:09,  6.86s/it, running train loss: 0.59766]\u001b[A\n",
            "Trainging:  38%|███▊      | 204/537 [14:25<32:31,  5.86s/it, running train loss: 0.59766]\u001b[A\n",
            "Trainging:  38%|███▊      | 204/537 [14:29<32:31,  5.86s/it, running train loss: 0.49247]\u001b[A\n",
            "Trainging:  38%|███▊      | 205/537 [14:29<30:14,  5.46s/it, running train loss: 0.49247]\u001b[A\n",
            "Trainging:  38%|███▊      | 205/537 [14:33<30:14,  5.46s/it, running train loss: 0.62042]\u001b[A\n",
            "Trainging:  38%|███▊      | 206/537 [14:33<27:32,  4.99s/it, running train loss: 0.62042]\u001b[A\n",
            "Trainging:  38%|███▊      | 206/537 [14:37<27:32,  4.99s/it, running train loss: 0.55874]\u001b[A\n",
            "Trainging:  39%|███▊      | 207/537 [14:37<24:54,  4.53s/it, running train loss: 0.55874]\u001b[A\n",
            "Trainging:  39%|███▊      | 207/537 [14:41<24:54,  4.53s/it, running train loss: 0.54519]\u001b[A\n",
            "Trainging:  39%|███▊      | 208/537 [14:41<23:50,  4.35s/it, running train loss: 0.54519]\u001b[A\n",
            "Trainging:  39%|███▊      | 208/537 [14:44<23:50,  4.35s/it, running train loss: 0.61666]\u001b[A\n",
            "Trainging:  39%|███▉      | 209/537 [14:44<21:53,  4.00s/it, running train loss: 0.61666]\u001b[A\n",
            "Trainging:  39%|███▉      | 209/537 [14:48<21:53,  4.00s/it, running train loss: 0.55944]\u001b[A\n",
            "Trainging:  39%|███▉      | 210/537 [14:48<21:35,  3.96s/it, running train loss: 0.55944]\u001b[A\n",
            "Trainging:  39%|███▉      | 210/537 [14:51<21:35,  3.96s/it, running train loss: 0.56601]\u001b[A\n",
            "Trainging:  39%|███▉      | 211/537 [14:51<21:16,  3.92s/it, running train loss: 0.56601]\u001b[A\n",
            "Trainging:  39%|███▉      | 211/537 [14:55<21:16,  3.92s/it, running train loss: 0.57983]\u001b[A\n",
            "Trainging:  39%|███▉      | 212/537 [14:55<20:50,  3.85s/it, running train loss: 0.57983]\u001b[A\n",
            "Trainging:  39%|███▉      | 212/537 [14:59<20:50,  3.85s/it, running train loss: 0.56149]\u001b[A\n",
            "Trainging:  40%|███▉      | 213/537 [14:59<19:57,  3.70s/it, running train loss: 0.56149]\u001b[A\n",
            "Trainging:  40%|███▉      | 213/537 [15:02<19:57,  3.70s/it, running train loss: 0.55717]\u001b[A\n",
            "Trainging:  40%|███▉      | 214/537 [15:03<20:21,  3.78s/it, running train loss: 0.55717]\u001b[A\n",
            "Trainging:  40%|███▉      | 214/537 [15:07<20:21,  3.78s/it, running train loss: 0.53874]\u001b[A\n",
            "Trainging:  40%|████      | 215/537 [15:07<21:20,  3.98s/it, running train loss: 0.53874]\u001b[A\n",
            "Trainging:  40%|████      | 215/537 [15:11<21:20,  3.98s/it, running train loss: 0.54556]\u001b[A\n",
            "Trainging:  40%|████      | 216/537 [15:11<21:44,  4.07s/it, running train loss: 0.54556]\u001b[A\n",
            "Trainging:  40%|████      | 216/537 [15:15<21:44,  4.07s/it, running train loss: 0.57758]\u001b[A\n",
            "Trainging:  40%|████      | 217/537 [15:15<21:25,  4.02s/it, running train loss: 0.57758]\u001b[A\n",
            "Trainging:  40%|████      | 217/537 [15:20<21:25,  4.02s/it, running train loss: 0.58933]\u001b[A\n",
            "Trainging:  41%|████      | 218/537 [15:20<22:08,  4.16s/it, running train loss: 0.58933]\u001b[A\n",
            "Trainging:  41%|████      | 218/537 [15:24<22:08,  4.16s/it, running train loss: 0.50676]\u001b[A\n",
            "Trainging:  41%|████      | 219/537 [15:24<22:21,  4.22s/it, running train loss: 0.50676]\u001b[A\n",
            "Trainging:  41%|████      | 219/537 [15:28<22:21,  4.22s/it, running train loss: 0.53773]\u001b[A\n",
            "Trainging:  41%|████      | 220/537 [15:28<21:17,  4.03s/it, running train loss: 0.53773]\u001b[A\n",
            "Trainging:  41%|████      | 220/537 [15:31<21:17,  4.03s/it, running train loss: 0.58251]\u001b[A\n",
            "Trainging:  41%|████      | 221/537 [15:31<20:56,  3.97s/it, running train loss: 0.58251]\u001b[A\n",
            "Trainging:  41%|████      | 221/537 [15:35<20:56,  3.97s/it, running train loss: 0.45074]\u001b[A\n",
            "Trainging:  41%|████▏     | 222/537 [15:35<20:39,  3.93s/it, running train loss: 0.45074]\u001b[A\n",
            "Trainging:  41%|████▏     | 222/537 [15:39<20:39,  3.93s/it, running train loss: 0.56540]\u001b[A\n",
            "Trainging:  42%|████▏     | 223/537 [15:39<20:30,  3.92s/it, running train loss: 0.56540]\u001b[A\n",
            "Trainging:  42%|████▏     | 223/537 [15:43<20:30,  3.92s/it, running train loss: 0.45551]\u001b[A\n",
            "Trainging:  42%|████▏     | 224/537 [15:43<20:13,  3.88s/it, running train loss: 0.45551]\u001b[A\n",
            "Trainging:  42%|████▏     | 224/537 [15:47<20:13,  3.88s/it, running train loss: 0.60034]\u001b[A\n",
            "Trainging:  42%|████▏     | 225/537 [15:47<20:21,  3.91s/it, running train loss: 0.60034]\u001b[A\n",
            "Trainging:  42%|████▏     | 225/537 [15:51<20:21,  3.91s/it, running train loss: 0.51041]\u001b[A\n",
            "Trainging:  42%|████▏     | 226/537 [15:51<20:16,  3.91s/it, running train loss: 0.51041]\u001b[A\n",
            "Trainging:  42%|████▏     | 226/537 [15:55<20:16,  3.91s/it, running train loss: 0.58900]\u001b[A\n",
            "Trainging:  42%|████▏     | 227/537 [15:55<21:19,  4.13s/it, running train loss: 0.58900]\u001b[A\n",
            "Trainging:  42%|████▏     | 227/537 [16:00<21:19,  4.13s/it, running train loss: 0.60051]\u001b[A\n",
            "Trainging:  42%|████▏     | 228/537 [16:00<22:03,  4.28s/it, running train loss: 0.60051]\u001b[A\n",
            "Trainging:  42%|████▏     | 228/537 [16:03<22:03,  4.28s/it, running train loss: 0.49123]\u001b[A\n",
            "Trainging:  43%|████▎     | 229/537 [16:03<20:28,  3.99s/it, running train loss: 0.49123]\u001b[A\n",
            "Trainging:  43%|████▎     | 229/537 [16:07<20:28,  3.99s/it, running train loss: 0.57204]\u001b[A\n",
            "Trainging:  43%|████▎     | 230/537 [16:07<19:12,  3.75s/it, running train loss: 0.57204]\u001b[A\n",
            "Trainging:  43%|████▎     | 230/537 [16:10<19:12,  3.75s/it, running train loss: 0.60263]\u001b[A\n",
            "Trainging:  43%|████▎     | 231/537 [16:10<18:58,  3.72s/it, running train loss: 0.60263]\u001b[A\n",
            "Trainging:  43%|████▎     | 231/537 [16:15<18:58,  3.72s/it, running train loss: 0.57346]\u001b[A\n",
            "Trainging:  43%|████▎     | 232/537 [16:15<19:51,  3.91s/it, running train loss: 0.57346]\u001b[A\n",
            "Trainging:  43%|████▎     | 232/537 [16:18<19:51,  3.91s/it, running train loss: 0.46762]\u001b[A\n",
            "Trainging:  43%|████▎     | 233/537 [16:18<18:59,  3.75s/it, running train loss: 0.46762]\u001b[A\n",
            "Trainging:  43%|████▎     | 233/537 [16:22<18:59,  3.75s/it, running train loss: 0.55480]\u001b[A\n",
            "Trainging:  44%|████▎     | 234/537 [16:22<19:49,  3.93s/it, running train loss: 0.55480]\u001b[A\n",
            "Trainging:  44%|████▎     | 234/537 [16:26<19:49,  3.93s/it, running train loss: 0.50115]\u001b[A\n",
            "Trainging:  44%|████▍     | 235/537 [16:26<19:19,  3.84s/it, running train loss: 0.50115]\u001b[A\n",
            "Trainging:  44%|████▍     | 235/537 [16:29<19:19,  3.84s/it, running train loss: 0.45831]\u001b[A\n",
            "Trainging:  44%|████▍     | 236/537 [16:29<18:42,  3.73s/it, running train loss: 0.45831]\u001b[A\n",
            "Trainging:  44%|████▍     | 236/537 [16:33<18:42,  3.73s/it, running train loss: 0.50462]\u001b[A\n",
            "Trainging:  44%|████▍     | 237/537 [16:33<18:53,  3.78s/it, running train loss: 0.50462]\u001b[A\n",
            "Trainging:  44%|████▍     | 237/537 [16:37<18:53,  3.78s/it, running train loss: 0.54149]\u001b[A\n",
            "Trainging:  44%|████▍     | 238/537 [16:37<18:54,  3.80s/it, running train loss: 0.54149]\u001b[A\n",
            "Trainging:  44%|████▍     | 238/537 [16:43<18:54,  3.80s/it, running train loss: 0.59232]\u001b[A\n",
            "Trainging:  45%|████▍     | 239/537 [16:43<22:18,  4.49s/it, running train loss: 0.59232]\u001b[A\n",
            "Trainging:  45%|████▍     | 239/537 [16:47<22:18,  4.49s/it, running train loss: 0.48834]\u001b[A\n",
            "Trainging:  45%|████▍     | 240/537 [16:47<21:32,  4.35s/it, running train loss: 0.48834]\u001b[A\n",
            "Trainging:  45%|████▍     | 240/537 [16:52<21:32,  4.35s/it, running train loss: 0.57493]\u001b[A\n",
            "Trainging:  45%|████▍     | 241/537 [16:52<21:45,  4.41s/it, running train loss: 0.57493]\u001b[A\n",
            "Trainging:  45%|████▍     | 241/537 [16:56<21:45,  4.41s/it, running train loss: 0.47372]\u001b[A\n",
            "Trainging:  45%|████▌     | 242/537 [16:56<21:29,  4.37s/it, running train loss: 0.47372]\u001b[A\n",
            "Trainging:  45%|████▌     | 242/537 [17:00<21:29,  4.37s/it, running train loss: 0.57007]\u001b[A\n",
            "Trainging:  45%|████▌     | 243/537 [17:00<20:18,  4.15s/it, running train loss: 0.57007]\u001b[A\n",
            "Trainging:  45%|████▌     | 243/537 [17:03<20:18,  4.15s/it, running train loss: 0.61051]\u001b[A\n",
            "Trainging:  45%|████▌     | 244/537 [17:03<19:26,  3.98s/it, running train loss: 0.61051]\u001b[A\n",
            "Trainging:  45%|████▌     | 244/537 [17:07<19:26,  3.98s/it, running train loss: 0.57537]\u001b[A\n",
            "Trainging:  46%|████▌     | 245/537 [17:07<18:47,  3.86s/it, running train loss: 0.57537]\u001b[A\n",
            "Trainging:  46%|████▌     | 245/537 [17:10<18:47,  3.86s/it, running train loss: 0.58451]\u001b[A\n",
            "Trainging:  46%|████▌     | 246/537 [17:10<18:16,  3.77s/it, running train loss: 0.58451]\u001b[A\n",
            "Trainging:  46%|████▌     | 246/537 [17:14<18:16,  3.77s/it, running train loss: 0.57890]\u001b[A\n",
            "Trainging:  46%|████▌     | 247/537 [17:14<18:24,  3.81s/it, running train loss: 0.57890]\u001b[A\n",
            "Trainging:  46%|████▌     | 247/537 [17:18<18:24,  3.81s/it, running train loss: 0.61720]\u001b[A\n",
            "Trainging:  46%|████▌     | 248/537 [17:18<18:01,  3.74s/it, running train loss: 0.61720]\u001b[A\n",
            "Trainging:  46%|████▌     | 248/537 [17:22<18:01,  3.74s/it, running train loss: 0.50205]\u001b[A\n",
            "Trainging:  46%|████▋     | 249/537 [17:22<17:51,  3.72s/it, running train loss: 0.50205]\u001b[A\n",
            "Trainging:  46%|████▋     | 249/537 [17:25<17:51,  3.72s/it, running train loss: 0.47959]\u001b[A\n",
            "Trainging:  47%|████▋     | 250/537 [17:25<17:33,  3.67s/it, running train loss: 0.47959]\u001b[A\n",
            "Trainging:  47%|████▋     | 250/537 [17:29<17:33,  3.67s/it, running train loss: 0.39495]\u001b[A\n",
            "Trainging:  47%|████▋     | 251/537 [17:29<17:08,  3.59s/it, running train loss: 0.39495]\u001b[A\n",
            "Trainging:  47%|████▋     | 251/537 [17:33<17:08,  3.59s/it, running train loss: 0.63381]\u001b[A\n",
            "Trainging:  47%|████▋     | 252/537 [17:33<18:08,  3.82s/it, running train loss: 0.63381]\u001b[A\n",
            "Trainging:  47%|████▋     | 252/537 [17:37<18:08,  3.82s/it, running train loss: 0.59772]\u001b[A\n",
            "Trainging:  47%|████▋     | 253/537 [17:37<17:53,  3.78s/it, running train loss: 0.59772]\u001b[A\n",
            "Trainging:  47%|████▋     | 253/537 [17:41<17:53,  3.78s/it, running train loss: 0.61240]\u001b[A\n",
            "Trainging:  47%|████▋     | 254/537 [17:41<18:07,  3.84s/it, running train loss: 0.61240]\u001b[A\n",
            "Trainging:  47%|████▋     | 254/537 [17:44<18:07,  3.84s/it, running train loss: 0.51479]\u001b[A\n",
            "Trainging:  47%|████▋     | 255/537 [17:44<18:00,  3.83s/it, running train loss: 0.51479]\u001b[A\n",
            "Trainging:  47%|████▋     | 255/537 [17:48<18:00,  3.83s/it, running train loss: 0.49448]\u001b[A\n",
            "Trainging:  48%|████▊     | 256/537 [17:48<18:09,  3.88s/it, running train loss: 0.49448]\u001b[A\n",
            "Trainging:  48%|████▊     | 256/537 [17:52<18:09,  3.88s/it, running train loss: 0.55463]\u001b[A\n",
            "Trainging:  48%|████▊     | 257/537 [17:52<18:00,  3.86s/it, running train loss: 0.55463]\u001b[A\n",
            "Trainging:  48%|████▊     | 257/537 [17:56<18:00,  3.86s/it, running train loss: 0.49469]\u001b[A\n",
            "Trainging:  48%|████▊     | 258/537 [17:56<17:32,  3.77s/it, running train loss: 0.49469]\u001b[A\n",
            "Trainging:  48%|████▊     | 258/537 [18:00<17:32,  3.77s/it, running train loss: 0.66251]\u001b[A\n",
            "Trainging:  48%|████▊     | 259/537 [18:00<18:44,  4.04s/it, running train loss: 0.66251]\u001b[A\n",
            "Trainging:  48%|████▊     | 259/537 [18:04<18:44,  4.04s/it, running train loss: 0.52035]\u001b[A\n",
            "Trainging:  48%|████▊     | 260/537 [18:04<17:53,  3.87s/it, running train loss: 0.52035]\u001b[A\n",
            "Trainging:  48%|████▊     | 260/537 [18:08<17:53,  3.87s/it, running train loss: 0.50455]\u001b[A\n",
            "Trainging:  49%|████▊     | 261/537 [18:08<18:25,  4.01s/it, running train loss: 0.50455]\u001b[A\n",
            "Trainging:  49%|████▊     | 261/537 [18:12<18:25,  4.01s/it, running train loss: 0.55839]\u001b[A\n",
            "Trainging:  49%|████▉     | 262/537 [18:12<18:13,  3.98s/it, running train loss: 0.55839]\u001b[A\n",
            "Trainging:  49%|████▉     | 262/537 [18:16<18:13,  3.98s/it, running train loss: 0.54262]\u001b[A\n",
            "Trainging:  49%|████▉     | 263/537 [18:16<18:05,  3.96s/it, running train loss: 0.54262]\u001b[A\n",
            "Trainging:  49%|████▉     | 263/537 [18:20<18:05,  3.96s/it, running train loss: 0.52616]\u001b[A\n",
            "Trainging:  49%|████▉     | 264/537 [18:20<18:00,  3.96s/it, running train loss: 0.52616]\u001b[A\n",
            "Trainging:  49%|████▉     | 264/537 [18:23<18:00,  3.96s/it, running train loss: 0.40541]\u001b[A\n",
            "Trainging:  49%|████▉     | 265/537 [18:23<17:13,  3.80s/it, running train loss: 0.40541]\u001b[A\n",
            "Trainging:  49%|████▉     | 265/537 [18:27<17:13,  3.80s/it, running train loss: 0.56578]\u001b[A\n",
            "Trainging:  50%|████▉     | 266/537 [18:27<16:43,  3.70s/it, running train loss: 0.56578]\u001b[A\n",
            "Trainging:  50%|████▉     | 266/537 [18:31<16:43,  3.70s/it, running train loss: 0.70378]\u001b[A\n",
            "Trainging:  50%|████▉     | 267/537 [18:31<16:52,  3.75s/it, running train loss: 0.70378]\u001b[A\n",
            "Trainging:  50%|████▉     | 267/537 [18:34<16:52,  3.75s/it, running train loss: 0.62417]\u001b[A\n",
            "Trainging:  50%|████▉     | 268/537 [18:34<16:27,  3.67s/it, running train loss: 0.62417]\u001b[A\n",
            "Trainging:  50%|████▉     | 268/537 [18:38<16:27,  3.67s/it, running train loss: 0.57052]\u001b[A\n",
            "Trainging:  50%|█████     | 269/537 [18:38<16:11,  3.62s/it, running train loss: 0.57052]\u001b[A\n",
            "Trainging:  50%|█████     | 269/537 [18:42<16:11,  3.62s/it, running train loss: 0.50486]\u001b[A\n",
            "Trainging:  50%|█████     | 270/537 [18:42<17:10,  3.86s/it, running train loss: 0.50486]\u001b[A\n",
            "Trainging:  50%|█████     | 270/537 [18:48<17:10,  3.86s/it, running train loss: 0.53753]\u001b[A\n",
            "Trainging:  50%|█████     | 271/537 [18:48<19:34,  4.42s/it, running train loss: 0.53753]\u001b[A\n",
            "Trainging:  50%|█████     | 271/537 [18:51<19:34,  4.42s/it, running train loss: 0.52964]\u001b[A\n",
            "Trainging:  51%|█████     | 272/537 [18:52<18:24,  4.17s/it, running train loss: 0.52964]\u001b[A\n",
            "Trainging:  51%|█████     | 272/537 [18:55<18:24,  4.17s/it, running train loss: 0.52692]\u001b[A\n",
            "Trainging:  51%|█████     | 273/537 [18:55<17:45,  4.04s/it, running train loss: 0.52692]\u001b[A\n",
            "Trainging:  51%|█████     | 273/537 [18:59<17:45,  4.04s/it, running train loss: 0.53466]\u001b[A\n",
            "Trainging:  51%|█████     | 274/537 [18:59<16:52,  3.85s/it, running train loss: 0.53466]\u001b[A\n",
            "Trainging:  51%|█████     | 274/537 [19:04<16:52,  3.85s/it, running train loss: 0.46618]\u001b[A\n",
            "Trainging:  51%|█████     | 275/537 [19:04<18:25,  4.22s/it, running train loss: 0.46618]\u001b[A\n",
            "Trainging:  51%|█████     | 275/537 [19:07<18:25,  4.22s/it, running train loss: 0.50774]\u001b[A\n",
            "Trainging:  51%|█████▏    | 276/537 [19:07<17:18,  3.98s/it, running train loss: 0.50774]\u001b[A\n",
            "Trainging:  51%|█████▏    | 276/537 [19:11<17:18,  3.98s/it, running train loss: 0.50213]\u001b[A\n",
            "Trainging:  52%|█████▏    | 277/537 [19:11<17:04,  3.94s/it, running train loss: 0.50213]\u001b[A\n",
            "Trainging:  52%|█████▏    | 277/537 [19:16<17:04,  3.94s/it, running train loss: 0.52694]\u001b[A\n",
            "Trainging:  52%|█████▏    | 278/537 [19:16<17:45,  4.12s/it, running train loss: 0.52694]\u001b[A\n",
            "Trainging:  52%|█████▏    | 278/537 [19:19<17:45,  4.12s/it, running train loss: 0.53351]\u001b[A\n",
            "Trainging:  52%|█████▏    | 279/537 [19:19<16:53,  3.93s/it, running train loss: 0.53351]\u001b[A\n",
            "Trainging:  52%|█████▏    | 279/537 [19:23<16:53,  3.93s/it, running train loss: 0.69738]\u001b[A\n",
            "Trainging:  52%|█████▏    | 280/537 [19:23<16:32,  3.86s/it, running train loss: 0.69738]\u001b[A\n",
            "Trainging:  52%|█████▏    | 280/537 [19:26<16:32,  3.86s/it, running train loss: 0.63183]\u001b[A\n",
            "Trainging:  52%|█████▏    | 281/537 [19:26<16:08,  3.78s/it, running train loss: 0.63183]\u001b[A\n",
            "Trainging:  52%|█████▏    | 281/537 [19:30<16:08,  3.78s/it, running train loss: 0.54338]\u001b[A\n",
            "Trainging:  53%|█████▎    | 282/537 [19:30<16:10,  3.81s/it, running train loss: 0.54338]\u001b[A\n",
            "Trainging:  53%|█████▎    | 282/537 [19:34<16:10,  3.81s/it, running train loss: 0.53542]\u001b[A\n",
            "Trainging:  53%|█████▎    | 283/537 [19:34<15:58,  3.77s/it, running train loss: 0.53542]\u001b[A\n",
            "Trainging:  53%|█████▎    | 283/537 [19:37<15:58,  3.77s/it, running train loss: 0.48339]\u001b[A\n",
            "Trainging:  53%|█████▎    | 284/537 [19:37<15:34,  3.69s/it, running train loss: 0.48339]\u001b[A\n",
            "Trainging:  53%|█████▎    | 284/537 [19:42<15:34,  3.69s/it, running train loss: 0.45072]\u001b[A\n",
            "Trainging:  53%|█████▎    | 285/537 [19:42<16:32,  3.94s/it, running train loss: 0.45072]\u001b[A\n",
            "Trainging:  53%|█████▎    | 285/537 [19:46<16:32,  3.94s/it, running train loss: 0.51934]\u001b[A\n",
            "Trainging:  53%|█████▎    | 286/537 [19:46<16:18,  3.90s/it, running train loss: 0.51934]\u001b[A\n",
            "Trainging:  53%|█████▎    | 286/537 [19:50<16:18,  3.90s/it, running train loss: 0.51890]\u001b[A\n",
            "Trainging:  53%|█████▎    | 287/537 [19:50<16:12,  3.89s/it, running train loss: 0.51890]\u001b[A\n",
            "Trainging:  53%|█████▎    | 287/537 [19:53<16:12,  3.89s/it, running train loss: 0.64258]\u001b[A\n",
            "Trainging:  54%|█████▎    | 288/537 [19:53<15:45,  3.80s/it, running train loss: 0.64258]\u001b[A\n",
            "Trainging:  54%|█████▎    | 288/537 [19:57<15:45,  3.80s/it, running train loss: 0.50234]\u001b[A\n",
            "Trainging:  54%|█████▍    | 289/537 [19:57<16:17,  3.94s/it, running train loss: 0.50234]\u001b[A\n",
            "Trainging:  54%|█████▍    | 289/537 [20:01<16:17,  3.94s/it, running train loss: 0.42252]\u001b[A\n",
            "Trainging:  54%|█████▍    | 290/537 [20:01<16:07,  3.92s/it, running train loss: 0.42252]\u001b[A\n",
            "Trainging:  54%|█████▍    | 290/537 [20:06<16:07,  3.92s/it, running train loss: 0.55653]\u001b[A\n",
            "Trainging:  54%|█████▍    | 291/537 [20:06<17:10,  4.19s/it, running train loss: 0.55653]\u001b[A\n",
            "Trainging:  54%|█████▍    | 291/537 [20:11<17:10,  4.19s/it, running train loss: 0.58082]\u001b[A\n",
            "Trainging:  54%|█████▍    | 292/537 [20:11<17:34,  4.31s/it, running train loss: 0.58082]\u001b[A\n",
            "Trainging:  54%|█████▍    | 292/537 [20:14<17:34,  4.31s/it, running train loss: 0.57788]\u001b[A\n",
            "Trainging:  55%|█████▍    | 293/537 [20:14<16:38,  4.09s/it, running train loss: 0.57788]\u001b[A\n",
            "Trainging:  55%|█████▍    | 293/537 [20:18<16:38,  4.09s/it, running train loss: 0.47782]\u001b[A\n",
            "Trainging:  55%|█████▍    | 294/537 [20:18<16:12,  4.00s/it, running train loss: 0.47782]\u001b[A\n",
            "Trainging:  55%|█████▍    | 294/537 [20:20<16:12,  4.00s/it, running train loss: 0.52844]\u001b[A\n",
            "Trainging:  55%|█████▍    | 295/537 [20:20<13:59,  3.47s/it, running train loss: 0.52844]\u001b[A\n",
            "Trainging:  55%|█████▍    | 295/537 [20:24<13:59,  3.47s/it, running train loss: 0.58850]\u001b[A\n",
            "Trainging:  55%|█████▌    | 296/537 [20:24<14:02,  3.50s/it, running train loss: 0.58850]\u001b[A\n",
            "Trainging:  55%|█████▌    | 296/537 [20:28<14:02,  3.50s/it, running train loss: 0.59702]\u001b[A\n",
            "Trainging:  55%|█████▌    | 297/537 [20:28<15:17,  3.82s/it, running train loss: 0.59702]\u001b[A\n",
            "Trainging:  55%|█████▌    | 297/537 [20:32<15:17,  3.82s/it, running train loss: 0.57122]\u001b[A\n",
            "Trainging:  55%|█████▌    | 298/537 [20:32<14:47,  3.71s/it, running train loss: 0.57122]\u001b[A\n",
            "Trainging:  55%|█████▌    | 298/537 [20:37<14:47,  3.71s/it, running train loss: 0.59969]\u001b[A\n",
            "Trainging:  56%|█████▌    | 299/537 [20:37<15:59,  4.03s/it, running train loss: 0.59969]\u001b[A\n",
            "Trainging:  56%|█████▌    | 299/537 [20:41<15:59,  4.03s/it, running train loss: 0.54539]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|▏         | 1/68 [00:00<00:38,  1.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 2/68 [00:01<00:32,  2.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 3/68 [00:01<00:27,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 4/68 [00:01<00:27,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 5/68 [00:02<00:26,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 6/68 [00:02<00:24,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|█         | 7/68 [00:02<00:24,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 8/68 [00:03<00:24,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 9/68 [00:03<00:24,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 10/68 [00:04<00:23,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 11/68 [00:04<00:23,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 12/68 [00:05<00:23,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 13/68 [00:05<00:23,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 14/68 [00:05<00:22,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 15/68 [00:06<00:22,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▎       | 16/68 [00:06<00:22,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▌       | 17/68 [00:07<00:20,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 18/68 [00:07<00:20,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 19/68 [00:07<00:18,  2.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 20/68 [00:08<00:18,  2.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 21/68 [00:08<00:18,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 22/68 [00:09<00:17,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 23/68 [00:09<00:17,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▌      | 24/68 [00:09<00:16,  2.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 25/68 [00:10<00:16,  2.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 26/68 [00:10<00:17,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 27/68 [00:11<00:16,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████      | 28/68 [00:11<00:15,  2.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 29/68 [00:11<00:15,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 30/68 [00:12<00:15,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▌     | 31/68 [00:12<00:14,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 32/68 [00:12<00:14,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▊     | 33/68 [00:13<00:14,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 34/68 [00:13<00:13,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████▏    | 35/68 [00:14<00:12,  2.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 36/68 [00:14<00:12,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▍    | 37/68 [00:14<00:12,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 38/68 [00:15<00:12,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 39/68 [00:15<00:11,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▉    | 40/68 [00:16<00:11,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 41/68 [00:16<00:10,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 42/68 [00:17<00:10,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 43/68 [00:17<00:10,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▍   | 44/68 [00:17<00:10,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 45/68 [00:18<00:09,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 46/68 [00:18<00:09,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 47/68 [00:19<00:09,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 48/68 [00:19<00:08,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 49/68 [00:20<00:08,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 50/68 [00:20<00:07,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 51/68 [00:20<00:07,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▋  | 52/68 [00:21<00:06,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 53/68 [00:21<00:06,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 54/68 [00:22<00:06,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 55/68 [00:22<00:05,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 56/68 [00:22<00:04,  2.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 57/68 [00:23<00:04,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 58/68 [00:23<00:04,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 59/68 [00:24<00:03,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 60/68 [00:24<00:03,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|████████▉ | 61/68 [00:24<00:02,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 62/68 [00:25<00:02,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 63/68 [00:25<00:02,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 64/68 [00:26<00:01,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 65/68 [00:26<00:01,  2.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 66/68 [00:26<00:00,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▊| 67/68 [00:27<00:00,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 68/68 [00:27<00:00,  2.45it/s]\n",
            "\n",
            "Trainging:  56%|█████▌    | 300/537 [21:11<51:37, 13.07s/it, running train loss: 0.54539]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> training loss: 0.544851, valid loss: 0.526998,valid f1: 0.477660, valid acc:0.702039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Trainging:  56%|█████▌    | 300/537 [21:15<51:37, 13.07s/it, running train loss: 0.48187]\u001b[A\n",
            "Trainging:  56%|█████▌    | 301/537 [21:15<40:22, 10.27s/it, running train loss: 0.48187]\u001b[A\n",
            "Trainging:  56%|█████▌    | 301/537 [21:18<40:22, 10.27s/it, running train loss: 0.60991]\u001b[A\n",
            "Trainging:  56%|█████▌    | 302/537 [21:18<32:13,  8.23s/it, running train loss: 0.60991]\u001b[A\n",
            "Trainging:  56%|█████▌    | 302/537 [21:23<32:13,  8.23s/it, running train loss: 0.50224]\u001b[A\n",
            "Trainging:  56%|█████▋    | 303/537 [21:23<28:43,  7.37s/it, running train loss: 0.50224]\u001b[A\n",
            "Trainging:  56%|█████▋    | 303/537 [21:27<28:43,  7.37s/it, running train loss: 0.47104]\u001b[A\n",
            "Trainging:  57%|█████▋    | 304/537 [21:27<24:00,  6.18s/it, running train loss: 0.47104]\u001b[A\n",
            "Trainging:  57%|█████▋    | 304/537 [21:30<24:00,  6.18s/it, running train loss: 0.46893]\u001b[A\n",
            "Trainging:  57%|█████▋    | 305/537 [21:30<20:35,  5.33s/it, running train loss: 0.46893]\u001b[A\n",
            "Trainging:  57%|█████▋    | 305/537 [21:34<20:35,  5.33s/it, running train loss: 0.62619]\u001b[A\n",
            "Trainging:  57%|█████▋    | 306/537 [21:34<18:27,  4.79s/it, running train loss: 0.62619]\u001b[A\n",
            "Trainging:  57%|█████▋    | 306/537 [21:37<18:27,  4.79s/it, running train loss: 0.58073]\u001b[A\n",
            "Trainging:  57%|█████▋    | 307/537 [21:37<16:44,  4.37s/it, running train loss: 0.58073]\u001b[A\n",
            "Trainging:  57%|█████▋    | 307/537 [21:40<16:44,  4.37s/it, running train loss: 0.45767]\u001b[A\n",
            "Trainging:  57%|█████▋    | 308/537 [21:40<15:30,  4.06s/it, running train loss: 0.45767]\u001b[A\n",
            "Trainging:  57%|█████▋    | 308/537 [21:44<15:30,  4.06s/it, running train loss: 0.49011]\u001b[A\n",
            "Trainging:  58%|█████▊    | 309/537 [21:44<14:39,  3.86s/it, running train loss: 0.49011]\u001b[A\n",
            "Trainging:  58%|█████▊    | 309/537 [21:47<14:39,  3.86s/it, running train loss: 0.55893]\u001b[A\n",
            "Trainging:  58%|█████▊    | 310/537 [21:47<14:12,  3.76s/it, running train loss: 0.55893]\u001b[A\n",
            "Trainging:  58%|█████▊    | 310/537 [21:51<14:12,  3.76s/it, running train loss: 0.42425]\u001b[A\n",
            "Trainging:  58%|█████▊    | 311/537 [21:51<13:42,  3.64s/it, running train loss: 0.42425]\u001b[A\n",
            "Trainging:  58%|█████▊    | 311/537 [21:54<13:42,  3.64s/it, running train loss: 0.46615]\u001b[A\n",
            "Trainging:  58%|█████▊    | 312/537 [21:54<13:20,  3.56s/it, running train loss: 0.46615]\u001b[A\n",
            "Trainging:  58%|█████▊    | 312/537 [21:58<13:20,  3.56s/it, running train loss: 0.44684]\u001b[A\n",
            "Trainging:  58%|█████▊    | 313/537 [21:58<13:47,  3.70s/it, running train loss: 0.44684]\u001b[A\n",
            "Trainging:  58%|█████▊    | 313/537 [22:02<13:47,  3.70s/it, running train loss: 0.53440]\u001b[A\n",
            "Trainging:  58%|█████▊    | 314/537 [22:02<13:28,  3.62s/it, running train loss: 0.53440]\u001b[A\n",
            "Trainging:  58%|█████▊    | 314/537 [22:06<13:28,  3.62s/it, running train loss: 0.54245]\u001b[A\n",
            "Trainging:  59%|█████▊    | 315/537 [22:06<14:47,  4.00s/it, running train loss: 0.54245]\u001b[A\n",
            "Trainging:  59%|█████▊    | 315/537 [22:10<14:47,  4.00s/it, running train loss: 0.56011]\u001b[A\n",
            "Trainging:  59%|█████▉    | 316/537 [22:10<14:14,  3.87s/it, running train loss: 0.56011]\u001b[A\n",
            "Trainging:  59%|█████▉    | 316/537 [22:14<14:14,  3.87s/it, running train loss: 0.55684]\u001b[A\n",
            "Trainging:  59%|█████▉    | 317/537 [22:14<14:20,  3.91s/it, running train loss: 0.55684]\u001b[A\n",
            "Trainging:  59%|█████▉    | 317/537 [22:20<14:20,  3.91s/it, running train loss: 0.51263]\u001b[A\n",
            "Trainging:  59%|█████▉    | 318/537 [22:20<16:07,  4.42s/it, running train loss: 0.51263]\u001b[A\n",
            "Trainging:  59%|█████▉    | 318/537 [22:24<16:07,  4.42s/it, running train loss: 0.50927]\u001b[A\n",
            "Trainging:  59%|█████▉    | 319/537 [22:24<15:35,  4.29s/it, running train loss: 0.50927]\u001b[A\n",
            "Trainging:  59%|█████▉    | 319/537 [22:27<15:35,  4.29s/it, running train loss: 0.47393]\u001b[A\n",
            "Trainging:  60%|█████▉    | 320/537 [22:27<15:03,  4.16s/it, running train loss: 0.47393]\u001b[A\n",
            "Trainging:  60%|█████▉    | 320/537 [22:31<15:03,  4.16s/it, running train loss: 0.45906]\u001b[A\n",
            "Trainging:  60%|█████▉    | 321/537 [22:31<14:48,  4.12s/it, running train loss: 0.45906]\u001b[A\n",
            "Trainging:  60%|█████▉    | 321/537 [22:35<14:48,  4.12s/it, running train loss: 0.46185]\u001b[A\n",
            "Trainging:  60%|█████▉    | 322/537 [22:35<13:57,  3.89s/it, running train loss: 0.46185]\u001b[A\n",
            "Trainging:  60%|█████▉    | 322/537 [22:38<13:57,  3.89s/it, running train loss: 0.46044]\u001b[A\n",
            "Trainging:  60%|██████    | 323/537 [22:38<13:27,  3.77s/it, running train loss: 0.46044]\u001b[A\n",
            "Trainging:  60%|██████    | 323/537 [22:42<13:27,  3.77s/it, running train loss: 0.60975]\u001b[A\n",
            "Trainging:  60%|██████    | 324/537 [22:42<13:12,  3.72s/it, running train loss: 0.60975]\u001b[A\n",
            "Trainging:  60%|██████    | 324/537 [22:46<13:12,  3.72s/it, running train loss: 0.40798]\u001b[A\n",
            "Trainging:  61%|██████    | 325/537 [22:46<13:19,  3.77s/it, running train loss: 0.40798]\u001b[A\n",
            "Trainging:  61%|██████    | 325/537 [22:50<13:19,  3.77s/it, running train loss: 0.49321]\u001b[A\n",
            "Trainging:  61%|██████    | 326/537 [22:50<13:26,  3.82s/it, running train loss: 0.49321]\u001b[A\n",
            "Trainging:  61%|██████    | 326/537 [22:53<13:26,  3.82s/it, running train loss: 0.64643]\u001b[A\n",
            "Trainging:  61%|██████    | 327/537 [22:53<13:12,  3.77s/it, running train loss: 0.64643]\u001b[A\n",
            "Trainging:  61%|██████    | 327/537 [22:57<13:12,  3.77s/it, running train loss: 0.68584]\u001b[A\n",
            "Trainging:  61%|██████    | 328/537 [22:57<13:06,  3.76s/it, running train loss: 0.68584]\u001b[A\n",
            "Trainging:  61%|██████    | 328/537 [23:01<13:06,  3.76s/it, running train loss: 0.45190]\u001b[A\n",
            "Trainging:  61%|██████▏   | 329/537 [23:01<12:44,  3.68s/it, running train loss: 0.45190]\u001b[A\n",
            "Trainging:  61%|██████▏   | 329/537 [23:04<12:44,  3.68s/it, running train loss: 0.57940]\u001b[A\n",
            "Trainging:  61%|██████▏   | 330/537 [23:04<12:39,  3.67s/it, running train loss: 0.57940]\u001b[A\n",
            "Trainging:  61%|██████▏   | 330/537 [23:09<12:39,  3.67s/it, running train loss: 0.45707]\u001b[A\n",
            "Trainging:  62%|██████▏   | 331/537 [23:09<13:39,  3.98s/it, running train loss: 0.45707]\u001b[A\n",
            "Trainging:  62%|██████▏   | 331/537 [23:12<13:39,  3.98s/it, running train loss: 0.47419]\u001b[A\n",
            "Trainging:  62%|██████▏   | 332/537 [23:12<13:01,  3.81s/it, running train loss: 0.47419]\u001b[A\n",
            "Trainging:  62%|██████▏   | 332/537 [23:18<13:01,  3.81s/it, running train loss: 0.52421]\u001b[A\n",
            "Trainging:  62%|██████▏   | 333/537 [23:18<14:39,  4.31s/it, running train loss: 0.52421]\u001b[A\n",
            "Trainging:  62%|██████▏   | 333/537 [23:21<14:39,  4.31s/it, running train loss: 0.49004]\u001b[A\n",
            "Trainging:  62%|██████▏   | 334/537 [23:21<13:52,  4.10s/it, running train loss: 0.49004]\u001b[A\n",
            "Trainging:  62%|██████▏   | 334/537 [23:25<13:52,  4.10s/it, running train loss: 0.52279]\u001b[A\n",
            "Trainging:  62%|██████▏   | 335/537 [23:25<13:30,  4.01s/it, running train loss: 0.52279]\u001b[A\n",
            "Trainging:  62%|██████▏   | 335/537 [23:29<13:30,  4.01s/it, running train loss: 0.53851]\u001b[A\n",
            "Trainging:  63%|██████▎   | 336/537 [23:29<13:05,  3.91s/it, running train loss: 0.53851]\u001b[A\n",
            "Trainging:  63%|██████▎   | 336/537 [23:32<13:05,  3.91s/it, running train loss: 0.50238]\u001b[A\n",
            "Trainging:  63%|██████▎   | 337/537 [23:32<12:34,  3.77s/it, running train loss: 0.50238]\u001b[A\n",
            "Trainging:  63%|██████▎   | 337/537 [23:36<12:34,  3.77s/it, running train loss: 0.51155]\u001b[A\n",
            "Trainging:  63%|██████▎   | 338/537 [23:36<12:43,  3.84s/it, running train loss: 0.51155]\u001b[A\n",
            "Trainging:  63%|██████▎   | 338/537 [23:41<12:43,  3.84s/it, running train loss: 0.62787]\u001b[A\n",
            "Trainging:  63%|██████▎   | 339/537 [23:41<13:15,  4.02s/it, running train loss: 0.62787]\u001b[A\n",
            "Trainging:  63%|██████▎   | 339/537 [23:46<13:15,  4.02s/it, running train loss: 0.50516]\u001b[A\n",
            "Trainging:  63%|██████▎   | 340/537 [23:46<13:59,  4.26s/it, running train loss: 0.50516]\u001b[A\n",
            "Trainging:  63%|██████▎   | 340/537 [23:49<13:59,  4.26s/it, running train loss: 0.48801]\u001b[A\n",
            "Trainging:  64%|██████▎   | 341/537 [23:49<13:08,  4.02s/it, running train loss: 0.48801]\u001b[A\n",
            "Trainging:  64%|██████▎   | 341/537 [23:53<13:08,  4.02s/it, running train loss: 0.54270]\u001b[A\n",
            "Trainging:  64%|██████▎   | 342/537 [23:53<12:52,  3.96s/it, running train loss: 0.54270]\u001b[A\n",
            "Trainging:  64%|██████▎   | 342/537 [23:56<12:52,  3.96s/it, running train loss: 0.48053]\u001b[A\n",
            "Trainging:  64%|██████▍   | 343/537 [23:56<12:18,  3.81s/it, running train loss: 0.48053]\u001b[A\n",
            "Trainging:  64%|██████▍   | 343/537 [24:01<12:18,  3.81s/it, running train loss: 0.66658]\u001b[A\n",
            "Trainging:  64%|██████▍   | 344/537 [24:01<12:51,  4.00s/it, running train loss: 0.66658]\u001b[A\n",
            "Trainging:  64%|██████▍   | 344/537 [24:04<12:51,  4.00s/it, running train loss: 0.51611]\u001b[A\n",
            "Trainging:  64%|██████▍   | 345/537 [24:04<12:21,  3.86s/it, running train loss: 0.51611]\u001b[A\n",
            "Trainging:  64%|██████▍   | 345/537 [24:08<12:21,  3.86s/it, running train loss: 0.48639]\u001b[A\n",
            "Trainging:  64%|██████▍   | 346/537 [24:08<11:52,  3.73s/it, running train loss: 0.48639]\u001b[A\n",
            "Trainging:  64%|██████▍   | 346/537 [24:13<11:52,  3.73s/it, running train loss: 0.51958]\u001b[A\n",
            "Trainging:  65%|██████▍   | 347/537 [24:13<12:59,  4.10s/it, running train loss: 0.51958]\u001b[A\n",
            "Trainging:  65%|██████▍   | 347/537 [24:16<12:59,  4.10s/it, running train loss: 0.48634]\u001b[A\n",
            "Trainging:  65%|██████▍   | 348/537 [24:16<12:33,  3.99s/it, running train loss: 0.48634]\u001b[A\n",
            "Trainging:  65%|██████▍   | 348/537 [24:20<12:33,  3.99s/it, running train loss: 0.46209]\u001b[A\n",
            "Trainging:  65%|██████▍   | 349/537 [24:20<12:07,  3.87s/it, running train loss: 0.46209]\u001b[A\n",
            "Trainging:  65%|██████▍   | 349/537 [24:24<12:07,  3.87s/it, running train loss: 0.52199]\u001b[A\n",
            "Trainging:  65%|██████▌   | 350/537 [24:24<11:53,  3.82s/it, running train loss: 0.52199]\u001b[A\n",
            "Trainging:  65%|██████▌   | 350/537 [24:28<11:53,  3.82s/it, running train loss: 0.45902]\u001b[A\n",
            "Trainging:  65%|██████▌   | 351/537 [24:28<11:51,  3.82s/it, running train loss: 0.45902]\u001b[A\n",
            "Trainging:  65%|██████▌   | 351/537 [24:31<11:51,  3.82s/it, running train loss: 0.59624]\u001b[A\n",
            "Trainging:  66%|██████▌   | 352/537 [24:31<11:40,  3.79s/it, running train loss: 0.59624]\u001b[A\n",
            "Trainging:  66%|██████▌   | 352/537 [24:35<11:40,  3.79s/it, running train loss: 0.67702]\u001b[A\n",
            "Trainging:  66%|██████▌   | 353/537 [24:35<11:31,  3.76s/it, running train loss: 0.67702]\u001b[A\n",
            "Trainging:  66%|██████▌   | 353/537 [24:39<11:31,  3.76s/it, running train loss: 0.47254]\u001b[A\n",
            "Trainging:  66%|██████▌   | 354/537 [24:39<11:33,  3.79s/it, running train loss: 0.47254]\u001b[A\n",
            "Trainging:  66%|██████▌   | 354/537 [24:43<11:33,  3.79s/it, running train loss: 0.49755]\u001b[A\n",
            "Trainging:  66%|██████▌   | 355/537 [24:43<11:38,  3.84s/it, running train loss: 0.49755]\u001b[A\n",
            "Trainging:  66%|██████▌   | 355/537 [24:46<11:38,  3.84s/it, running train loss: 0.55760]\u001b[A\n",
            "Trainging:  66%|██████▋   | 356/537 [24:46<11:20,  3.76s/it, running train loss: 0.55760]\u001b[A\n",
            "Trainging:  66%|██████▋   | 356/537 [24:50<11:20,  3.76s/it, running train loss: 0.42466]\u001b[A\n",
            "Trainging:  66%|██████▋   | 357/537 [24:50<11:29,  3.83s/it, running train loss: 0.42466]\u001b[A\n",
            "Trainging:  66%|██████▋   | 357/537 [24:56<11:29,  3.83s/it, running train loss: 0.62339]\u001b[A\n",
            "Trainging:  67%|██████▋   | 358/537 [24:56<12:56,  4.34s/it, running train loss: 0.62339]\u001b[A\n",
            "Trainging:  67%|██████▋   | 358/537 [25:01<12:56,  4.34s/it, running train loss: 0.54052]\u001b[A\n",
            "Trainging:  67%|██████▋   | 359/537 [25:01<13:20,  4.50s/it, running train loss: 0.54052]\u001b[A\n",
            "Trainging:  67%|██████▋   | 359/537 [25:06<13:20,  4.50s/it, running train loss: 0.48605]\u001b[A\n",
            "Trainging:  67%|██████▋   | 360/537 [25:06<13:43,  4.65s/it, running train loss: 0.48605]\u001b[A\n",
            "Trainging:  67%|██████▋   | 360/537 [25:10<13:43,  4.65s/it, running train loss: 0.66082]\u001b[A\n",
            "Trainging:  67%|██████▋   | 361/537 [25:10<13:19,  4.54s/it, running train loss: 0.66082]\u001b[A\n",
            "Trainging:  67%|██████▋   | 361/537 [25:15<13:19,  4.54s/it, running train loss: 0.64342]\u001b[A\n",
            "Trainging:  67%|██████▋   | 362/537 [25:15<13:35,  4.66s/it, running train loss: 0.64342]\u001b[A\n",
            "Trainging:  67%|██████▋   | 362/537 [25:18<13:35,  4.66s/it, running train loss: 0.51576]\u001b[A\n",
            "Trainging:  68%|██████▊   | 363/537 [25:18<12:27,  4.29s/it, running train loss: 0.51576]\u001b[A\n",
            "Trainging:  68%|██████▊   | 363/537 [25:22<12:27,  4.29s/it, running train loss: 0.51375]\u001b[A\n",
            "Trainging:  68%|██████▊   | 364/537 [25:22<12:07,  4.21s/it, running train loss: 0.51375]\u001b[A\n",
            "Trainging:  68%|██████▊   | 364/537 [25:26<12:07,  4.21s/it, running train loss: 0.51344]\u001b[A\n",
            "Trainging:  68%|██████▊   | 365/537 [25:26<11:42,  4.08s/it, running train loss: 0.51344]\u001b[A\n",
            "Trainging:  68%|██████▊   | 365/537 [25:31<11:42,  4.08s/it, running train loss: 0.62004]\u001b[A\n",
            "Trainging:  68%|██████▊   | 366/537 [25:31<11:59,  4.21s/it, running train loss: 0.62004]\u001b[A\n",
            "Trainging:  68%|██████▊   | 366/537 [25:37<11:59,  4.21s/it, running train loss: 0.58437]\u001b[A\n",
            "Trainging:  68%|██████▊   | 367/537 [25:37<13:25,  4.74s/it, running train loss: 0.58437]\u001b[A\n",
            "Trainging:  68%|██████▊   | 367/537 [25:42<13:25,  4.74s/it, running train loss: 0.53779]\u001b[A\n",
            "Trainging:  69%|██████▊   | 368/537 [25:42<13:40,  4.85s/it, running train loss: 0.53779]\u001b[A\n",
            "Trainging:  69%|██████▊   | 368/537 [25:46<13:40,  4.85s/it, running train loss: 0.53893]\u001b[A\n",
            "Trainging:  69%|██████▊   | 369/537 [25:46<13:06,  4.68s/it, running train loss: 0.53893]\u001b[A\n",
            "Trainging:  69%|██████▊   | 369/537 [25:50<13:06,  4.68s/it, running train loss: 0.55529]\u001b[A\n",
            "Trainging:  69%|██████▉   | 370/537 [25:50<12:02,  4.33s/it, running train loss: 0.55529]\u001b[A\n",
            "Trainging:  69%|██████▉   | 370/537 [25:53<12:02,  4.33s/it, running train loss: 0.55688]\u001b[A\n",
            "Trainging:  69%|██████▉   | 371/537 [25:53<11:26,  4.14s/it, running train loss: 0.55688]\u001b[A\n",
            "Trainging:  69%|██████▉   | 371/537 [25:59<11:26,  4.14s/it, running train loss: 0.58825]\u001b[A\n",
            "Trainging:  69%|██████▉   | 372/537 [25:59<12:47,  4.65s/it, running train loss: 0.58825]\u001b[A\n",
            "Trainging:  69%|██████▉   | 372/537 [26:05<12:47,  4.65s/it, running train loss: 0.56312]\u001b[A\n",
            "Trainging:  69%|██████▉   | 373/537 [26:05<13:55,  5.09s/it, running train loss: 0.56312]\u001b[A\n",
            "Trainging:  69%|██████▉   | 373/537 [26:10<13:55,  5.09s/it, running train loss: 0.59686]\u001b[A\n",
            "Trainging:  70%|██████▉   | 374/537 [26:10<13:24,  4.94s/it, running train loss: 0.59686]\u001b[A\n",
            "Trainging:  70%|██████▉   | 374/537 [26:14<13:24,  4.94s/it, running train loss: 0.64070]\u001b[A\n",
            "Trainging:  70%|██████▉   | 375/537 [26:14<12:31,  4.64s/it, running train loss: 0.64070]\u001b[A\n",
            "Trainging:  70%|██████▉   | 375/537 [26:20<12:31,  4.64s/it, running train loss: 0.88859]\u001b[A\n",
            "Trainging:  70%|███████   | 376/537 [26:20<13:37,  5.08s/it, running train loss: 0.88859]\u001b[A\n",
            "Trainging:  70%|███████   | 376/537 [26:24<13:37,  5.08s/it, running train loss: 0.51293]\u001b[A\n",
            "Trainging:  70%|███████   | 377/537 [26:24<12:38,  4.74s/it, running train loss: 0.51293]\u001b[A\n",
            "Trainging:  70%|███████   | 377/537 [26:27<12:38,  4.74s/it, running train loss: 0.48448]\u001b[A\n",
            "Trainging:  70%|███████   | 378/537 [26:27<11:32,  4.36s/it, running train loss: 0.48448]\u001b[A\n",
            "Trainging:  70%|███████   | 378/537 [26:30<11:32,  4.36s/it, running train loss: 0.55024]\u001b[A\n",
            "Trainging:  71%|███████   | 379/537 [26:30<09:46,  3.71s/it, running train loss: 0.55024]\u001b[A\n",
            "Trainging:  71%|███████   | 379/537 [26:34<09:46,  3.71s/it, running train loss: 0.53080]\u001b[A\n",
            "Trainging:  71%|███████   | 380/537 [26:34<10:26,  3.99s/it, running train loss: 0.53080]\u001b[A\n",
            "Trainging:  71%|███████   | 380/537 [26:38<10:26,  3.99s/it, running train loss: 0.47143]\u001b[A\n",
            "Trainging:  71%|███████   | 381/537 [26:38<10:06,  3.89s/it, running train loss: 0.47143]\u001b[A\n",
            "Trainging:  71%|███████   | 381/537 [26:42<10:06,  3.89s/it, running train loss: 0.54684]\u001b[A\n",
            "Trainging:  71%|███████   | 382/537 [26:42<10:08,  3.92s/it, running train loss: 0.54684]\u001b[A\n",
            "Trainging:  71%|███████   | 382/537 [26:45<10:08,  3.92s/it, running train loss: 0.46600]\u001b[A\n",
            "Trainging:  71%|███████▏  | 383/537 [26:45<09:40,  3.77s/it, running train loss: 0.46600]\u001b[A\n",
            "Trainging:  71%|███████▏  | 383/537 [26:49<09:40,  3.77s/it, running train loss: 0.46600]\u001b[A\n",
            "Trainging:  72%|███████▏  | 384/537 [26:49<09:33,  3.75s/it, running train loss: 0.46600]\u001b[A\n",
            "Trainging:  72%|███████▏  | 384/537 [26:54<09:33,  3.75s/it, running train loss: 0.60269]\u001b[A\n",
            "Trainging:  72%|███████▏  | 385/537 [26:54<10:31,  4.15s/it, running train loss: 0.60269]\u001b[A\n",
            "Trainging:  72%|███████▏  | 385/537 [26:58<10:31,  4.15s/it, running train loss: 0.49637]\u001b[A\n",
            "Trainging:  72%|███████▏  | 386/537 [26:58<10:19,  4.10s/it, running train loss: 0.49637]\u001b[A\n",
            "Trainging:  72%|███████▏  | 386/537 [27:02<10:19,  4.10s/it, running train loss: 0.49989]\u001b[A\n",
            "Trainging:  72%|███████▏  | 387/537 [27:02<10:22,  4.15s/it, running train loss: 0.49989]\u001b[A\n",
            "Trainging:  72%|███████▏  | 387/537 [27:06<10:22,  4.15s/it, running train loss: 0.54615]\u001b[A\n",
            "Trainging:  72%|███████▏  | 388/537 [27:06<09:57,  4.01s/it, running train loss: 0.54615]\u001b[A\n",
            "Trainging:  72%|███████▏  | 388/537 [27:10<09:57,  4.01s/it, running train loss: 0.53030]\u001b[A\n",
            "Trainging:  72%|███████▏  | 389/537 [27:10<09:48,  3.98s/it, running train loss: 0.53030]\u001b[A\n",
            "Trainging:  72%|███████▏  | 389/537 [27:13<09:48,  3.98s/it, running train loss: 0.43990]\u001b[A\n",
            "Trainging:  73%|███████▎  | 390/537 [27:13<09:28,  3.86s/it, running train loss: 0.43990]\u001b[A\n",
            "Trainging:  73%|███████▎  | 390/537 [27:17<09:28,  3.86s/it, running train loss: 0.56278]\u001b[A\n",
            "Trainging:  73%|███████▎  | 391/537 [27:17<09:29,  3.90s/it, running train loss: 0.56278]\u001b[A\n",
            "Trainging:  73%|███████▎  | 391/537 [27:21<09:29,  3.90s/it, running train loss: 0.63965]\u001b[A\n",
            "Trainging:  73%|███████▎  | 392/537 [27:21<09:08,  3.78s/it, running train loss: 0.63965]\u001b[A\n",
            "Trainging:  73%|███████▎  | 392/537 [27:23<09:08,  3.78s/it, running train loss: 0.57593]\u001b[A\n",
            "Trainging:  73%|███████▎  | 393/537 [27:23<07:56,  3.31s/it, running train loss: 0.57593]\u001b[A\n",
            "Trainging:  73%|███████▎  | 393/537 [27:27<07:56,  3.31s/it, running train loss: 0.43215]\u001b[A\n",
            "Trainging:  73%|███████▎  | 394/537 [27:27<08:13,  3.45s/it, running train loss: 0.43215]\u001b[A\n",
            "Trainging:  73%|███████▎  | 394/537 [27:31<08:13,  3.45s/it, running train loss: 0.56422]\u001b[A\n",
            "Trainging:  74%|███████▎  | 395/537 [27:31<08:15,  3.49s/it, running train loss: 0.56422]\u001b[A\n",
            "Trainging:  74%|███████▎  | 395/537 [27:35<08:15,  3.49s/it, running train loss: 0.52159]\u001b[A\n",
            "Trainging:  74%|███████▎  | 396/537 [27:35<09:14,  3.94s/it, running train loss: 0.52159]\u001b[A\n",
            "Trainging:  74%|███████▎  | 396/537 [27:40<09:14,  3.94s/it, running train loss: 0.55451]\u001b[A\n",
            "Trainging:  74%|███████▍  | 397/537 [27:40<09:36,  4.11s/it, running train loss: 0.55451]\u001b[A\n",
            "Trainging:  74%|███████▍  | 397/537 [27:44<09:36,  4.11s/it, running train loss: 0.56884]\u001b[A\n",
            "Trainging:  74%|███████▍  | 398/537 [27:44<09:41,  4.19s/it, running train loss: 0.56884]\u001b[A\n",
            "Trainging:  74%|███████▍  | 398/537 [27:49<09:41,  4.19s/it, running train loss: 0.61560]\u001b[A\n",
            "Trainging:  74%|███████▍  | 399/537 [27:49<09:52,  4.29s/it, running train loss: 0.61560]\u001b[A\n",
            "Trainging:  74%|███████▍  | 399/537 [27:53<09:52,  4.29s/it, running train loss: 0.48899]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|▏         | 1/68 [00:00<00:39,  1.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 2/68 [00:01<00:34,  1.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 3/68 [00:01<00:28,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 4/68 [00:01<00:27,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 5/68 [00:02<00:27,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 6/68 [00:02<00:24,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|█         | 7/68 [00:03<00:24,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 8/68 [00:03<00:24,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 9/68 [00:03<00:24,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 10/68 [00:04<00:23,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 11/68 [00:04<00:23,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 12/68 [00:05<00:23,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 13/68 [00:05<00:23,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 14/68 [00:05<00:22,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 15/68 [00:06<00:22,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▎       | 16/68 [00:06<00:22,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▌       | 17/68 [00:07<00:20,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 18/68 [00:07<00:20,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 19/68 [00:07<00:18,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 20/68 [00:08<00:18,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 21/68 [00:08<00:18,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 22/68 [00:09<00:17,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 23/68 [00:09<00:18,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▌      | 24/68 [00:09<00:16,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 25/68 [00:10<00:17,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 26/68 [00:10<00:17,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 27/68 [00:11<00:16,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████      | 28/68 [00:11<00:15,  2.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 29/68 [00:11<00:15,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 30/68 [00:12<00:15,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▌     | 31/68 [00:12<00:14,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 32/68 [00:13<00:14,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▊     | 33/68 [00:13<00:14,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 34/68 [00:13<00:13,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████▏    | 35/68 [00:14<00:12,  2.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 36/68 [00:14<00:12,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▍    | 37/68 [00:15<00:12,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 38/68 [00:15<00:12,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 39/68 [00:15<00:11,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▉    | 40/68 [00:16<00:11,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 41/68 [00:16<00:10,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 42/68 [00:17<00:10,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 43/68 [00:17<00:10,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▍   | 44/68 [00:17<00:10,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 45/68 [00:18<00:09,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 46/68 [00:18<00:09,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 47/68 [00:19<00:08,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 48/68 [00:19<00:08,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 49/68 [00:20<00:08,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 50/68 [00:20<00:07,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 51/68 [00:21<00:07,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▋  | 52/68 [00:21<00:06,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 53/68 [00:21<00:06,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 54/68 [00:22<00:06,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 55/68 [00:22<00:05,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 56/68 [00:22<00:04,  2.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 57/68 [00:23<00:04,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 58/68 [00:23<00:04,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 59/68 [00:24<00:03,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 60/68 [00:24<00:03,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|████████▉ | 61/68 [00:25<00:02,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 62/68 [00:25<00:02,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 63/68 [00:25<00:02,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 64/68 [00:26<00:01,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 65/68 [00:26<00:01,  2.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 66/68 [00:27<00:00,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▊| 67/68 [00:27<00:00,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 68/68 [00:27<00:00,  2.45it/s]\n",
            "\n",
            "Trainging:  74%|███████▍  | 400/537 [28:22<29:34, 12.95s/it, running train loss: 0.48899]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> training loss: 0.533753, valid loss: 0.513328,valid f1: 0.458169, valid acc:0.713392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Trainging:  74%|███████▍  | 400/537 [28:26<29:34, 12.95s/it, running train loss: 0.53525]\u001b[A\n",
            "Trainging:  75%|███████▍  | 401/537 [28:26<22:57, 10.13s/it, running train loss: 0.53525]\u001b[A\n",
            "Trainging:  75%|███████▍  | 401/537 [28:29<22:57, 10.13s/it, running train loss: 0.52939]\u001b[A\n",
            "Trainging:  75%|███████▍  | 402/537 [28:29<18:16,  8.12s/it, running train loss: 0.52939]\u001b[A\n",
            "Trainging:  75%|███████▍  | 402/537 [28:33<18:16,  8.12s/it, running train loss: 0.53405]\u001b[A\n",
            "Trainging:  75%|███████▌  | 403/537 [28:33<15:19,  6.86s/it, running train loss: 0.53405]\u001b[A\n",
            "Trainging:  75%|███████▌  | 403/537 [28:36<15:19,  6.86s/it, running train loss: 0.46558]\u001b[A\n",
            "Trainging:  75%|███████▌  | 404/537 [28:36<12:58,  5.86s/it, running train loss: 0.46558]\u001b[A\n",
            "Trainging:  75%|███████▌  | 404/537 [28:40<12:58,  5.86s/it, running train loss: 0.52522]\u001b[A\n",
            "Trainging:  75%|███████▌  | 405/537 [28:40<11:13,  5.10s/it, running train loss: 0.52522]\u001b[A\n",
            "Trainging:  75%|███████▌  | 405/537 [28:44<11:13,  5.10s/it, running train loss: 0.55810]\u001b[A\n",
            "Trainging:  76%|███████▌  | 406/537 [28:44<10:23,  4.76s/it, running train loss: 0.55810]\u001b[A\n",
            "Trainging:  76%|███████▌  | 406/537 [28:47<10:23,  4.76s/it, running train loss: 0.57497]\u001b[A\n",
            "Trainging:  76%|███████▌  | 407/537 [28:47<09:26,  4.36s/it, running train loss: 0.57497]\u001b[A\n",
            "Trainging:  76%|███████▌  | 407/537 [28:52<09:26,  4.36s/it, running train loss: 0.57006]\u001b[A\n",
            "Trainging:  76%|███████▌  | 408/537 [28:52<09:33,  4.45s/it, running train loss: 0.57006]\u001b[A\n",
            "Trainging:  76%|███████▌  | 408/537 [28:57<09:33,  4.45s/it, running train loss: 0.47309]\u001b[A\n",
            "Trainging:  76%|███████▌  | 409/537 [28:57<09:42,  4.55s/it, running train loss: 0.47309]\u001b[A\n",
            "Trainging:  76%|███████▌  | 409/537 [29:00<09:42,  4.55s/it, running train loss: 0.42120]\u001b[A\n",
            "Trainging:  76%|███████▋  | 410/537 [29:00<09:09,  4.33s/it, running train loss: 0.42120]\u001b[A\n",
            "Trainging:  76%|███████▋  | 410/537 [29:04<09:09,  4.33s/it, running train loss: 0.51446]\u001b[A\n",
            "Trainging:  77%|███████▋  | 411/537 [29:04<08:36,  4.10s/it, running train loss: 0.51446]\u001b[A\n",
            "Trainging:  77%|███████▋  | 411/537 [29:08<08:36,  4.10s/it, running train loss: 0.55093]\u001b[A\n",
            "Trainging:  77%|███████▋  | 412/537 [29:08<08:15,  3.96s/it, running train loss: 0.55093]\u001b[A\n",
            "Trainging:  77%|███████▋  | 412/537 [29:12<08:15,  3.96s/it, running train loss: 0.51386]\u001b[A\n",
            "Trainging:  77%|███████▋  | 413/537 [29:12<08:07,  3.93s/it, running train loss: 0.51386]\u001b[A\n",
            "Trainging:  77%|███████▋  | 413/537 [29:16<08:07,  3.93s/it, running train loss: 0.55468]\u001b[A\n",
            "Trainging:  77%|███████▋  | 414/537 [29:16<08:34,  4.18s/it, running train loss: 0.55468]\u001b[A\n",
            "Trainging:  77%|███████▋  | 414/537 [29:20<08:34,  4.18s/it, running train loss: 0.50004]\u001b[A\n",
            "Trainging:  77%|███████▋  | 415/537 [29:20<08:00,  3.94s/it, running train loss: 0.50004]\u001b[A\n",
            "Trainging:  77%|███████▋  | 415/537 [29:25<08:00,  3.94s/it, running train loss: 0.51449]\u001b[A\n",
            "Trainging:  77%|███████▋  | 416/537 [29:25<09:01,  4.47s/it, running train loss: 0.51449]\u001b[A\n",
            "Trainging:  77%|███████▋  | 416/537 [29:29<09:01,  4.47s/it, running train loss: 0.65311]\u001b[A\n",
            "Trainging:  78%|███████▊  | 417/537 [29:29<08:38,  4.32s/it, running train loss: 0.65311]\u001b[A\n",
            "Trainging:  78%|███████▊  | 417/537 [29:33<08:38,  4.32s/it, running train loss: 0.49677]\u001b[A\n",
            "Trainging:  78%|███████▊  | 418/537 [29:33<08:19,  4.20s/it, running train loss: 0.49677]\u001b[A\n",
            "Trainging:  78%|███████▊  | 418/537 [29:37<08:19,  4.20s/it, running train loss: 0.58108]\u001b[A\n",
            "Trainging:  78%|███████▊  | 419/537 [29:37<07:46,  3.95s/it, running train loss: 0.58108]\u001b[A\n",
            "Trainging:  78%|███████▊  | 419/537 [29:40<07:46,  3.95s/it, running train loss: 0.51903]\u001b[A\n",
            "Trainging:  78%|███████▊  | 420/537 [29:40<07:36,  3.90s/it, running train loss: 0.51903]\u001b[A\n",
            "Trainging:  78%|███████▊  | 420/537 [29:43<07:36,  3.90s/it, running train loss: 0.77544]\u001b[A\n",
            "Trainging:  78%|███████▊  | 421/537 [29:43<07:02,  3.64s/it, running train loss: 0.77544]\u001b[A\n",
            "Trainging:  78%|███████▊  | 421/537 [29:47<07:02,  3.64s/it, running train loss: 0.40536]\u001b[A\n",
            "Trainging:  79%|███████▊  | 422/537 [29:47<07:10,  3.74s/it, running train loss: 0.40536]\u001b[A\n",
            "Trainging:  79%|███████▊  | 422/537 [29:51<07:10,  3.74s/it, running train loss: 0.63568]\u001b[A\n",
            "Trainging:  79%|███████▉  | 423/537 [29:51<06:58,  3.67s/it, running train loss: 0.63568]\u001b[A\n",
            "Trainging:  79%|███████▉  | 423/537 [29:55<06:58,  3.67s/it, running train loss: 0.54778]\u001b[A\n",
            "Trainging:  79%|███████▉  | 424/537 [29:55<07:17,  3.87s/it, running train loss: 0.54778]\u001b[A\n",
            "Trainging:  79%|███████▉  | 424/537 [29:59<07:17,  3.87s/it, running train loss: 0.49518]\u001b[A\n",
            "Trainging:  79%|███████▉  | 425/537 [29:59<07:07,  3.82s/it, running train loss: 0.49518]\u001b[A\n",
            "Trainging:  79%|███████▉  | 425/537 [30:05<07:07,  3.82s/it, running train loss: 0.52211]\u001b[A\n",
            "Trainging:  79%|███████▉  | 426/537 [30:05<08:01,  4.34s/it, running train loss: 0.52211]\u001b[A\n",
            "Trainging:  79%|███████▉  | 426/537 [30:09<08:01,  4.34s/it, running train loss: 0.55538]\u001b[A\n",
            "Trainging:  80%|███████▉  | 427/537 [30:09<07:57,  4.34s/it, running train loss: 0.55538]\u001b[A\n",
            "Trainging:  80%|███████▉  | 427/537 [30:13<07:57,  4.34s/it, running train loss: 0.56854]\u001b[A\n",
            "Trainging:  80%|███████▉  | 428/537 [30:13<07:31,  4.14s/it, running train loss: 0.56854]\u001b[A\n",
            "Trainging:  80%|███████▉  | 428/537 [30:16<07:31,  4.14s/it, running train loss: 0.40969]\u001b[A\n",
            "Trainging:  80%|███████▉  | 429/537 [30:16<07:04,  3.93s/it, running train loss: 0.40969]\u001b[A\n",
            "Trainging:  80%|███████▉  | 429/537 [30:20<07:04,  3.93s/it, running train loss: 0.57652]\u001b[A\n",
            "Trainging:  80%|████████  | 430/537 [30:20<06:56,  3.89s/it, running train loss: 0.57652]\u001b[A\n",
            "Trainging:  80%|████████  | 430/537 [30:24<06:56,  3.89s/it, running train loss: 0.55130]\u001b[A\n",
            "Trainging:  80%|████████  | 431/537 [30:24<06:51,  3.88s/it, running train loss: 0.55130]\u001b[A\n",
            "Trainging:  80%|████████  | 431/537 [30:27<06:51,  3.88s/it, running train loss: 0.55413]\u001b[A\n",
            "Trainging:  80%|████████  | 432/537 [30:27<06:38,  3.80s/it, running train loss: 0.55413]\u001b[A\n",
            "Trainging:  80%|████████  | 432/537 [30:32<06:38,  3.80s/it, running train loss: 0.49032]\u001b[A\n",
            "Trainging:  81%|████████  | 433/537 [30:32<06:52,  3.97s/it, running train loss: 0.49032]\u001b[A\n",
            "Trainging:  81%|████████  | 433/537 [30:35<06:52,  3.97s/it, running train loss: 0.61134]\u001b[A\n",
            "Trainging:  81%|████████  | 434/537 [30:35<06:40,  3.89s/it, running train loss: 0.61134]\u001b[A\n",
            "Trainging:  81%|████████  | 434/537 [30:39<06:40,  3.89s/it, running train loss: 0.47966]\u001b[A\n",
            "Trainging:  81%|████████  | 435/537 [30:39<06:26,  3.79s/it, running train loss: 0.47966]\u001b[A\n",
            "Trainging:  81%|████████  | 435/537 [30:43<06:26,  3.79s/it, running train loss: 0.44137]\u001b[A\n",
            "Trainging:  81%|████████  | 436/537 [30:43<06:28,  3.85s/it, running train loss: 0.44137]\u001b[A\n",
            "Trainging:  81%|████████  | 436/537 [30:46<06:28,  3.85s/it, running train loss: 0.53282]\u001b[A\n",
            "Trainging:  81%|████████▏ | 437/537 [30:46<06:14,  3.74s/it, running train loss: 0.53282]\u001b[A\n",
            "Trainging:  81%|████████▏ | 437/537 [30:51<06:14,  3.74s/it, running train loss: 0.52233]\u001b[A\n",
            "Trainging:  82%|████████▏ | 438/537 [30:51<06:27,  3.91s/it, running train loss: 0.52233]\u001b[A\n",
            "Trainging:  82%|████████▏ | 438/537 [30:54<06:27,  3.91s/it, running train loss: 0.47998]\u001b[A\n",
            "Trainging:  82%|████████▏ | 439/537 [30:55<06:21,  3.89s/it, running train loss: 0.47998]\u001b[A\n",
            "Trainging:  82%|████████▏ | 439/537 [30:58<06:21,  3.89s/it, running train loss: 0.48463]\u001b[A\n",
            "Trainging:  82%|████████▏ | 440/537 [30:58<06:11,  3.83s/it, running train loss: 0.48463]\u001b[A\n",
            "Trainging:  82%|████████▏ | 440/537 [31:02<06:11,  3.83s/it, running train loss: 0.58208]\u001b[A\n",
            "Trainging:  82%|████████▏ | 441/537 [31:02<06:06,  3.82s/it, running train loss: 0.58208]\u001b[A\n",
            "Trainging:  82%|████████▏ | 441/537 [31:05<06:06,  3.82s/it, running train loss: 0.53979]\u001b[A\n",
            "Trainging:  82%|████████▏ | 442/537 [31:05<05:51,  3.70s/it, running train loss: 0.53979]\u001b[A\n",
            "Trainging:  82%|████████▏ | 442/537 [31:09<05:51,  3.70s/it, running train loss: 0.52027]\u001b[A\n",
            "Trainging:  82%|████████▏ | 443/537 [31:09<05:44,  3.66s/it, running train loss: 0.52027]\u001b[A\n",
            "Trainging:  82%|████████▏ | 443/537 [31:12<05:44,  3.66s/it, running train loss: 0.46840]\u001b[A\n",
            "Trainging:  83%|████████▎ | 444/537 [31:12<05:34,  3.60s/it, running train loss: 0.46840]\u001b[A\n",
            "Trainging:  83%|████████▎ | 444/537 [31:16<05:34,  3.60s/it, running train loss: 0.49662]\u001b[A\n",
            "Trainging:  83%|████████▎ | 445/537 [31:16<05:20,  3.48s/it, running train loss: 0.49662]\u001b[A\n",
            "Trainging:  83%|████████▎ | 445/537 [31:19<05:20,  3.48s/it, running train loss: 0.36593]\u001b[A\n",
            "Trainging:  83%|████████▎ | 446/537 [31:19<05:18,  3.51s/it, running train loss: 0.36593]\u001b[A\n",
            "Trainging:  83%|████████▎ | 446/537 [31:23<05:18,  3.51s/it, running train loss: 0.50795]\u001b[A\n",
            "Trainging:  83%|████████▎ | 447/537 [31:23<05:16,  3.52s/it, running train loss: 0.50795]\u001b[A\n",
            "Trainging:  83%|████████▎ | 447/537 [31:26<05:16,  3.52s/it, running train loss: 0.43443]\u001b[A\n",
            "Trainging:  83%|████████▎ | 448/537 [31:26<05:13,  3.53s/it, running train loss: 0.43443]\u001b[A\n",
            "Trainging:  83%|████████▎ | 448/537 [31:30<05:13,  3.53s/it, running train loss: 0.48948]\u001b[A\n",
            "Trainging:  84%|████████▎ | 449/537 [31:30<05:22,  3.67s/it, running train loss: 0.48948]\u001b[A\n",
            "Trainging:  84%|████████▎ | 449/537 [31:34<05:22,  3.67s/it, running train loss: 0.52595]\u001b[A\n",
            "Trainging:  84%|████████▍ | 450/537 [31:34<05:27,  3.77s/it, running train loss: 0.52595]\u001b[A\n",
            "Trainging:  84%|████████▍ | 450/537 [31:39<05:27,  3.77s/it, running train loss: 0.58763]\u001b[A\n",
            "Trainging:  84%|████████▍ | 451/537 [31:39<05:43,  4.00s/it, running train loss: 0.58763]\u001b[A\n",
            "Trainging:  84%|████████▍ | 451/537 [31:42<05:43,  4.00s/it, running train loss: 0.49184]\u001b[A\n",
            "Trainging:  84%|████████▍ | 452/537 [31:42<05:28,  3.86s/it, running train loss: 0.49184]\u001b[A\n",
            "Trainging:  84%|████████▍ | 452/537 [31:46<05:28,  3.86s/it, running train loss: 0.49439]\u001b[A\n",
            "Trainging:  84%|████████▍ | 453/537 [31:46<05:28,  3.91s/it, running train loss: 0.49439]\u001b[A\n",
            "Trainging:  84%|████████▍ | 453/537 [31:50<05:28,  3.91s/it, running train loss: 0.51246]\u001b[A\n",
            "Trainging:  85%|████████▍ | 454/537 [31:50<05:09,  3.73s/it, running train loss: 0.51246]\u001b[A\n",
            "Trainging:  85%|████████▍ | 454/537 [31:54<05:09,  3.73s/it, running train loss: 0.57456]\u001b[A\n",
            "Trainging:  85%|████████▍ | 455/537 [31:54<05:24,  3.96s/it, running train loss: 0.57456]\u001b[A\n",
            "Trainging:  85%|████████▍ | 455/537 [31:58<05:24,  3.96s/it, running train loss: 0.51838]\u001b[A\n",
            "Trainging:  85%|████████▍ | 456/537 [31:58<05:16,  3.91s/it, running train loss: 0.51838]\u001b[A\n",
            "Trainging:  85%|████████▍ | 456/537 [32:02<05:16,  3.91s/it, running train loss: 0.52621]\u001b[A\n",
            "Trainging:  85%|████████▌ | 457/537 [32:02<05:07,  3.85s/it, running train loss: 0.52621]\u001b[A\n",
            "Trainging:  85%|████████▌ | 457/537 [32:06<05:07,  3.85s/it, running train loss: 0.51749]\u001b[A\n",
            "Trainging:  85%|████████▌ | 458/537 [32:06<05:03,  3.85s/it, running train loss: 0.51749]\u001b[A\n",
            "Trainging:  85%|████████▌ | 458/537 [32:09<05:03,  3.85s/it, running train loss: 0.46449]\u001b[A\n",
            "Trainging:  85%|████████▌ | 459/537 [32:09<05:00,  3.85s/it, running train loss: 0.46449]\u001b[A\n",
            "Trainging:  85%|████████▌ | 459/537 [32:13<05:00,  3.85s/it, running train loss: 0.57927]\u001b[A\n",
            "Trainging:  86%|████████▌ | 460/537 [32:13<04:52,  3.81s/it, running train loss: 0.57927]\u001b[A\n",
            "Trainging:  86%|████████▌ | 460/537 [32:18<04:52,  3.81s/it, running train loss: 0.50335]\u001b[A\n",
            "Trainging:  86%|████████▌ | 461/537 [32:18<05:13,  4.12s/it, running train loss: 0.50335]\u001b[A\n",
            "Trainging:  86%|████████▌ | 461/537 [32:22<05:13,  4.12s/it, running train loss: 0.61880]\u001b[A\n",
            "Trainging:  86%|████████▌ | 462/537 [32:22<05:01,  4.02s/it, running train loss: 0.61880]\u001b[A\n",
            "Trainging:  86%|████████▌ | 462/537 [32:26<05:01,  4.02s/it, running train loss: 0.51312]\u001b[A\n",
            "Trainging:  86%|████████▌ | 463/537 [32:26<05:08,  4.17s/it, running train loss: 0.51312]\u001b[A\n",
            "Trainging:  86%|████████▌ | 463/537 [32:30<05:08,  4.17s/it, running train loss: 0.54580]\u001b[A\n",
            "Trainging:  86%|████████▋ | 464/537 [32:30<04:59,  4.10s/it, running train loss: 0.54580]\u001b[A\n",
            "Trainging:  86%|████████▋ | 464/537 [32:34<04:59,  4.10s/it, running train loss: 0.48388]\u001b[A\n",
            "Trainging:  87%|████████▋ | 465/537 [32:34<04:45,  3.97s/it, running train loss: 0.48388]\u001b[A\n",
            "Trainging:  87%|████████▋ | 465/537 [32:39<04:45,  3.97s/it, running train loss: 0.52822]\u001b[A\n",
            "Trainging:  87%|████████▋ | 466/537 [32:39<04:58,  4.21s/it, running train loss: 0.52822]\u001b[A\n",
            "Trainging:  87%|████████▋ | 466/537 [32:43<04:58,  4.21s/it, running train loss: 0.51231]\u001b[A\n",
            "Trainging:  87%|████████▋ | 467/537 [32:43<04:50,  4.14s/it, running train loss: 0.51231]\u001b[A\n",
            "Trainging:  87%|████████▋ | 467/537 [32:48<04:50,  4.14s/it, running train loss: 0.47359]\u001b[A\n",
            "Trainging:  87%|████████▋ | 468/537 [32:48<05:02,  4.39s/it, running train loss: 0.47359]\u001b[A\n",
            "Trainging:  87%|████████▋ | 468/537 [32:52<05:02,  4.39s/it, running train loss: 0.54116]\u001b[A\n",
            "Trainging:  87%|████████▋ | 469/537 [32:52<04:56,  4.35s/it, running train loss: 0.54116]\u001b[A\n",
            "Trainging:  87%|████████▋ | 469/537 [32:56<04:56,  4.35s/it, running train loss: 0.43448]\u001b[A\n",
            "Trainging:  88%|████████▊ | 470/537 [32:56<04:44,  4.25s/it, running train loss: 0.43448]\u001b[A\n",
            "Trainging:  88%|████████▊ | 470/537 [33:01<04:44,  4.25s/it, running train loss: 0.64985]\u001b[A\n",
            "Trainging:  88%|████████▊ | 471/537 [33:01<04:52,  4.43s/it, running train loss: 0.64985]\u001b[A\n",
            "Trainging:  88%|████████▊ | 471/537 [33:05<04:52,  4.43s/it, running train loss: 0.60344]\u001b[A\n",
            "Trainging:  88%|████████▊ | 472/537 [33:05<04:39,  4.30s/it, running train loss: 0.60344]\u001b[A\n",
            "Trainging:  88%|████████▊ | 472/537 [33:08<04:39,  4.30s/it, running train loss: 0.44870]\u001b[A\n",
            "Trainging:  88%|████████▊ | 473/537 [33:08<04:21,  4.09s/it, running train loss: 0.44870]\u001b[A\n",
            "Trainging:  88%|████████▊ | 473/537 [33:13<04:21,  4.09s/it, running train loss: 0.52243]\u001b[A\n",
            "Trainging:  88%|████████▊ | 474/537 [33:13<04:21,  4.15s/it, running train loss: 0.52243]\u001b[A\n",
            "Trainging:  88%|████████▊ | 474/537 [33:16<04:21,  4.15s/it, running train loss: 0.55249]\u001b[A\n",
            "Trainging:  88%|████████▊ | 475/537 [33:16<04:08,  4.00s/it, running train loss: 0.55249]\u001b[A\n",
            "Trainging:  88%|████████▊ | 475/537 [33:20<04:08,  4.00s/it, running train loss: 0.55795]\u001b[A\n",
            "Trainging:  89%|████████▊ | 476/537 [33:20<04:02,  3.98s/it, running train loss: 0.55795]\u001b[A\n",
            "Trainging:  89%|████████▊ | 476/537 [33:24<04:02,  3.98s/it, running train loss: 0.60920]\u001b[A\n",
            "Trainging:  89%|████████▉ | 477/537 [33:24<03:48,  3.81s/it, running train loss: 0.60920]\u001b[A\n",
            "Trainging:  89%|████████▉ | 477/537 [33:28<03:48,  3.81s/it, running train loss: 0.51657]\u001b[A\n",
            "Trainging:  89%|████████▉ | 478/537 [33:28<03:58,  4.05s/it, running train loss: 0.51657]\u001b[A\n",
            "Trainging:  89%|████████▉ | 478/537 [33:33<03:58,  4.05s/it, running train loss: 0.43578]\u001b[A\n",
            "Trainging:  89%|████████▉ | 479/537 [33:33<04:00,  4.15s/it, running train loss: 0.43578]\u001b[A\n",
            "Trainging:  89%|████████▉ | 479/537 [33:37<04:00,  4.15s/it, running train loss: 0.48748]\u001b[A\n",
            "Trainging:  89%|████████▉ | 480/537 [33:37<03:52,  4.07s/it, running train loss: 0.48748]\u001b[A\n",
            "Trainging:  89%|████████▉ | 480/537 [33:41<03:52,  4.07s/it, running train loss: 0.59078]\u001b[A\n",
            "Trainging:  90%|████████▉ | 481/537 [33:41<03:56,  4.22s/it, running train loss: 0.59078]\u001b[A\n",
            "Trainging:  90%|████████▉ | 481/537 [33:46<03:56,  4.22s/it, running train loss: 0.46182]\u001b[A\n",
            "Trainging:  90%|████████▉ | 482/537 [33:46<03:59,  4.36s/it, running train loss: 0.46182]\u001b[A\n",
            "Trainging:  90%|████████▉ | 482/537 [33:49<03:59,  4.36s/it, running train loss: 0.54956]\u001b[A\n",
            "Trainging:  90%|████████▉ | 483/537 [33:49<03:39,  4.07s/it, running train loss: 0.54956]\u001b[A\n",
            "Trainging:  90%|████████▉ | 483/537 [33:54<03:39,  4.07s/it, running train loss: 0.53248]\u001b[A\n",
            "Trainging:  90%|█████████ | 484/537 [33:54<03:42,  4.20s/it, running train loss: 0.53248]\u001b[A\n",
            "Trainging:  90%|█████████ | 484/537 [33:58<03:42,  4.20s/it, running train loss: 0.47779]\u001b[A\n",
            "Trainging:  90%|█████████ | 485/537 [33:58<03:34,  4.12s/it, running train loss: 0.47779]\u001b[A\n",
            "Trainging:  90%|█████████ | 485/537 [34:01<03:34,  4.12s/it, running train loss: 0.51962]\u001b[A\n",
            "Trainging:  91%|█████████ | 486/537 [34:01<03:21,  3.95s/it, running train loss: 0.51962]\u001b[A\n",
            "Trainging:  91%|█████████ | 486/537 [34:03<03:21,  3.95s/it, running train loss: 0.61010]\u001b[A\n",
            "Trainging:  91%|█████████ | 487/537 [34:03<02:51,  3.43s/it, running train loss: 0.61010]\u001b[A\n",
            "Trainging:  91%|█████████ | 487/537 [34:06<02:51,  3.43s/it, running train loss: 0.45962]\u001b[A\n",
            "Trainging:  91%|█████████ | 488/537 [34:06<02:30,  3.08s/it, running train loss: 0.45962]\u001b[A\n",
            "Trainging:  91%|█████████ | 488/537 [34:09<02:30,  3.08s/it, running train loss: 0.61265]\u001b[A\n",
            "Trainging:  91%|█████████ | 489/537 [34:09<02:33,  3.20s/it, running train loss: 0.61265]\u001b[A\n",
            "Trainging:  91%|█████████ | 489/537 [34:12<02:33,  3.20s/it, running train loss: 0.52593]\u001b[A\n",
            "Trainging:  91%|█████████ | 490/537 [34:12<02:32,  3.25s/it, running train loss: 0.52593]\u001b[A\n",
            "Trainging:  91%|█████████ | 490/537 [34:17<02:32,  3.25s/it, running train loss: 0.55256]\u001b[A\n",
            "Trainging:  91%|█████████▏| 491/537 [34:17<02:46,  3.62s/it, running train loss: 0.55256]\u001b[A\n",
            "Trainging:  91%|█████████▏| 491/537 [34:22<02:46,  3.62s/it, running train loss: 0.59957]\u001b[A\n",
            "Trainging:  92%|█████████▏| 492/537 [34:22<03:06,  4.13s/it, running train loss: 0.59957]\u001b[A\n",
            "Trainging:  92%|█████████▏| 492/537 [34:26<03:06,  4.13s/it, running train loss: 0.64648]\u001b[A\n",
            "Trainging:  92%|█████████▏| 493/537 [34:26<02:51,  3.90s/it, running train loss: 0.64648]\u001b[A\n",
            "Trainging:  92%|█████████▏| 493/537 [34:31<02:51,  3.90s/it, running train loss: 0.60624]\u001b[A\n",
            "Trainging:  92%|█████████▏| 494/537 [34:31<03:07,  4.35s/it, running train loss: 0.60624]\u001b[A\n",
            "Trainging:  92%|█████████▏| 494/537 [34:35<03:07,  4.35s/it, running train loss: 0.54138]\u001b[A\n",
            "Trainging:  92%|█████████▏| 495/537 [34:35<02:56,  4.21s/it, running train loss: 0.54138]\u001b[A\n",
            "Trainging:  92%|█████████▏| 495/537 [34:39<02:56,  4.21s/it, running train loss: 0.54305]\u001b[A\n",
            "Trainging:  92%|█████████▏| 496/537 [34:39<02:46,  4.05s/it, running train loss: 0.54305]\u001b[A\n",
            "Trainging:  92%|█████████▏| 496/537 [34:43<02:46,  4.05s/it, running train loss: 0.52761]\u001b[A\n",
            "Trainging:  93%|█████████▎| 497/537 [34:43<02:40,  4.02s/it, running train loss: 0.52761]\u001b[A\n",
            "Trainging:  93%|█████████▎| 497/537 [34:46<02:40,  4.02s/it, running train loss: 0.50966]\u001b[A\n",
            "Trainging:  93%|█████████▎| 498/537 [34:46<02:32,  3.90s/it, running train loss: 0.50966]\u001b[A\n",
            "Trainging:  93%|█████████▎| 498/537 [34:51<02:32,  3.90s/it, running train loss: 0.53740]\u001b[A\n",
            "Trainging:  93%|█████████▎| 499/537 [34:51<02:34,  4.06s/it, running train loss: 0.53740]\u001b[A\n",
            "Trainging:  93%|█████████▎| 499/537 [34:55<02:34,  4.06s/it, running train loss: 0.50977]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> EMA starting .....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|▏         | 1/68 [00:00<00:41,  1.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 2/68 [00:01<00:33,  1.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 3/68 [00:01<00:28,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 4/68 [00:01<00:27,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 5/68 [00:02<00:27,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 6/68 [00:02<00:24,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|█         | 7/68 [00:02<00:24,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 8/68 [00:03<00:24,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 9/68 [00:03<00:24,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 10/68 [00:04<00:23,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 11/68 [00:04<00:23,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 12/68 [00:05<00:23,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 13/68 [00:05<00:23,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 14/68 [00:05<00:22,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 15/68 [00:06<00:22,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▎       | 16/68 [00:06<00:22,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▌       | 17/68 [00:07<00:20,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 18/68 [00:07<00:20,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 19/68 [00:07<00:18,  2.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 20/68 [00:08<00:18,  2.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 21/68 [00:08<00:18,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 22/68 [00:09<00:17,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 23/68 [00:09<00:18,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▌      | 24/68 [00:09<00:16,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 25/68 [00:10<00:16,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 26/68 [00:10<00:17,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 27/68 [00:11<00:16,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████      | 28/68 [00:11<00:15,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 29/68 [00:11<00:15,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 30/68 [00:12<00:15,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▌     | 31/68 [00:12<00:13,  2.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 32/68 [00:13<00:14,  2.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▊     | 33/68 [00:13<00:14,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 34/68 [00:13<00:13,  2.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████▏    | 35/68 [00:14<00:12,  2.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 36/68 [00:14<00:12,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▍    | 37/68 [00:15<00:12,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 38/68 [00:15<00:12,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 39/68 [00:15<00:11,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▉    | 40/68 [00:16<00:11,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 41/68 [00:16<00:10,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 42/68 [00:17<00:10,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 43/68 [00:17<00:10,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▍   | 44/68 [00:17<00:09,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 45/68 [00:18<00:09,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 46/68 [00:18<00:09,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 47/68 [00:19<00:08,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 48/68 [00:19<00:08,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 49/68 [00:20<00:08,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 50/68 [00:20<00:07,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 51/68 [00:20<00:07,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▋  | 52/68 [00:21<00:06,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 53/68 [00:21<00:06,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 54/68 [00:22<00:05,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 55/68 [00:22<00:05,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 56/68 [00:22<00:04,  2.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 57/68 [00:23<00:04,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 58/68 [00:23<00:04,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 59/68 [00:24<00:03,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 60/68 [00:24<00:03,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|████████▉ | 61/68 [00:24<00:02,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 62/68 [00:25<00:02,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 63/68 [00:25<00:02,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 64/68 [00:26<00:01,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 65/68 [00:26<00:01,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 66/68 [00:26<00:00,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▊| 67/68 [00:27<00:00,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 68/68 [00:27<00:00,  2.45it/s]\n",
            "\n",
            "Trainging:  93%|█████████▎| 500/537 [35:24<07:55, 12.85s/it, running train loss: 0.50977]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> training loss: 0.528752, valid loss: 0.508450,valid f1: 0.525030, valid acc:0.720806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Trainging:  93%|█████████▎| 500/537 [35:28<07:55, 12.85s/it, running train loss: 0.59890]\u001b[A\n",
            "Trainging:  93%|█████████▎| 501/537 [35:28<06:04, 10.12s/it, running train loss: 0.59890]\u001b[A\n",
            "Trainging:  93%|█████████▎| 501/537 [35:31<06:04, 10.12s/it, running train loss: 0.48773]\u001b[A\n",
            "Trainging:  93%|█████████▎| 502/537 [35:31<04:44,  8.12s/it, running train loss: 0.48773]\u001b[A\n",
            "Trainging:  93%|█████████▎| 502/537 [35:37<04:44,  8.12s/it, running train loss: 0.51034]\u001b[A\n",
            "Trainging:  94%|█████████▎| 503/537 [35:37<04:13,  7.46s/it, running train loss: 0.51034]\u001b[A\n",
            "Trainging:  94%|█████████▎| 503/537 [35:41<04:13,  7.46s/it, running train loss: 0.55732]\u001b[A\n",
            "Trainging:  94%|█████████▍| 504/537 [35:41<03:28,  6.33s/it, running train loss: 0.55732]\u001b[A\n",
            "Trainging:  94%|█████████▍| 504/537 [35:46<03:28,  6.33s/it, running train loss: 0.56393]\u001b[A\n",
            "Trainging:  94%|█████████▍| 505/537 [35:46<03:10,  5.96s/it, running train loss: 0.56393]\u001b[A\n",
            "Trainging:  94%|█████████▍| 505/537 [35:50<03:10,  5.96s/it, running train loss: 0.45962]\u001b[A\n",
            "Trainging:  94%|█████████▍| 506/537 [35:50<02:43,  5.28s/it, running train loss: 0.45962]\u001b[A\n",
            "Trainging:  94%|█████████▍| 506/537 [35:54<02:43,  5.28s/it, running train loss: 0.45491]\u001b[A\n",
            "Trainging:  94%|█████████▍| 507/537 [35:54<02:26,  4.87s/it, running train loss: 0.45491]\u001b[A\n",
            "Trainging:  94%|█████████▍| 507/537 [35:57<02:26,  4.87s/it, running train loss: 0.56994]\u001b[A\n",
            "Trainging:  95%|█████████▍| 508/537 [35:57<02:10,  4.49s/it, running train loss: 0.56994]\u001b[A\n",
            "Trainging:  95%|█████████▍| 508/537 [36:02<02:10,  4.49s/it, running train loss: 0.45953]\u001b[A\n",
            "Trainging:  95%|█████████▍| 509/537 [36:02<02:06,  4.53s/it, running train loss: 0.45953]\u001b[A\n",
            "Trainging:  95%|█████████▍| 509/537 [36:05<02:06,  4.53s/it, running train loss: 0.49372]\u001b[A\n",
            "Trainging:  95%|█████████▍| 510/537 [36:05<01:54,  4.26s/it, running train loss: 0.49372]\u001b[A\n",
            "Trainging:  95%|█████████▍| 510/537 [36:09<01:54,  4.26s/it, running train loss: 0.53921]\u001b[A\n",
            "Trainging:  95%|█████████▌| 511/537 [36:09<01:47,  4.14s/it, running train loss: 0.53921]\u001b[A\n",
            "Trainging:  95%|█████████▌| 511/537 [36:15<01:47,  4.14s/it, running train loss: 0.45824]\u001b[A\n",
            "Trainging:  95%|█████████▌| 512/537 [36:15<01:58,  4.74s/it, running train loss: 0.45824]\u001b[A\n",
            "Trainging:  95%|█████████▌| 512/537 [36:19<01:58,  4.74s/it, running train loss: 0.39312]\u001b[A\n",
            "Trainging:  96%|█████████▌| 513/537 [36:19<01:44,  4.36s/it, running train loss: 0.39312]\u001b[A\n",
            "Trainging:  96%|█████████▌| 513/537 [36:23<01:44,  4.36s/it, running train loss: 0.58530]\u001b[A\n",
            "Trainging:  96%|█████████▌| 514/537 [36:23<01:35,  4.17s/it, running train loss: 0.58530]\u001b[A\n",
            "Trainging:  96%|█████████▌| 514/537 [36:26<01:35,  4.17s/it, running train loss: 0.44700]\u001b[A\n",
            "Trainging:  96%|█████████▌| 515/537 [36:26<01:26,  3.92s/it, running train loss: 0.44700]\u001b[A\n",
            "Trainging:  96%|█████████▌| 515/537 [36:30<01:26,  3.92s/it, running train loss: 0.48986]\u001b[A\n",
            "Trainging:  96%|█████████▌| 516/537 [36:30<01:25,  4.06s/it, running train loss: 0.48986]\u001b[A\n",
            "Trainging:  96%|█████████▌| 516/537 [36:34<01:25,  4.06s/it, running train loss: 0.51272]\u001b[A\n",
            "Trainging:  96%|█████████▋| 517/537 [36:34<01:18,  3.92s/it, running train loss: 0.51272]\u001b[A\n",
            "Trainging:  96%|█████████▋| 517/537 [36:40<01:18,  3.92s/it, running train loss: 0.66785]\u001b[A\n",
            "Trainging:  96%|█████████▋| 518/537 [36:40<01:27,  4.59s/it, running train loss: 0.66785]\u001b[A\n",
            "Trainging:  96%|█████████▋| 518/537 [36:44<01:27,  4.59s/it, running train loss: 0.51037]\u001b[A\n",
            "Trainging:  97%|█████████▋| 519/537 [36:44<01:17,  4.30s/it, running train loss: 0.51037]\u001b[A\n",
            "Trainging:  97%|█████████▋| 519/537 [36:47<01:17,  4.30s/it, running train loss: 0.52458]\u001b[A\n",
            "Trainging:  97%|█████████▋| 520/537 [36:47<01:09,  4.08s/it, running train loss: 0.52458]\u001b[A\n",
            "Trainging:  97%|█████████▋| 520/537 [36:52<01:09,  4.08s/it, running train loss: 0.50643]\u001b[A\n",
            "Trainging:  97%|█████████▋| 521/537 [36:52<01:06,  4.15s/it, running train loss: 0.50643]\u001b[A\n",
            "Trainging:  97%|█████████▋| 521/537 [36:57<01:06,  4.15s/it, running train loss: 0.43124]\u001b[A\n",
            "Trainging:  97%|█████████▋| 522/537 [36:57<01:06,  4.41s/it, running train loss: 0.43124]\u001b[A\n",
            "Trainging:  97%|█████████▋| 522/537 [37:00<01:06,  4.41s/it, running train loss: 0.54398]\u001b[A\n",
            "Trainging:  97%|█████████▋| 523/537 [37:00<00:58,  4.15s/it, running train loss: 0.54398]\u001b[A\n",
            "Trainging:  97%|█████████▋| 523/537 [37:04<00:58,  4.15s/it, running train loss: 0.53666]\u001b[A\n",
            "Trainging:  98%|█████████▊| 524/537 [37:04<00:51,  3.96s/it, running train loss: 0.53666]\u001b[A\n",
            "Trainging:  98%|█████████▊| 524/537 [37:08<00:51,  3.96s/it, running train loss: 0.46254]\u001b[A\n",
            "Trainging:  98%|█████████▊| 525/537 [37:08<00:47,  3.96s/it, running train loss: 0.46254]\u001b[A\n",
            "Trainging:  98%|█████████▊| 525/537 [37:11<00:47,  3.96s/it, running train loss: 0.54715]\u001b[A\n",
            "Trainging:  98%|█████████▊| 526/537 [37:11<00:43,  3.95s/it, running train loss: 0.54715]\u001b[A\n",
            "Trainging:  98%|█████████▊| 526/537 [37:16<00:43,  3.95s/it, running train loss: 0.51325]\u001b[A\n",
            "Trainging:  98%|█████████▊| 527/537 [37:16<00:40,  4.06s/it, running train loss: 0.51325]\u001b[A\n",
            "Trainging:  98%|█████████▊| 527/537 [37:19<00:40,  4.06s/it, running train loss: 0.48419]\u001b[A\n",
            "Trainging:  98%|█████████▊| 528/537 [37:19<00:35,  3.91s/it, running train loss: 0.48419]\u001b[A\n",
            "Trainging:  98%|█████████▊| 528/537 [37:24<00:35,  3.91s/it, running train loss: 0.57385]\u001b[A\n",
            "Trainging:  99%|█████████▊| 529/537 [37:24<00:34,  4.25s/it, running train loss: 0.57385]\u001b[A\n",
            "Trainging:  99%|█████████▊| 529/537 [37:28<00:34,  4.25s/it, running train loss: 0.51870]\u001b[A\n",
            "Trainging:  99%|█████████▊| 530/537 [37:28<00:29,  4.15s/it, running train loss: 0.51870]\u001b[A\n",
            "Trainging:  99%|█████████▊| 530/537 [37:33<00:29,  4.15s/it, running train loss: 0.51028]\u001b[A\n",
            "Trainging:  99%|█████████▉| 531/537 [37:33<00:25,  4.24s/it, running train loss: 0.51028]\u001b[A\n",
            "Trainging:  99%|█████████▉| 531/537 [37:36<00:25,  4.24s/it, running train loss: 0.50982]\u001b[A\n",
            "Trainging:  99%|█████████▉| 532/537 [37:36<00:20,  4.00s/it, running train loss: 0.50982]\u001b[A\n",
            "Trainging:  99%|█████████▉| 532/537 [37:40<00:20,  4.00s/it, running train loss: 0.49961]\u001b[A\n",
            "Trainging:  99%|█████████▉| 533/537 [37:40<00:16,  4.02s/it, running train loss: 0.49961]\u001b[A\n",
            "Trainging:  99%|█████████▉| 533/537 [37:44<00:16,  4.02s/it, running train loss: 0.55173]\u001b[A\n",
            "Trainging:  99%|█████████▉| 534/537 [37:44<00:11,  3.91s/it, running train loss: 0.55173]\u001b[A\n",
            "Trainging:  99%|█████████▉| 534/537 [37:48<00:11,  3.91s/it, running train loss: 0.52598]\u001b[A\n",
            "Trainging: 100%|█████████▉| 535/537 [37:48<00:07,  3.90s/it, running train loss: 0.52598]\u001b[A\n",
            "Trainging: 100%|█████████▉| 535/537 [37:52<00:07,  3.90s/it, running train loss: 0.48228]\u001b[A\n",
            "Trainging: 100%|█████████▉| 536/537 [37:52<00:04,  4.11s/it, running train loss: 0.48228]\u001b[A\n",
            "Trainging: 100%|█████████▉| 536/537 [37:56<00:04,  4.11s/it, running train loss: 0.60387]\u001b[A\n",
            "Trainging: 100%|██████████| 537/537 [37:56<00:00,  4.24s/it, running train loss: 0.60387]\n",
            "100%|██████████| 1/1 [37:56<00:00, 2276.52s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(BertForSequenceClassification(\n",
              "   (bert): BertModel(\n",
              "     (embeddings): BertEmbeddings(\n",
              "       (word_embeddings): Embedding(21128, 768, padding_idx=1)\n",
              "       (position_embeddings): Embedding(512, 768)\n",
              "       (token_type_embeddings): Embedding(2, 768)\n",
              "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "     )\n",
              "     (encoder): BertEncoder(\n",
              "       (layer): ModuleList(\n",
              "         (0): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (1): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (2): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (3): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (4): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (5): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (6): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (7): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (8): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (9): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (10): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (11): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "       )\n",
              "     )\n",
              "     (pooler): BertPooler(\n",
              "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "       (activation): Tanh()\n",
              "     )\n",
              "   )\n",
              "   (dropout): Dropout(p=0.1, inplace=False)\n",
              "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              " ), './checkpoint- 500 - 0.720806')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}