{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_self_AFQMC_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dVXLkF-4_zR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aba0b58-2851-4b1a-dd76-70020f7873df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  2 13:20:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "#设置路径\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "9KtM0UMx_8TA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0d0ec9-5a98-4fee-b298-7ca9a6d7002b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers==4.0.1"
      ],
      "metadata": {
        "id": "VE9Fr9z4_-l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7882474e-d831-4a3e-e8d7-b5fb52d183a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.0.1\n",
            "  Downloading transformers-4.0.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (21.3)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 28.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.50.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.0.1) (3.0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
            "Collecting click==8.0\n",
            "  Downloading click-8.0.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.50-py3-none-any.whl size=895166 sha256=3fab6d2b830925aec825551543bb7a667ae7aaaeaf262b29cd6b05a3d5962b15\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/72/54/519f0d5143cc6c73fa3297509123c86fc8586a7fdea8d25311\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: click, tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed click-8.0.0 sacremoses-0.0.50 tokenizers-0.9.4 transformers-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch版本为1.6\n",
        "! pip install torch==1.6.0"
      ],
      "metadata": {
        "id": "CGn36OdbAAN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357cd3d9-95ff-45b2-bc90-7b5004c8468c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 18 kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torchvision 是PyTorch中专门用来处理图像的库\n",
        "! pip install torchvision==0.7.0"
      ],
      "metadata": {
        "id": "k4GiZN9pAHqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3afc0cde-7544-4670-a808-e032b5fb8bce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.7.0\n",
            "  Downloading torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (1.21.6)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->torchvision==0.7.0) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "Successfully installed torchvision-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "config = {\n",
        "    'train_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/train.json',\n",
        "    'dev_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/dev.json',\n",
        "    'test_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/test.json',\n",
        "    'model_path':'/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model',\n",
        "    'output_path': '.',\n",
        "    'train_val_ratio':0.1,\n",
        "    'vocab_size':30000,\n",
        "    'batch_size':64,\n",
        "    'max_seq_len':64,\n",
        "    'num_epochs':1,\n",
        "    'learning_rate':2e-5,\n",
        "    'eps': 0.1,\n",
        "    'alpha': 0.3,\n",
        "    'adv': 'fgm',\n",
        "    'warmup_ratio': 0.05,\n",
        "    'weight_decay': 0.01,\n",
        "    'use_bucket': True,\n",
        "    'bucket_multiplier': 200,\n",
        "    'n_gpus': 0,\n",
        "    'use_amp': True, # 只针对有 tensor core 的gpu有效\n",
        "    'ema_start_step': 500,\n",
        "    'ema_start': False,\n",
        "    'logging_step':100,\n",
        "    'device': 'cuda',\n",
        "    'seed':2022\n",
        "}\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  config['device'] = 'cpu'\n",
        "else:\n",
        "  config['n_gpus'] = torch.cuda.device_count()\n",
        "  config['batch_size'] *= config['n_gpus']\n",
        "\n",
        "if not os.path.exists(config['output_path']):\n",
        "    os.makedirs((config['output_path']))\n",
        "\n",
        "    \n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  return seed\n",
        "\n",
        "seed_everything(config['seed'])"
      ],
      "metadata": {
        "id": "wf9S2cB1ASJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96b7130-1b0f-4181-fcdc-5cde44022a8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2022"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data(path, data_type='train'):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "\n",
        "  with open(path, 'r', encoding = 'utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      if data_type != 'test':\n",
        "        labels.append(int(line['label']))\n",
        "      else:\n",
        "        labels.append(0)\n",
        "\n",
        "  df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns = ['text_a', 'text_b', 'labels'])\n",
        "  return df"
      ],
      "metadata": {
        "id": "uI_qfoiepHf-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## encode和encode_plus的区别\n",
        "1. encode仅返回input_ids\n",
        "2. encode_plus返回所有的编码信息，具体如下：\n",
        "’input_ids:是单词在词典中的编码; \n",
        "‘token_type_ids’:区分两个句子的编码（上句全为0，下句全为1）; \n",
        "‘attention_mask’:指定对哪些词进行self-Attention操作\n",
        "\n",
        "```\n",
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "# a.通过词典导入分词器\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "sentence = \"Hello, my son is laughing.\"\n",
        "\n",
        "print(tokenizer.encode(sentence))\n",
        "print(tokenizer.encode_plus(sentence))\n",
        "\n",
        "\n",
        "运行结果：\n",
        "\n",
        "[101, 7592, 1010, 2026, 2365, 2003, 5870, 1012, 102]\n",
        "{'input_ids': [101, 7592, 1010, 2026, 2365, 2003, 5870, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "```"
      ],
      "metadata": {
        "id": "TBd0MFciyFAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs: defaultdict(list)\n",
        "def build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
        "  # add_special_tokens [CLS] [SEP]\n",
        "  # return_token_type_ids 该词属于sentence_a(返回0) or sentence_b(返回1). \n",
        "  # return_attention_mask pad=0, 不是pad的部分标为1， 是pad标为0.\n",
        "  inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens = True,\n",
        "                     return_token_type_ids = True,\n",
        "                     return_attention_mask = True)\n",
        "  inputs['input_ids'].append(inputs_dict['input_ids'])\n",
        "  inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
        "  inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
        "  inputs['labels'].append(label)"
      ],
      "metadata": {
        "id": "MpNesXYtvHhB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## defaultdict(list)\n",
        "```\n",
        "from collections import defaultdict\n",
        "result = defaultdict(list)\n",
        "data = [(\"p\", 1), (\"p\", 2), (\"p\", 3),\n",
        "     (\"h\", 1), (\"h\", 2), (\"h\", 3)]\n",
        " \n",
        "for (key, value) in data:\n",
        "    result[key].append(value)\n",
        "print(result)#defaultdict(<class 'list'>, {'p': [1, 2, 3], 'h': [1, 2, 3]})\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "MAp0Hp7L2duD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "def read_data(config, tokenizer):\n",
        "  train_df = parse_data(config['train_file_path'], data_type = 'train')\n",
        "  dev_df = parse_data(config['dev_file_path'], data_type = 'dev')\n",
        "  test_df = parse_data(config['test_file_path'], data_type = 'test')\n",
        "\n",
        "  # 把这些 df 打包成字典\n",
        "  data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
        "  #保存 BERT 的输入\n",
        "  processed_data = {}\n",
        "  # 遍历字典(data_df)\n",
        "  for data_type, df in data_df.items():\n",
        "    inputs = defaultdict(list)\n",
        "    #遍历每一行\n",
        "    for i, row in tqdm(df.iterrows(), desc= f'Preprocessing {data_type} data', total = len(df)):\n",
        "      label = row[2]\n",
        "      sentence_a, sentence_b = row[0], row[1]\n",
        "      build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
        "\n",
        "    processed_data[data_type] = inputs\n",
        "  return processed_data"
      ],
      "metadata": {
        "id": "M5nlW7HV5IjC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(config['model_path'])\n",
        "dt = read_data(config, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iHgufu3Hr-w",
        "outputId": "ec87467e-f20e-4ca0-b858-8f32a71646c9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 87796.65it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 116220.47it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 43526.15it/s]\n",
            "Preprocessing train data: 100%|██████████| 34334/34334 [00:32<00:00, 1065.94it/s]\n",
            "Preprocessing dev data: 100%|██████████| 4316/4316 [00:03<00:00, 1419.83it/s]\n",
            "Preprocessing test data: 100%|██████████| 3861/3861 [00:03<00:00, 988.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('train_df中 input_ids的第一条数据',dt['train']['input_ids'][0])\n",
        "print('dev_df中 token_type_ids的第一条数据',dt['dev']['token_type_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JAghBwfII4n",
        "outputId": "a84b05a0-97bc-4bd3-f781-747c4538a1bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df中 input_ids的第一条数据 [101, 6010, 6009, 955, 1446, 5023, 7583, 6820, 3621, 1377, 809, 2940, 2768, 1044, 2622, 1400, 3315, 1408, 102, 955, 1446, 3300, 1044, 2622, 1168, 3309, 6820, 3315, 1408, 102]\n",
            "dev_df中 token_type_ids的第一条数据 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "  \n",
        "  # 返回一个example\n",
        "  def __getitem__(self, idx):\n",
        "    data = (self.data_dict['input_ids'][idx],\n",
        "         self.data_dict['token_type_ids'][idx],\n",
        "         self.data_dict['attention_mask'][idx],\n",
        "         self.data_dict['labels'][idx])\n",
        "    return data\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])"
      ],
      "metadata": {
        "id": "H6u3Lttbs8qx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Collator:\n",
        "  def __init__(self, max_seq_len, tokenizer):\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.tokenizer = tokenizer\n",
        "  \n",
        "  def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
        "    input_ids = torch.zeros((len(input_ids_list), max_seq_len),dtype=torch.long)\n",
        "    token_type_ids = torch.zeros_like(input_ids)\n",
        "    attention_mask = torch.zeros_like(input_ids)\n",
        "    \n",
        "    for i in range(len(input_ids_list)):\n",
        "      seq_len = len(input_ids_list[i])\n",
        "      if seq_len <= max_seq_len:\n",
        "        input_ids[i, :seq_len] = torch.tensor(input_ids_list[i], dtype = torch.long)\n",
        "        token_type_ids[i, :seq_len] = torch.tensor(token_type_ids_list[i], dtype = torch.long)\n",
        "        attention_mask[i, :seq_len] = torch.tensor(attention_mask_list[i], dtype = torch.long)\n",
        "      else:\n",
        "        # input_ids 最后一位放上一个特殊的token\n",
        "        input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len-1] + [self.tokenizer.sep_token_id], dtype = torch.long)\n",
        "        # token_type_ids 和 attention_mask 不需要加上特殊token\n",
        "        token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype = torch.long)\n",
        "        attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype = torch.long)\n",
        "    labels = torch.tensor(labels_list, dtype = torch.long)\n",
        "    return input_ids, token_type_ids, attention_mask, labels\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    input_ids_list, token_type_ids_list, attention_mask_list, labels_list = list(zip(*examples))\n",
        "    cur_max_seq_len = max(len(input_id) for input_id in input_ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "    \n",
        "    input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len)                     \n",
        "    \n",
        "    data_dict = {\n",
        "        'input_ids': input_ids,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels\n",
        "    }\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "1Xf4ahXKw6PX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn = Collator(config['max_seq_len'], tokenizer)"
      ],
      "metadata": {
        "id": "4eU_XHZx6F24"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 采样（Dataloader）"
      ],
      "metadata": {
        "id": "lvgR0VQpw_SV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Dataloader](https://img-blog.csdnimg.cn/b80cee8a1c7d49b79e7b80cc81150d66.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "DSqsAdWE9Vf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampler \n",
        "所有采样器都继承自Sampler这个类\n",
        "\n",
        "每个Sampler子类都要实现iter方法【迭代数据集example索引的方法】，以及返回迭代器长度的len方法"
      ],
      "metadata": {
        "id": "QS-HVTDV9xfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sampler](https://img-blog.csdnimg.cn/1c40aedade9f40a493b4df97d0c1def0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "HhylX0uH9Vb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 顺序采样\n",
        "![sequentialsampler](https://img-blog.csdnimg.cn/9e8ee018cea84729ac6b5742395d8ea2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)\n",
        "\n",
        "***在初始化时拿到数据集data_source， 按顺序对元素进行采样，每次只返回一个索引值 ***"
      ],
      "metadata": {
        "id": "FXjatAc6BUGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 顺序采样举例\n",
        "# randperm 把 0-23 数据打乱 形成3维tensor\n",
        "# (2,3,4) batch_size:2 seq_len=3, embedding_dim=4，每个 batch 有2条数据，每个句子包含3个词， 每个词的维度是4\n",
        "a = torch.randperm(24).reshape((2,3,4))\n",
        "print('a:',a)\n",
        "b = torch.utils.data.SequentialSampler(a)\n",
        "print('b:',b)\n",
        "# i 是索引\n",
        "for i in b:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dZh5C6acm0S",
        "outputId": "e00d20cd-dd98-4006-da02-5005a7a3b62d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([[[ 5, 14,  9,  2],\n",
            "         [13, 12, 20,  1],\n",
            "         [16, 15,  7,  4]],\n",
            "\n",
            "        [[17,  0,  3, 19],\n",
            "         [10, 22,  6, 18],\n",
            "         [ 8, 23, 11, 21]]])\n",
            "b: <torch.utils.data.sampler.SequentialSampler object at 0x7f73424e7e90>\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 随机采样\n",
        "replacement : True 表示可以重复采样\n",
        "\n",
        "num_samples: 指定采样的数量\n",
        "\n",
        "PS:当使用replacement=False，不应制定num_samples\n",
        "![randomsampler](https://img-blog.csdnimg.cn/9d2e2afdbe4d4df4aee3102e46054650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "7wyFCf2CEq7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 随机采样举例\n",
        "a = torch.randperm(60).reshape((5,3,4))\n",
        "print('a:',a)\n",
        "# 随机采样3条数据\n",
        "b = torch.utils.data.RandomSampler(a, replacement=True, num_samples=3)\n",
        "print('b:',b)\n",
        "for i in b:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52NgI_URFidd",
        "outputId": "cb1b10b5-32d1-4e4f-c572-a4c519eaea75"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([[[47, 36, 58, 12],\n",
            "         [49,  7, 24,  9],\n",
            "         [27, 13, 45,  0]],\n",
            "\n",
            "        [[ 3, 28, 23, 39],\n",
            "         [37, 29, 10, 59],\n",
            "         [ 4, 35, 56, 53]],\n",
            "\n",
            "        [[54, 32, 18, 42],\n",
            "         [41, 46, 30, 14],\n",
            "         [38, 22, 11,  5]],\n",
            "\n",
            "        [[48, 33, 57, 26],\n",
            "         [15, 19, 55, 16],\n",
            "         [20, 40, 31,  6]],\n",
            "\n",
            "        [[51, 17,  1, 25],\n",
            "         [34,  2, 43, 21],\n",
            "         [52, 50,  8, 44]]])\n",
            "b: <torch.utils.data.sampler.RandomSampler object at 0x7f733e7b5490>\n",
            "3\n",
            "3\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subset随机采样\n",
        "SubsetRandomSampler： 从给定的索引列表中随机采样元素，不放回采样 \n",
        "\n",
        "indices(sequence): 索引序列\n",
        "![sunsetRandomSampler](https://img-blog.csdnimg.cn/e80f6a1bafe042f28da652dc5a2388ab.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "5BoYgZchGmqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset采样举例\n",
        "a = torch.randperm(60).reshape((5,3,4))\n",
        "print('a:',a)\n",
        "# 从索引2以后的样本中随机采样\n",
        "b = torch.utils.data.SubsetRandomSampler(indices=a[2:])\n",
        "for i in b:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDJ0vAv4HCci",
        "outputId": "18793451-3741-4b5f-8ac6-5fe97a71163a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([[[ 8, 18, 55, 16],\n",
            "         [49, 54, 14,  7],\n",
            "         [33, 37, 39,  2]],\n",
            "\n",
            "        [[45,  6, 24, 29],\n",
            "         [58, 57,  3, 47],\n",
            "         [46, 56, 26, 21]],\n",
            "\n",
            "        [[12, 25, 52, 40],\n",
            "         [ 9, 53, 10, 50],\n",
            "         [48, 59, 27, 22]],\n",
            "\n",
            "        [[ 0, 20, 34, 13],\n",
            "         [41, 32, 35, 51],\n",
            "         [15,  4, 36, 38]],\n",
            "\n",
            "        [[11, 19,  5, 43],\n",
            "         [23, 31, 44, 30],\n",
            "         [28,  1, 42, 17]]])\n",
            "tensor([[ 0, 20, 34, 13],\n",
            "        [41, 32, 35, 51],\n",
            "        [15,  4, 36, 38]])\n",
            "tensor([[12, 25, 52, 40],\n",
            "        [ 9, 53, 10, 50],\n",
            "        [48, 59, 27, 22]])\n",
            "tensor([[11, 19,  5, 43],\n",
            "        [23, 31, 44, 30],\n",
            "        [28,  1, 42, 17]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 分批采样\n",
        "sampler: 基采样器 \n",
        "\n",
        "batch_size: size of mini-batch\n",
        "\n",
        "drop_last=True, 如果一个batch的长度小于batch_size则丢弃\n",
        "![BatchSampler](https://img-blog.csdnimg.cn/8a1b2f5ae320453c9fae8ae8e0ef2080.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)\n",
        "\n"
      ],
      "metadata": {
        "id": "SF2c2rgKoquU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 分批采样举例\n",
        "a = torch.randperm(60).reshape((5,3,4))\n",
        "print('a:',a)\n",
        "# 要传一个基采样器torch.utils.data.RandomSampler(a)\n",
        "b = torch.utils.data.BatchSampler(torch.utils.data.RandomSampler(a), 2, drop_last=True)\n",
        "# 上面的i都是一个数；现在是batch_size的列表\n",
        "for i in b:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNi3EU11oqEc",
        "outputId": "c1a87321-ffd8-42ae-bc0a-8870695a7125"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([[[34, 50, 28, 24],\n",
            "         [46,  0, 35, 21],\n",
            "         [51, 52, 33, 59]],\n",
            "\n",
            "        [[31,  5, 26, 42],\n",
            "         [11, 49,  8, 29],\n",
            "         [ 9, 17, 53, 36]],\n",
            "\n",
            "        [[ 1, 37, 22, 40],\n",
            "         [18, 20, 45,  7],\n",
            "         [10, 47, 19, 32]],\n",
            "\n",
            "        [[38, 14, 58,  3],\n",
            "         [13, 25, 27, 48],\n",
            "         [ 6, 44, 55, 30]],\n",
            "\n",
            "        [[56,  2, 57, 12],\n",
            "         [ 4, 23, 16, 15],\n",
            "         [43, 54, 41, 39]]])\n",
            "[3, 4]\n",
            "[1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 桶采样\n",
        "sort_key: 按XXX排序\n",
        "\n",
        "bucket_sampler: batch_size * bucket_size_multiplier 相当于 n * batch_size\n",
        "；len(sampler)最大为数据集的长度\n",
        "![BucketSampler](https://img-blog.csdnimg.cn/6413cea5dfbf4494a6b2b64504f74a97.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "66V_qSLzuZpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![SortedSampler](https://img-blog.csdnimg.cn/64f60217df474cf1b0d7aa1c3558cc1f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "V-VMY39qxSKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![BucketSampler](https://img-blog.csdnimg.cn/d7e03938f2824f9cb8a6c3a895f5a78a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "_WX-4OucxiqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 桶采样举例\n",
        "# Dataset -> 得到‘大桶'的排序索引\n",
        "\n",
        "# 真实train中数据，前6条\n",
        "mini_dataset = {k: v[:6] for k, v in dt['train'].items()}\n",
        "mini_data = AFQMCDataset(mini_dataset)\n",
        "print(mini_data)\n",
        "# mini_data 的前6条数据的长度\n",
        "for i, d in enumerate(mini_data):\n",
        "    print(d[0]) # input_ids\n",
        "    print(len(d[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epOJ-plhx-XG",
        "outputId": "fd097c0f-0d39-45b9-fcbb-f4c5fa8c890a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.AFQMCDataset object at 0x7f73417ee810>\n",
            "[101, 6010, 6009, 955, 1446, 5023, 7583, 6820, 3621, 1377, 809, 2940, 2768, 1044, 2622, 1400, 3315, 1408, 102, 955, 1446, 3300, 1044, 2622, 1168, 3309, 6820, 3315, 1408, 102]\n",
            "30\n",
            "[101, 6010, 6009, 5709, 1446, 6432, 2769, 6824, 5276, 671, 3613, 102, 6010, 6009, 5709, 1446, 6824, 5276, 6121, 711, 3221, 784, 720, 102]\n",
            "24\n",
            "[101, 2376, 2769, 4692, 671, 678, 3315, 3299, 5709, 1446, 6572, 1296, 3300, 3766, 3300, 5310, 3926, 102, 678, 3299, 5709, 1446, 6572, 1296, 102]\n",
            "25\n",
            "[101, 6010, 6009, 955, 1446, 1914, 7270, 3198, 7313, 5341, 1394, 6397, 844, 671, 3613, 102, 955, 1446, 2533, 6397, 844, 1914, 719, 102]\n",
            "24\n",
            "[101, 2769, 4638, 5709, 1446, 6572, 1296, 3221, 115, 115, 115, 8024, 6820, 3621, 2582, 720, 3221, 115, 115, 115, 102, 2769, 4638, 5709, 1446, 8024, 3299, 5310, 1139, 3341, 6432, 6375, 2769, 6820, 115, 115, 115, 1039, 8024, 2769, 5632, 2346, 5050, 749, 671, 678, 6422, 5301, 1399, 1296, 2769, 2418, 6421, 6820, 115, 115, 115, 1039, 102]\n",
            "59\n",
            "[101, 6010, 6009, 955, 1446, 4638, 7583, 2428, 1377, 809, 794, 4509, 6435, 679, 102, 6010, 6009, 955, 1446, 5688, 969, 3189, 1377, 809, 955, 3621, 1408, 102]\n",
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bucket_sampler import SortedSampler\n",
        "random_sampler = torch.utils.data.RandomSampler(mini_data, replacement=False)\n",
        "# print(list(random_sampler))\n",
        "# 关于dataset的随机索引 [3, 5, 4, 1, 0, 2]\n",
        "\n",
        "batch_sampler = torch.utils.data.BatchSampler(random_sampler, 4, drop_last=True)\n",
        "# [0, 5, 2, 4] 【还有[1, 3] 但是丢弃了】\n",
        "\n",
        "for samp in batch_sampler:\n",
        "    print('samp:',samp)\n",
        "    sorted_sampler = SortedSampler(samp, sort_key=lambda x:len(mini_data[x][0]))\n",
        "    print('list_sorted_sampler:',list(sorted_sampler))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yyjconvz6TO",
        "outputId": "a06f5de4-2d15-4aa6-cb4c-f4ff42f00c08"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "samp: [0, 3, 1, 4]\n",
            "list_sorted_sampler: [1, 2, 0, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "[0, 5, 2, 4]分别对应mini_data中的长度[30, 28, 25, 59]\n",
        "\n",
        "[2, 1, 0, 3] \n",
        "\n",
        "2（位置2的数据len最小） -> 2 -> 25 \n",
        "\n",
        "1 -> 5 -> 28 \n",
        "\n",
        "0 -> 0 -> 30 \n",
        "\n",
        "3（位置3的数据len最大） -> 4 -> 59\n",
        "```"
      ],
      "metadata": {
        "id": "-A6oFU9B1RXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 得到‘大桶'的排序索引 -> 返回‘小桶'在‘大桶'中的位置\n",
        "c = list(torch.utils.data.BatchSampler(sorted_sampler, 2, drop_last=True))\n",
        "print(c)\n",
        "# c 把大桶 分成 batch_size大小的小桶"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az7SLVSs2idz",
        "outputId": "9357019e-e5b8-4a24-c7ca-fb94f25950bb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2], [0, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "[[2, 1], [0, 3]]\n",
        "```"
      ],
      "metadata": {
        "id": "ABvwvlmZ5_r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in torch.utils.data.SubsetRandomSampler(c):\n",
        "    print('从给定的索引列表中随机采样元素')\n",
        "    print(batch)\n",
        "    print('所对应的原序列是什么：')\n",
        "    print([samp[i] for i in batch])\n",
        "    # 参考上面 from bucket_sampler import SortedSampler 单元格对应法则"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGcdBAPo3Za7",
        "outputId": "5e61978d-8c47-4c44-e608-e19eb962281b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "从给定的索引列表中随机采样元素\n",
            "[1, 2]\n",
            "所对应的原序列是什么：\n",
            "[3, 1]\n",
            "从给定的索引列表中随机采样元素\n",
            "[0, 3]\n",
            "所对应的原序列是什么：\n",
            "[0, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "从给定的索引列表中随机采样元素\n",
        "[2, 1]\n",
        "所对应的原序列是什么：\n",
        "[2, 5]\n",
        "从给定的索引列表中随机采样元素\n",
        "[0, 3]\n",
        "所对应的原序列是什么：\n",
        "[0, 4]\n",
        "```"
      ],
      "metadata": {
        "id": "l_W55Rvx5WxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 采样在dataloader中使用\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from bucket_sampler import BucketBatchSampler\n",
        "\n",
        "def build_dataloader(config, data, collate_fn):\n",
        "  train_dataset = AFQMCDataset(data['train'])\n",
        "  dev_dataset = AFQMCDataset(data['dev'])\n",
        "  test_dataset = AFQMCDataset(data['test'])\n",
        "\n",
        "  if config['use_bucket']:\n",
        "    # 先放一个基采样器\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    # sort_key 以input_ids 的len排序\n",
        "    bucket_sampler = BucketBatchSampler(train_sampler, \n",
        "                       batch_size = config['batch_size'],\n",
        "                       drop_last = False,\n",
        "                       sort_key = lambda x:len(train_dataset[x][0]),\n",
        "                       bucket_size_multiplier = config['bucket_multiplier'])\n",
        "    train_dataloader = DataLoader(dataset = train_dataset, batch_sampler = bucket_sampler,\n",
        "                    num_workers = 4, collate_fn = collate_fn)\n",
        "    \n",
        "  else:\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = config['batch_size'],\n",
        "                    shuffle = True, num_workers = 4, collate_fn = collate_fn)\n",
        "  dev_dataloader = DataLoader(dev_dataset, batch_size = config['batch_size'],\n",
        "                  shuffle = False, num_workers = 4, collate_fn = collate_fn)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size = config['batch_size'],\n",
        "                  shuffle = False, num_workers = 4, collate_fn = collate_fn)\n",
        "  return train_dataloader, dev_dataloader, test_dataloader  \n"
      ],
      "metadata": {
        "id": "UwqaHWKQ_qJV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, dev_dataloader, test_dataloader = build_dataloader(config, dt, collate_fn)"
      ],
      "metadata": {
        "id": "ck6rtFpEAQ70"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataloader:\n",
        "    print('train_dataloader一个batch:',i)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5epRv3qR2mC",
        "outputId": "25bb2eb9-c2dd-4f29-d9e1-6b181134014c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader一个batch: {'input_ids': tensor([[ 101, 2769,  671,  ...,  955, 1446,  102],\n",
            "        [ 101,  671, 2476,  ..., 1126,  702,  102],\n",
            "        [ 101, 2769, 4500,  ..., 1168, 6572,  102],\n",
            "        ...,\n",
            "        [ 101, 2769, 4638,  ..., 1921, 6820,  102],\n",
            "        [ 101, 2769, 4638,  ..., 5709, 1446,  102],\n",
            "        [ 101, 2769,  955,  ...,  955, 1446,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
            "        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自动混合精度（混合精度训练）\n",
        "\n",
        "![amp](https://img-blog.csdnimg.cn/e4226734b82f462e983aa905de50891a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)\n"
      ],
      "metadata": {
        "id": "fDXEWnmmAQlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 混合精度训练\n",
        "作用：训练时，尽量不降低性能，并提升速度 \n",
        "\n",
        "Float16优点:\n",
        "\n",
        "* 减少内存的使用\n",
        "* 加快训练和推断的计算，能带来多一倍速的体验\n",
        "\n",
        "Float16缺点:\n",
        "* 溢出错误\n",
        "* 舍入误差"
      ],
      "metadata": {
        "id": "bIV8wdDKpjpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.FloatTensor 32位\n",
        "a = torch.zeros(2,3)\n",
        "print(a.type())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrzjvFGxDNqo",
        "outputId": "e853cfb7-f423-4cbb-9eea-8b66e50387bd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.FloatTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "混合精度将 ***autocast*** 和 ***GradScaler*** 一起使用"
      ],
      "metadata": {
        "id": "u0K13AmcEtwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***当进入autocast()时， 系统自动切换为float16, autocast上下文只包含前向传播，建议不用反向传播***"
      ],
      "metadata": {
        "id": "61kZsJEeEB0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![amp2](https://img-blog.csdnimg.cn/72b642b508024cc2a6207c308347c7e7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "WJZyS-0hwR9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient Scaling\n",
        "* scaler.scale(loss) 将给定的损失乘以缩放器的当前比例因子，进行反向传播\n",
        "* scaler.step(optimizer) 取消缩放梯度并调用optimizer.step()\n",
        "* scaler.update() 更新缩放器的比例因子"
      ],
      "metadata": {
        "id": "sn1VIDX5ETYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![scaling](https://img-blog.csdnimg.cn/bbae5cdd360748ecb59cee8dc6f728f2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "J8rDZvQnwXVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![GradScaler](https://img-blog.csdnimg.cn/2c0fdf08602748ea8a7655f2d5bb1829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "lrT4nqUiway0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![example](https://img-blog.csdnimg.cn/dfeebde4d34b496096062bb7dbbee7b6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "eSAb4I01wdOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def evaluation(config, model, val_dataloader):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  labels = []\n",
        "  val_loss = 0.\n",
        "  val_iterator = tqdm(val_dataloader, desc = 'Evaluation', total = len(val_dataloader))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_iterator:\n",
        "      labels.append(batch['labels'])\n",
        "      batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
        "      loss, logits = model(**batch_cuda)[:2]\n",
        "\n",
        "      if config['n_gpus'] > 1:\n",
        "        loss = loss.mean()\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      preds.append(logits.argmax(dim = -1).detach().cpu())\n",
        "\n",
        "  avg_val_loss = val_loss / len(val_dataloader)\n",
        "  labels = torch.cat(labels, dim = 0).numpy()\n",
        "  preds = torch.cat(preds, dim = 0).numpy()\n",
        "  f1 = f1_score(labels, preds)\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return avg_val_loss, f1, acc"
      ],
      "metadata": {
        "id": "gG3kyfNQ7Xcu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EMA:\n",
        "  def __init__(self, model, decay):\n",
        "    self.model = model\n",
        "    self.decay = decay\n",
        "    self.shadow = {}\n",
        "    self.backup = {}\n",
        "    self.register()\n",
        "\n",
        "  def register(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        self.shadow[name] = param.data.clone()\n",
        "\n",
        "  def update(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        # 如果 name in self.shadow 则运行下面两行代码， 否则报错\n",
        "        assert name in self.shadow\n",
        "        new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
        "        self.shadow[name] = new_average.clone()\n",
        "\n",
        "  def apply_shadow(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        assert name in self.shadow\n",
        "        self.backup[name] = param.data\n",
        "        param.data = self.shadow[name]\n",
        " \n",
        "  def restore(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        assert name in self.backup\n",
        "        param.data = self.backup[name]\n",
        "    self.backup = {}\n",
        "  "
      ],
      "metadata": {
        "id": "j2yDMxscHcem"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from torch.cuda import amp\n",
        "from transformers import AdamW\n",
        "from extra_pgd import *\n",
        "from extra_loss import *\n",
        "from extra_fgm import *\n",
        "from extra_optim import *\n",
        "from tqdm import trange\n",
        "def train(config, train_dataloader, dev_dataloader):\n",
        "  # 封装好 BertForSequenceClassification(用于文本分类的类)\n",
        "  model = BertForSequenceClassification.from_pretrained(config['model_path'])\n",
        "\n",
        "  # param_optimizer = model.named_parameters() ->> 之前写法 \n",
        "  # 定义模型参数，以列表形式返回\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "\n",
        "  # 实例化scaler对象 enabled=True 可以使用梯度缩放\n",
        "  scaler = amp.GradScaler(enabled = config['use_amp'])\n",
        "\n",
        "  # 权重缩减\n",
        "  no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "  # 参数名称包含 ['bias', 'LayerNorm.weight']的权重， 其权重衰减因子为0\n",
        "  # 参数名称不包含 ['bias', 'LayerNorm.weight']的权重， 其权重衰减因子为 0.01\n",
        "  # any() 理解成any True的意思，是否存在True，只要有一个是True，结果就是True\n",
        "  # 列表中有两个字典\n",
        "  optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay': config['weight_decay']},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay': 0.0}\n",
        "  ]\n",
        "\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr = config['learning_rate'],\n",
        "            eps = 1e-8)\n",
        "  \n",
        "  # lookahead 预先查看由 AdamW 生成的快速权重 来选择搜索方向\n",
        "  optimizer = Lookahead(optimizer, 5, 1)\n",
        "  total_steps = config['num_epochs'] * len(train_dataloader)\n",
        "\n",
        "  # 使用Warmup来调整学习率，每调用warmup_steps次，对应的学习率就会调整一次。\n",
        "  lr_scheduler = WarmupLinearSchedule(optimizer, warmup_steps = int(config['warmup_ratio'] * total_steps),\n",
        "                     t_total = total_steps)\n",
        "  \n",
        "\n",
        "                                  \n",
        "  model.to(config['device'])\n",
        "\n",
        "  # 选择对抗训练模型\n",
        "  if config['adv'] == 'fgm':\n",
        "    fgm = FGM(model)\n",
        "  else:\n",
        "    pgd = PGD(model)\n",
        "    K = 3\n",
        "\n",
        "  epoch_iterator = trange(config['num_epochs'])\n",
        "  global_steps = 0\n",
        "  train_loss = 0.\n",
        "  logging_loss = 0.\n",
        "  best_acc = 0.\n",
        "  best_model_path = ''\n",
        "\n",
        "  # 多卡情况\n",
        "  if config['n_gpus'] > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "  for _ in epoch_iterator:\n",
        "    train_iterator = tqdm(train_dataloader, desc = 'Trainging', total = len(train_dataloader))\n",
        "    model.train()\n",
        "    for batch in train_iterator:\n",
        "      batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
        "\n",
        "      # 前向过程（前向传播 + loss）\n",
        "      with amp.autocast(enabled = config['use_amp']):\n",
        "        loss = model(**batch_cuda)[0]\n",
        "        # 多卡 每个卡会计算出一个loss 最后取平均\n",
        "        if config['n_gpus'] > 1:\n",
        "          loss = loss.mean()\n",
        "      \n",
        "      # 反向传播  \n",
        "      scaler.scale(loss).backward()\n",
        "\n",
        "      if config['adv'] == 'fgm':\n",
        "        # 在embedding上加扰动\n",
        "        fgm.attack(epsilon = config['eps'])\n",
        "\n",
        "        # autocast\n",
        "        with amp.autocast(enabled = config['use_amp']):\n",
        "          loss_adv = model(**batch_cuda)[0]\n",
        "\n",
        "          if config['n_gpus'] > 1:\n",
        "            loss_adv =loss_adv.mean()\n",
        "\n",
        "        scaler.scale(loss_adv).backward()\n",
        "        # 恢复embedding参数\n",
        "        fgm.restore()\n",
        "      else:\n",
        "        pgd.backup_grad()\n",
        "        for t in range(K):\n",
        "          pgd.attack(epsilon = config['eps'], alpha = config['alpha'], is_first_attack= ( t == 0))\n",
        "          if t != K - 1:\n",
        "            model.zero_grad()\n",
        "          else:\n",
        "            pgd.restore_grad()\n",
        "          with amp.autocast(enabled = config['use_amp']):\n",
        "            loss_adv = model(**batch_cuda)[0]\n",
        "            if config['n_gpus'] > 1:\n",
        "              loss_adv = loss_adv.mean()\n",
        "\n",
        "          scaler.scale(loss_adv).backward()\n",
        "        pgd.restore()\n",
        "        \n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "      lr_scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if config['ema_start']:\n",
        "        ema.update()\n",
        "        \n",
        "      train_loss += loss.item()\n",
        "      global_steps += 1\n",
        "\n",
        "      train_iterator.set_postfix_str(f'running train loss: {loss.item():.5f}')\n",
        "\n",
        "      if global_steps % config['logging_step'] == 0:\n",
        "        if global_steps >= config['ema_start_step'] and not config['ema_start']:\n",
        "          print('\\n>>> EMA starting .....')\n",
        "          config['ema_start'] = True\n",
        "\n",
        "          ema = EMA(model.module if hasattr(model, 'module') else model, decay = 0.99)\n",
        "\n",
        "        print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
        "        logging_loss = train_loss\n",
        "\n",
        "        if config['ema_start']:\n",
        "          ema.apply_shadow()\n",
        "        val_loss, f1, acc = evaluation(config, model, dev_dataloader)\n",
        "\n",
        "        print_log = f'\\n>>> training loss: {print_train_loss:.6f}, valid loss: {val_loss:.6f},' \n",
        "\n",
        "        if acc > best_acc:\n",
        "          model_save_path = os.path.join(config['output_path'],\n",
        "                          f'checkpoint- {global_steps} - {acc:.6f}')\n",
        "          model_to_save = model.module if hasattr(model, 'module') else model\n",
        "          model_to_save.save_pretrained(model_save_path)\n",
        "          best_acc = acc\n",
        "          best_model_path = model_save_path\n",
        "        print_log += f'valid f1: {f1:.6f}, valid acc:{acc:.6f}'\n",
        "\n",
        "        print(print_log)\n",
        "        model.train()\n",
        "\n",
        "        if config['ema_start']:\n",
        "          ema.restore()\n",
        "\n",
        "\n",
        "  return model, best_model_path        \n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "91xvXsDETp7m"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(config, train_dataloader, dev_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpCUDSWDk2hk",
        "outputId": "e3cbf6df-f9c2-4345-c0c6-34a53ac6854d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Trainging:   0%|          | 0/537 [00:00<?, ?it/s]\u001b[A\n",
            "Trainging:   0%|          | 0/537 [00:04<?, ?it/s, running train loss: 0.62231]\u001b[A\n",
            "Trainging:   0%|          | 1/537 [00:04<44:08,  4.94s/it, running train loss: 0.62231]\u001b[A\n",
            "Trainging:   0%|          | 1/537 [00:09<44:08,  4.94s/it, running train loss: 0.61432]\u001b[A\n",
            "Trainging:   0%|          | 2/537 [00:09<41:58,  4.71s/it, running train loss: 0.61432]\u001b[A\n",
            "Trainging:   0%|          | 2/537 [00:13<41:58,  4.71s/it, running train loss: 0.61790]\u001b[A\n",
            "Trainging:   1%|          | 3/537 [00:13<38:30,  4.33s/it, running train loss: 0.61790]\u001b[A\n",
            "Trainging:   1%|          | 3/537 [00:16<38:30,  4.33s/it, running train loss: 0.54319]\u001b[A\n",
            "Trainging:   1%|          | 4/537 [00:16<34:50,  3.92s/it, running train loss: 0.54319]\u001b[A\n",
            "Trainging:   1%|          | 4/537 [00:19<34:50,  3.92s/it, running train loss: 0.59943]\u001b[A\n",
            "Trainging:   1%|          | 5/537 [00:19<32:20,  3.65s/it, running train loss: 0.59943]\u001b[A\n",
            "Trainging:   1%|          | 5/537 [00:23<32:20,  3.65s/it, running train loss: 0.66518]\u001b[A\n",
            "Trainging:   1%|          | 6/537 [00:23<31:26,  3.55s/it, running train loss: 0.66518]\u001b[A\n",
            "Trainging:   1%|          | 6/537 [00:26<31:26,  3.55s/it, running train loss: 0.58813]\u001b[A\n",
            "Trainging:   1%|▏         | 7/537 [00:26<31:09,  3.53s/it, running train loss: 0.58813]\u001b[A\n",
            "Trainging:   1%|▏         | 7/537 [00:30<31:09,  3.53s/it, running train loss: 0.65023]\u001b[A\n",
            "Trainging:   1%|▏         | 8/537 [00:30<31:17,  3.55s/it, running train loss: 0.65023]\u001b[A\n",
            "Trainging:   1%|▏         | 8/537 [00:34<31:17,  3.55s/it, running train loss: 0.56604]\u001b[A\n",
            "Trainging:   2%|▏         | 9/537 [00:34<33:44,  3.83s/it, running train loss: 0.56604]\u001b[A\n",
            "Trainging:   2%|▏         | 9/537 [00:38<33:44,  3.83s/it, running train loss: 0.59254]\u001b[A\n",
            "Trainging:   2%|▏         | 10/537 [00:38<32:50,  3.74s/it, running train loss: 0.59254]\u001b[A\n",
            "Trainging:   2%|▏         | 10/537 [00:42<32:50,  3.74s/it, running train loss: 0.55515]\u001b[A\n",
            "Trainging:   2%|▏         | 11/537 [00:42<33:22,  3.81s/it, running train loss: 0.55515]\u001b[A\n",
            "Trainging:   2%|▏         | 11/537 [00:45<33:22,  3.81s/it, running train loss: 0.63968]\u001b[A\n",
            "Trainging:   2%|▏         | 12/537 [00:45<32:19,  3.69s/it, running train loss: 0.63968]\u001b[A\n",
            "Trainging:   2%|▏         | 12/537 [00:50<32:19,  3.69s/it, running train loss: 0.56992]\u001b[A\n",
            "Trainging:   2%|▏         | 13/537 [00:50<34:22,  3.94s/it, running train loss: 0.56992]\u001b[A\n",
            "Trainging:   2%|▏         | 13/537 [00:54<34:22,  3.94s/it, running train loss: 0.63720]\u001b[A\n",
            "Trainging:   3%|▎         | 14/537 [00:54<36:33,  4.19s/it, running train loss: 0.63720]\u001b[A\n",
            "Trainging:   3%|▎         | 14/537 [00:59<36:33,  4.19s/it, running train loss: 0.73981]\u001b[A\n",
            "Trainging:   3%|▎         | 15/537 [00:59<37:34,  4.32s/it, running train loss: 0.73981]\u001b[A\n",
            "Trainging:   3%|▎         | 15/537 [01:03<37:34,  4.32s/it, running train loss: 0.65505]\u001b[A\n",
            "Trainging:   3%|▎         | 16/537 [01:03<35:30,  4.09s/it, running train loss: 0.65505]\u001b[A\n",
            "Trainging:   3%|▎         | 16/537 [01:06<35:30,  4.09s/it, running train loss: 0.55137]\u001b[A\n",
            "Trainging:   3%|▎         | 17/537 [01:06<33:29,  3.86s/it, running train loss: 0.55137]\u001b[A\n",
            "Trainging:   3%|▎         | 17/537 [01:10<33:29,  3.86s/it, running train loss: 0.58615]\u001b[A\n",
            "Trainging:   3%|▎         | 18/537 [01:10<32:50,  3.80s/it, running train loss: 0.58615]\u001b[A\n",
            "Trainging:   3%|▎         | 18/537 [01:13<32:50,  3.80s/it, running train loss: 0.55769]\u001b[A\n",
            "Trainging:   4%|▎         | 19/537 [01:13<32:44,  3.79s/it, running train loss: 0.55769]\u001b[A\n",
            "Trainging:   4%|▎         | 19/537 [01:18<32:44,  3.79s/it, running train loss: 0.66329]\u001b[A\n",
            "Trainging:   4%|▎         | 20/537 [01:18<35:07,  4.08s/it, running train loss: 0.66329]\u001b[A\n",
            "Trainging:   4%|▎         | 20/537 [01:24<35:07,  4.08s/it, running train loss: 0.55482]\u001b[A\n",
            "Trainging:   4%|▍         | 21/537 [01:24<38:35,  4.49s/it, running train loss: 0.55482]\u001b[A\n",
            "Trainging:   4%|▍         | 21/537 [01:27<38:35,  4.49s/it, running train loss: 0.59358]\u001b[A\n",
            "Trainging:   4%|▍         | 22/537 [01:27<36:02,  4.20s/it, running train loss: 0.59358]\u001b[A\n",
            "Trainging:   4%|▍         | 22/537 [01:30<36:02,  4.20s/it, running train loss: 0.60292]\u001b[A\n",
            "Trainging:   4%|▍         | 23/537 [01:30<33:31,  3.91s/it, running train loss: 0.60292]\u001b[A\n",
            "Trainging:   4%|▍         | 23/537 [01:35<33:31,  3.91s/it, running train loss: 0.64755]\u001b[A\n",
            "Trainging:   4%|▍         | 24/537 [01:35<35:05,  4.10s/it, running train loss: 0.64755]\u001b[A\n",
            "Trainging:   4%|▍         | 24/537 [01:39<35:05,  4.10s/it, running train loss: 0.63174]\u001b[A\n",
            "Trainging:   5%|▍         | 25/537 [01:39<36:04,  4.23s/it, running train loss: 0.63174]\u001b[A\n",
            "Trainging:   5%|▍         | 25/537 [01:43<36:04,  4.23s/it, running train loss: 0.65112]\u001b[A\n",
            "Trainging:   5%|▍         | 26/537 [01:43<35:00,  4.11s/it, running train loss: 0.65112]\u001b[A\n",
            "Trainging:   5%|▍         | 26/537 [01:48<35:00,  4.11s/it, running train loss: 0.60633]\u001b[A\n",
            "Trainging:   5%|▌         | 27/537 [01:48<37:05,  4.36s/it, running train loss: 0.60633]\u001b[A\n",
            "Trainging:   5%|▌         | 27/537 [01:52<37:05,  4.36s/it, running train loss: 0.71202]\u001b[A\n",
            "Trainging:   5%|▌         | 28/537 [01:52<35:31,  4.19s/it, running train loss: 0.71202]\u001b[A\n",
            "Trainging:   5%|▌         | 28/537 [01:55<35:31,  4.19s/it, running train loss: 0.67540]\u001b[A\n",
            "Trainging:   5%|▌         | 29/537 [01:55<33:41,  3.98s/it, running train loss: 0.67540]\u001b[A\n",
            "Trainging:   5%|▌         | 29/537 [01:59<33:41,  3.98s/it, running train loss: 0.60605]\u001b[A\n",
            "Trainging:   6%|▌         | 30/537 [01:59<32:47,  3.88s/it, running train loss: 0.60605]\u001b[A\n",
            "Trainging:   6%|▌         | 30/537 [02:04<32:47,  3.88s/it, running train loss: 0.63858]\u001b[A\n",
            "Trainging:   6%|▌         | 31/537 [02:04<35:16,  4.18s/it, running train loss: 0.63858]\u001b[A\n",
            "Trainging:   6%|▌         | 31/537 [02:07<35:16,  4.18s/it, running train loss: 0.68499]\u001b[A\n",
            "Trainging:   6%|▌         | 32/537 [02:07<33:28,  3.98s/it, running train loss: 0.68499]\u001b[A\n",
            "Trainging:   6%|▌         | 32/537 [02:11<33:28,  3.98s/it, running train loss: 0.66573]\u001b[A\n",
            "Trainging:   6%|▌         | 33/537 [02:11<32:34,  3.88s/it, running train loss: 0.66573]\u001b[A\n",
            "Trainging:   6%|▌         | 33/537 [02:15<32:34,  3.88s/it, running train loss: 0.70958]\u001b[A\n",
            "Trainging:   6%|▋         | 34/537 [02:15<31:25,  3.75s/it, running train loss: 0.70958]\u001b[A\n",
            "Trainging:   6%|▋         | 34/537 [02:18<31:25,  3.75s/it, running train loss: 0.54714]\u001b[A\n",
            "Trainging:   7%|▋         | 35/537 [02:18<31:01,  3.71s/it, running train loss: 0.54714]\u001b[A\n",
            "Trainging:   7%|▋         | 35/537 [02:22<31:01,  3.71s/it, running train loss: 0.59900]\u001b[A\n",
            "Trainging:   7%|▋         | 36/537 [02:22<31:38,  3.79s/it, running train loss: 0.59900]\u001b[A\n",
            "Trainging:   7%|▋         | 36/537 [02:25<31:38,  3.79s/it, running train loss: 0.63428]\u001b[A\n",
            "Trainging:   7%|▋         | 37/537 [02:25<30:21,  3.64s/it, running train loss: 0.63428]\u001b[A\n",
            "Trainging:   7%|▋         | 37/537 [02:29<30:21,  3.64s/it, running train loss: 0.68162]\u001b[A\n",
            "Trainging:   7%|▋         | 38/537 [02:29<30:20,  3.65s/it, running train loss: 0.68162]\u001b[A\n",
            "Trainging:   7%|▋         | 38/537 [02:33<30:20,  3.65s/it, running train loss: 0.63205]\u001b[A\n",
            "Trainging:   7%|▋         | 39/537 [02:33<29:38,  3.57s/it, running train loss: 0.63205]\u001b[A\n",
            "Trainging:   7%|▋         | 39/537 [02:36<29:38,  3.57s/it, running train loss: 0.59141]\u001b[A\n",
            "Trainging:   7%|▋         | 40/537 [02:36<30:01,  3.62s/it, running train loss: 0.59141]\u001b[A\n",
            "Trainging:   7%|▋         | 40/537 [02:40<30:01,  3.62s/it, running train loss: 0.62551]\u001b[A\n",
            "Trainging:   8%|▊         | 41/537 [02:40<30:48,  3.73s/it, running train loss: 0.62551]\u001b[A\n",
            "Trainging:   8%|▊         | 41/537 [02:44<30:48,  3.73s/it, running train loss: 0.59446]\u001b[A\n",
            "Trainging:   8%|▊         | 42/537 [02:44<31:12,  3.78s/it, running train loss: 0.59446]\u001b[A\n",
            "Trainging:   8%|▊         | 42/537 [02:48<31:12,  3.78s/it, running train loss: 0.58179]\u001b[A\n",
            "Trainging:   8%|▊         | 43/537 [02:48<30:16,  3.68s/it, running train loss: 0.58179]\u001b[A\n",
            "Trainging:   8%|▊         | 43/537 [02:51<30:16,  3.68s/it, running train loss: 0.63645]\u001b[A\n",
            "Trainging:   8%|▊         | 44/537 [02:51<30:44,  3.74s/it, running train loss: 0.63645]\u001b[A\n",
            "Trainging:   8%|▊         | 44/537 [02:55<30:44,  3.74s/it, running train loss: 0.67419]\u001b[A\n",
            "Trainging:   8%|▊         | 45/537 [02:55<30:18,  3.70s/it, running train loss: 0.67419]\u001b[A\n",
            "Trainging:   8%|▊         | 45/537 [02:59<30:18,  3.70s/it, running train loss: 0.59074]\u001b[A\n",
            "Trainging:   9%|▊         | 46/537 [02:59<30:41,  3.75s/it, running train loss: 0.59074]\u001b[A\n",
            "Trainging:   9%|▊         | 46/537 [03:03<30:41,  3.75s/it, running train loss: 0.52917]\u001b[A\n",
            "Trainging:   9%|▉         | 47/537 [03:03<30:17,  3.71s/it, running train loss: 0.52917]\u001b[A\n",
            "Trainging:   9%|▉         | 47/537 [03:05<30:17,  3.71s/it, running train loss: 0.72024]\u001b[A\n",
            "Trainging:   9%|▉         | 48/537 [03:05<26:18,  3.23s/it, running train loss: 0.72024]\u001b[A\n",
            "Trainging:   9%|▉         | 48/537 [03:08<26:18,  3.23s/it, running train loss: 0.47854]\u001b[A\n",
            "Trainging:   9%|▉         | 49/537 [03:08<27:18,  3.36s/it, running train loss: 0.47854]\u001b[A\n",
            "Trainging:   9%|▉         | 49/537 [03:12<27:18,  3.36s/it, running train loss: 0.70298]\u001b[A\n",
            "Trainging:   9%|▉         | 50/537 [03:12<28:27,  3.51s/it, running train loss: 0.70298]\u001b[A\n",
            "Trainging:   9%|▉         | 50/537 [03:16<28:27,  3.51s/it, running train loss: 0.65933]\u001b[A\n",
            "Trainging:   9%|▉         | 51/537 [03:16<28:12,  3.48s/it, running train loss: 0.65933]\u001b[A\n",
            "Trainging:   9%|▉         | 51/537 [03:19<28:12,  3.48s/it, running train loss: 0.54744]\u001b[A\n",
            "Trainging:  10%|▉         | 52/537 [03:19<28:54,  3.58s/it, running train loss: 0.54744]\u001b[A\n",
            "Trainging:  10%|▉         | 52/537 [03:23<28:54,  3.58s/it, running train loss: 0.69047]\u001b[A\n",
            "Trainging:  10%|▉         | 53/537 [03:23<29:35,  3.67s/it, running train loss: 0.69047]\u001b[A\n",
            "Trainging:  10%|▉         | 53/537 [03:27<29:35,  3.67s/it, running train loss: 0.65854]\u001b[A\n",
            "Trainging:  10%|█         | 54/537 [03:27<29:56,  3.72s/it, running train loss: 0.65854]\u001b[A\n",
            "Trainging:  10%|█         | 54/537 [03:31<29:56,  3.72s/it, running train loss: 0.56422]\u001b[A\n",
            "Trainging:  10%|█         | 55/537 [03:31<30:24,  3.78s/it, running train loss: 0.56422]\u001b[A\n",
            "Trainging:  10%|█         | 55/537 [03:35<30:24,  3.78s/it, running train loss: 0.64325]\u001b[A\n",
            "Trainging:  10%|█         | 56/537 [03:35<29:34,  3.69s/it, running train loss: 0.64325]\u001b[A\n",
            "Trainging:  10%|█         | 56/537 [03:38<29:34,  3.69s/it, running train loss: 0.56751]\u001b[A\n",
            "Trainging:  11%|█         | 57/537 [03:38<29:46,  3.72s/it, running train loss: 0.56751]\u001b[A\n",
            "Trainging:  11%|█         | 57/537 [03:42<29:46,  3.72s/it, running train loss: 0.61280]\u001b[A\n",
            "Trainging:  11%|█         | 58/537 [03:42<29:17,  3.67s/it, running train loss: 0.61280]\u001b[A\n",
            "Trainging:  11%|█         | 58/537 [03:46<29:17,  3.67s/it, running train loss: 0.56103]\u001b[A\n",
            "Trainging:  11%|█         | 59/537 [03:46<29:39,  3.72s/it, running train loss: 0.56103]\u001b[A\n",
            "Trainging:  11%|█         | 59/537 [03:50<29:39,  3.72s/it, running train loss: 0.57946]\u001b[A\n",
            "Trainging:  11%|█         | 60/537 [03:50<29:57,  3.77s/it, running train loss: 0.57946]\u001b[A\n",
            "Trainging:  11%|█         | 60/537 [03:53<29:57,  3.77s/it, running train loss: 0.55780]\u001b[A\n",
            "Trainging:  11%|█▏        | 61/537 [03:53<29:23,  3.70s/it, running train loss: 0.55780]\u001b[A\n",
            "Trainging:  11%|█▏        | 61/537 [03:56<29:23,  3.70s/it, running train loss: 0.74163]\u001b[A\n",
            "Trainging:  12%|█▏        | 62/537 [03:56<28:02,  3.54s/it, running train loss: 0.74163]\u001b[A\n",
            "Trainging:  12%|█▏        | 62/537 [04:00<28:02,  3.54s/it, running train loss: 0.60884]\u001b[A\n",
            "Trainging:  12%|█▏        | 63/537 [04:00<28:13,  3.57s/it, running train loss: 0.60884]\u001b[A\n",
            "Trainging:  12%|█▏        | 63/537 [04:03<28:13,  3.57s/it, running train loss: 0.50894]\u001b[A\n",
            "Trainging:  12%|█▏        | 64/537 [04:03<28:05,  3.56s/it, running train loss: 0.50894]\u001b[A\n",
            "Trainging:  12%|█▏        | 64/537 [04:07<28:05,  3.56s/it, running train loss: 0.52933]\u001b[A\n",
            "Trainging:  12%|█▏        | 65/537 [04:07<27:21,  3.48s/it, running train loss: 0.52933]\u001b[A\n",
            "Trainging:  12%|█▏        | 65/537 [04:10<27:21,  3.48s/it, running train loss: 0.63954]\u001b[A\n",
            "Trainging:  12%|█▏        | 66/537 [04:10<27:12,  3.47s/it, running train loss: 0.63954]\u001b[A\n",
            "Trainging:  12%|█▏        | 66/537 [04:14<27:12,  3.47s/it, running train loss: 0.64987]\u001b[A\n",
            "Trainging:  12%|█▏        | 67/537 [04:14<27:47,  3.55s/it, running train loss: 0.64987]\u001b[A\n",
            "Trainging:  12%|█▏        | 67/537 [04:19<27:47,  3.55s/it, running train loss: 0.65335]\u001b[A\n",
            "Trainging:  13%|█▎        | 68/537 [04:19<30:38,  3.92s/it, running train loss: 0.65335]\u001b[A\n",
            "Trainging:  13%|█▎        | 68/537 [04:24<30:38,  3.92s/it, running train loss: 0.55463]\u001b[A\n",
            "Trainging:  13%|█▎        | 69/537 [04:24<32:55,  4.22s/it, running train loss: 0.55463]\u001b[A\n",
            "Trainging:  13%|█▎        | 69/537 [04:27<32:55,  4.22s/it, running train loss: 0.61327]\u001b[A\n",
            "Trainging:  13%|█▎        | 70/537 [04:27<31:23,  4.03s/it, running train loss: 0.61327]\u001b[A\n",
            "Trainging:  13%|█▎        | 70/537 [04:30<31:23,  4.03s/it, running train loss: 0.61592]\u001b[A\n",
            "Trainging:  13%|█▎        | 71/537 [04:30<29:18,  3.77s/it, running train loss: 0.61592]\u001b[A\n",
            "Trainging:  13%|█▎        | 71/537 [04:35<29:18,  3.77s/it, running train loss: 0.63029]\u001b[A\n",
            "Trainging:  13%|█▎        | 72/537 [04:35<30:18,  3.91s/it, running train loss: 0.63029]\u001b[A\n",
            "Trainging:  13%|█▎        | 72/537 [04:41<30:18,  3.91s/it, running train loss: 0.61918]\u001b[A\n",
            "Trainging:  14%|█▎        | 73/537 [04:41<35:13,  4.56s/it, running train loss: 0.61918]\u001b[A\n",
            "Trainging:  14%|█▎        | 73/537 [04:44<35:13,  4.56s/it, running train loss: 0.66470]\u001b[A\n",
            "Trainging:  14%|█▍        | 74/537 [04:44<33:17,  4.31s/it, running train loss: 0.66470]\u001b[A\n",
            "Trainging:  14%|█▍        | 74/537 [04:49<33:17,  4.31s/it, running train loss: 0.64488]\u001b[A\n",
            "Trainging:  14%|█▍        | 75/537 [04:49<33:19,  4.33s/it, running train loss: 0.64488]\u001b[A\n",
            "Trainging:  14%|█▍        | 75/537 [04:52<33:19,  4.33s/it, running train loss: 0.66065]\u001b[A\n",
            "Trainging:  14%|█▍        | 76/537 [04:52<31:07,  4.05s/it, running train loss: 0.66065]\u001b[A\n",
            "Trainging:  14%|█▍        | 76/537 [04:56<31:07,  4.05s/it, running train loss: 0.56504]\u001b[A\n",
            "Trainging:  14%|█▍        | 77/537 [04:56<30:01,  3.92s/it, running train loss: 0.56504]\u001b[A\n",
            "Trainging:  14%|█▍        | 77/537 [05:02<30:01,  3.92s/it, running train loss: 0.55815]\u001b[A\n",
            "Trainging:  15%|█▍        | 78/537 [05:02<34:52,  4.56s/it, running train loss: 0.55815]\u001b[A\n",
            "Trainging:  15%|█▍        | 78/537 [05:06<34:52,  4.56s/it, running train loss: 0.53933]\u001b[A\n",
            "Trainging:  15%|█▍        | 79/537 [05:06<34:10,  4.48s/it, running train loss: 0.53933]\u001b[A\n",
            "Trainging:  15%|█▍        | 79/537 [05:10<34:10,  4.48s/it, running train loss: 0.53327]\u001b[A\n",
            "Trainging:  15%|█▍        | 80/537 [05:10<32:37,  4.28s/it, running train loss: 0.53327]\u001b[A\n",
            "Trainging:  15%|█▍        | 80/537 [05:14<32:37,  4.28s/it, running train loss: 0.63678]\u001b[A\n",
            "Trainging:  15%|█▌        | 81/537 [05:14<31:21,  4.13s/it, running train loss: 0.63678]\u001b[A\n",
            "Trainging:  15%|█▌        | 81/537 [05:17<31:21,  4.13s/it, running train loss: 0.60600]\u001b[A\n",
            "Trainging:  15%|█▌        | 82/537 [05:17<29:44,  3.92s/it, running train loss: 0.60600]\u001b[A\n",
            "Trainging:  15%|█▌        | 82/537 [05:21<29:44,  3.92s/it, running train loss: 0.50913]\u001b[A\n",
            "Trainging:  15%|█▌        | 83/537 [05:21<29:26,  3.89s/it, running train loss: 0.50913]\u001b[A\n",
            "Trainging:  15%|█▌        | 83/537 [05:25<29:26,  3.89s/it, running train loss: 0.55747]\u001b[A\n",
            "Trainging:  16%|█▌        | 84/537 [05:25<30:30,  4.04s/it, running train loss: 0.55747]\u001b[A\n",
            "Trainging:  16%|█▌        | 84/537 [05:29<30:30,  4.04s/it, running train loss: 0.53358]\u001b[A\n",
            "Trainging:  16%|█▌        | 85/537 [05:29<30:11,  4.01s/it, running train loss: 0.53358]\u001b[A\n",
            "Trainging:  16%|█▌        | 85/537 [05:33<30:11,  4.01s/it, running train loss: 0.57526]\u001b[A\n",
            "Trainging:  16%|█▌        | 86/537 [05:33<28:38,  3.81s/it, running train loss: 0.57526]\u001b[A\n",
            "Trainging:  16%|█▌        | 86/537 [05:36<28:38,  3.81s/it, running train loss: 0.51626]\u001b[A\n",
            "Trainging:  16%|█▌        | 87/537 [05:36<27:20,  3.65s/it, running train loss: 0.51626]\u001b[A\n",
            "Trainging:  16%|█▌        | 87/537 [05:40<27:20,  3.65s/it, running train loss: 0.55004]\u001b[A\n",
            "Trainging:  16%|█▋        | 88/537 [05:40<27:35,  3.69s/it, running train loss: 0.55004]\u001b[A\n",
            "Trainging:  16%|█▋        | 88/537 [05:43<27:35,  3.69s/it, running train loss: 0.68524]\u001b[A\n",
            "Trainging:  17%|█▋        | 89/537 [05:43<27:05,  3.63s/it, running train loss: 0.68524]\u001b[A\n",
            "Trainging:  17%|█▋        | 89/537 [05:47<27:05,  3.63s/it, running train loss: 0.50318]\u001b[A\n",
            "Trainging:  17%|█▋        | 90/537 [05:47<26:25,  3.55s/it, running train loss: 0.50318]\u001b[A\n",
            "Trainging:  17%|█▋        | 90/537 [05:51<26:25,  3.55s/it, running train loss: 0.57749]\u001b[A\n",
            "Trainging:  17%|█▋        | 91/537 [05:51<27:50,  3.75s/it, running train loss: 0.57749]\u001b[A\n",
            "Trainging:  17%|█▋        | 91/537 [05:54<27:50,  3.75s/it, running train loss: 0.54518]\u001b[A\n",
            "Trainging:  17%|█▋        | 92/537 [05:54<27:35,  3.72s/it, running train loss: 0.54518]\u001b[A\n",
            "Trainging:  17%|█▋        | 92/537 [06:00<27:35,  3.72s/it, running train loss: 0.58960]\u001b[A\n",
            "Trainging:  17%|█▋        | 93/537 [06:00<31:40,  4.28s/it, running train loss: 0.58960]\u001b[A\n",
            "Trainging:  17%|█▋        | 93/537 [06:05<31:40,  4.28s/it, running train loss: 0.55663]\u001b[A\n",
            "Trainging:  18%|█▊        | 94/537 [06:05<32:07,  4.35s/it, running train loss: 0.55663]\u001b[A\n",
            "Trainging:  18%|█▊        | 94/537 [06:08<32:07,  4.35s/it, running train loss: 0.74626]\u001b[A\n",
            "Trainging:  18%|█▊        | 95/537 [06:08<30:04,  4.08s/it, running train loss: 0.74626]\u001b[A\n",
            "Trainging:  18%|█▊        | 95/537 [06:13<30:04,  4.08s/it, running train loss: 0.61724]\u001b[A\n",
            "Trainging:  18%|█▊        | 96/537 [06:13<31:11,  4.24s/it, running train loss: 0.61724]\u001b[A\n",
            "Trainging:  18%|█▊        | 96/537 [06:15<31:11,  4.24s/it, running train loss: 0.58628]\u001b[A\n",
            "Trainging:  18%|█▊        | 97/537 [06:15<26:34,  3.62s/it, running train loss: 0.58628]\u001b[A\n",
            "Trainging:  18%|█▊        | 97/537 [06:19<26:34,  3.62s/it, running train loss: 0.58650]\u001b[A\n",
            "Trainging:  18%|█▊        | 98/537 [06:19<26:57,  3.68s/it, running train loss: 0.58650]\u001b[A\n",
            "Trainging:  18%|█▊        | 98/537 [06:22<26:57,  3.68s/it, running train loss: 0.59557]\u001b[A\n",
            "Trainging:  18%|█▊        | 99/537 [06:22<26:03,  3.57s/it, running train loss: 0.59557]\u001b[A\n",
            "Trainging:  18%|█▊        | 99/537 [06:27<26:03,  3.57s/it, running train loss: 0.55185]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|▏         | 1/68 [00:00<00:40,  1.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 2/68 [00:01<00:33,  1.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 3/68 [00:01<00:28,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 4/68 [00:01<00:27,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 5/68 [00:02<00:27,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 6/68 [00:02<00:24,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|█         | 7/68 [00:03<00:24,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 8/68 [00:03<00:25,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 9/68 [00:03<00:24,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 10/68 [00:04<00:24,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 11/68 [00:04<00:24,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 12/68 [00:05<00:23,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 13/68 [00:05<00:23,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 14/68 [00:06<00:23,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 15/68 [00:06<00:22,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▎       | 16/68 [00:06<00:22,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▌       | 17/68 [00:07<00:20,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 18/68 [00:07<00:20,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 19/68 [00:08<00:19,  2.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 20/68 [00:08<00:18,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 21/68 [00:08<00:18,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 22/68 [00:09<00:18,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 23/68 [00:09<00:18,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▌      | 24/68 [00:09<00:16,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 25/68 [00:10<00:17,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 26/68 [00:10<00:17,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 27/68 [00:11<00:16,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████      | 28/68 [00:11<00:15,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 29/68 [00:12<00:15,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 30/68 [00:12<00:15,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▌     | 31/68 [00:12<00:14,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 32/68 [00:13<00:14,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▊     | 33/68 [00:13<00:14,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 34/68 [00:14<00:13,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████▏    | 35/68 [00:14<00:12,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 36/68 [00:14<00:12,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▍    | 37/68 [00:15<00:12,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 38/68 [00:15<00:12,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 39/68 [00:16<00:12,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▉    | 40/68 [00:16<00:11,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 41/68 [00:16<00:10,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 42/68 [00:17<00:10,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 43/68 [00:17<00:10,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▍   | 44/68 [00:18<00:10,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 45/68 [00:18<00:09,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 46/68 [00:19<00:09,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 47/68 [00:19<00:09,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 48/68 [00:19<00:08,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 49/68 [00:20<00:08,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 50/68 [00:20<00:07,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 51/68 [00:21<00:07,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▋  | 52/68 [00:21<00:07,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 53/68 [00:22<00:06,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 54/68 [00:22<00:06,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 55/68 [00:22<00:05,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 56/68 [00:23<00:04,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 57/68 [00:23<00:04,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 58/68 [00:24<00:04,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 59/68 [00:24<00:03,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 60/68 [00:24<00:03,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|████████▉ | 61/68 [00:25<00:02,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 62/68 [00:25<00:02,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 63/68 [00:26<00:02,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 64/68 [00:26<00:01,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 65/68 [00:27<00:01,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 66/68 [00:27<00:00,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▊| 67/68 [00:27<00:00,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 68/68 [00:28<00:00,  2.41it/s]\n",
            "\n",
            "Trainging:  19%|█▊        | 100/537 [06:57<1:34:32, 12.98s/it, running train loss: 0.55185]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> training loss: 0.608127, valid loss: 0.581218,valid f1: 0.000000, valid acc:0.689991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Trainging:  19%|█▊        | 100/537 [07:01<1:34:32, 12.98s/it, running train loss: 0.67692]\u001b[A\n",
            "Trainging:  19%|█▉        | 101/537 [07:01<1:15:35, 10.40s/it, running train loss: 0.67692]\u001b[A\n",
            "Trainging:  19%|█▉        | 101/537 [07:06<1:15:35, 10.40s/it, running train loss: 0.57110]\u001b[A\n",
            "Trainging:  19%|█▉        | 102/537 [07:06<1:02:05,  8.56s/it, running train loss: 0.57110]\u001b[A\n",
            "Trainging:  19%|█▉        | 102/537 [07:09<1:02:05,  8.56s/it, running train loss: 0.53864]\u001b[A\n",
            "Trainging:  19%|█▉        | 103/537 [07:09<50:33,  6.99s/it, running train loss: 0.53864]  \u001b[A\n",
            "Trainging:  19%|█▉        | 103/537 [07:14<50:33,  6.99s/it, running train loss: 0.58431]\u001b[A\n",
            "Trainging:  19%|█▉        | 104/537 [07:14<46:10,  6.40s/it, running train loss: 0.58431]\u001b[A\n",
            "Trainging:  19%|█▉        | 104/537 [07:18<46:10,  6.40s/it, running train loss: 0.54549]\u001b[A\n",
            "Trainging:  20%|█▉        | 105/537 [07:18<41:43,  5.79s/it, running train loss: 0.54549]\u001b[A\n",
            "Trainging:  20%|█▉        | 105/537 [07:22<41:43,  5.79s/it, running train loss: 0.47662]\u001b[A\n",
            "Trainging:  20%|█▉        | 106/537 [07:22<37:18,  5.19s/it, running train loss: 0.47662]\u001b[A\n",
            "Trainging:  20%|█▉        | 106/537 [07:26<37:18,  5.19s/it, running train loss: 0.64241]\u001b[A\n",
            "Trainging:  20%|█▉        | 107/537 [07:26<34:33,  4.82s/it, running train loss: 0.64241]\u001b[A\n",
            "Trainging:  20%|█▉        | 107/537 [07:30<34:33,  4.82s/it, running train loss: 0.52789]\u001b[A\n",
            "Trainging:  20%|██        | 108/537 [07:30<32:12,  4.51s/it, running train loss: 0.52789]\u001b[A\n",
            "Trainging:  20%|██        | 108/537 [07:34<32:12,  4.51s/it, running train loss: 0.64398]\u001b[A\n",
            "Trainging:  20%|██        | 109/537 [07:34<31:31,  4.42s/it, running train loss: 0.64398]\u001b[A\n",
            "Trainging:  20%|██        | 109/537 [07:38<31:31,  4.42s/it, running train loss: 0.56635]\u001b[A\n",
            "Trainging:  20%|██        | 110/537 [07:38<31:27,  4.42s/it, running train loss: 0.56635]\u001b[A\n",
            "Trainging:  20%|██        | 110/537 [07:42<31:27,  4.42s/it, running train loss: 0.60863]\u001b[A\n",
            "Trainging:  21%|██        | 111/537 [07:42<30:21,  4.27s/it, running train loss: 0.60863]\u001b[A\n",
            "Trainging:  21%|██        | 111/537 [07:47<30:21,  4.27s/it, running train loss: 0.72356]\u001b[A\n",
            "Trainging:  21%|██        | 112/537 [07:47<30:37,  4.32s/it, running train loss: 0.72356]\u001b[A\n",
            "Trainging:  21%|██        | 112/537 [07:51<30:37,  4.32s/it, running train loss: 0.59658]\u001b[A\n",
            "Trainging:  21%|██        | 113/537 [07:51<29:23,  4.16s/it, running train loss: 0.59658]\u001b[A\n",
            "Trainging:  21%|██        | 113/537 [07:55<29:23,  4.16s/it, running train loss: 0.57366]\u001b[A\n",
            "Trainging:  21%|██        | 114/537 [07:55<30:55,  4.39s/it, running train loss: 0.57366]\u001b[A\n",
            "Trainging:  21%|██        | 114/537 [07:59<30:55,  4.39s/it, running train loss: 0.52382]\u001b[A\n",
            "Trainging:  21%|██▏       | 115/537 [07:59<29:46,  4.23s/it, running train loss: 0.52382]\u001b[A\n",
            "Trainging:  21%|██▏       | 115/537 [08:04<29:46,  4.23s/it, running train loss: 0.53027]\u001b[A\n",
            "Trainging:  22%|██▏       | 116/537 [08:04<30:11,  4.30s/it, running train loss: 0.53027]\u001b[A\n",
            "Trainging:  22%|██▏       | 116/537 [08:07<30:11,  4.30s/it, running train loss: 0.50848]\u001b[A\n",
            "Trainging:  22%|██▏       | 117/537 [08:07<27:59,  4.00s/it, running train loss: 0.50848]\u001b[A\n",
            "Trainging:  22%|██▏       | 117/537 [08:11<27:59,  4.00s/it, running train loss: 0.56866]\u001b[A\n",
            "Trainging:  22%|██▏       | 118/537 [08:11<28:31,  4.09s/it, running train loss: 0.56866]\u001b[A\n",
            "Trainging:  22%|██▏       | 118/537 [08:15<28:31,  4.09s/it, running train loss: 0.59035]\u001b[A\n",
            "Trainging:  22%|██▏       | 119/537 [08:15<27:17,  3.92s/it, running train loss: 0.59035]\u001b[A\n",
            "Trainging:  22%|██▏       | 119/537 [08:19<27:17,  3.92s/it, running train loss: 0.61634]\u001b[A\n",
            "Trainging:  22%|██▏       | 120/537 [08:19<28:28,  4.10s/it, running train loss: 0.61634]\u001b[A\n",
            "Trainging:  22%|██▏       | 120/537 [08:23<28:28,  4.10s/it, running train loss: 0.47614]\u001b[A\n",
            "Trainging:  23%|██▎       | 121/537 [08:23<26:51,  3.87s/it, running train loss: 0.47614]\u001b[A\n",
            "Trainging:  23%|██▎       | 121/537 [08:27<26:51,  3.87s/it, running train loss: 0.54330]\u001b[A\n",
            "Trainging:  23%|██▎       | 122/537 [08:27<26:55,  3.89s/it, running train loss: 0.54330]\u001b[A\n",
            "Trainging:  23%|██▎       | 122/537 [08:30<26:55,  3.89s/it, running train loss: 0.62758]\u001b[A\n",
            "Trainging:  23%|██▎       | 123/537 [08:30<26:18,  3.81s/it, running train loss: 0.62758]\u001b[A\n",
            "Trainging:  23%|██▎       | 123/537 [08:34<26:18,  3.81s/it, running train loss: 0.46256]\u001b[A\n",
            "Trainging:  23%|██▎       | 124/537 [08:34<25:16,  3.67s/it, running train loss: 0.46256]\u001b[A\n",
            "Trainging:  23%|██▎       | 124/537 [08:37<25:16,  3.67s/it, running train loss: 0.55726]\u001b[A\n",
            "Trainging:  23%|██▎       | 125/537 [08:37<25:26,  3.71s/it, running train loss: 0.55726]\u001b[A\n",
            "Trainging:  23%|██▎       | 125/537 [08:41<25:26,  3.71s/it, running train loss: 0.62924]\u001b[A\n",
            "Trainging:  23%|██▎       | 126/537 [08:41<25:08,  3.67s/it, running train loss: 0.62924]\u001b[A\n",
            "Trainging:  23%|██▎       | 126/537 [08:43<25:08,  3.67s/it, running train loss: 0.54638]\u001b[A\n",
            "Trainging:  24%|██▎       | 127/537 [08:43<21:53,  3.20s/it, running train loss: 0.54638]\u001b[A\n",
            "Trainging:  24%|██▎       | 127/537 [08:47<21:53,  3.20s/it, running train loss: 0.51477]\u001b[A\n",
            "Trainging:  24%|██▍       | 128/537 [08:47<22:06,  3.24s/it, running train loss: 0.51477]\u001b[A\n",
            "Trainging:  24%|██▍       | 128/537 [08:50<22:06,  3.24s/it, running train loss: 0.68431]\u001b[A\n",
            "Trainging:  24%|██▍       | 129/537 [08:50<22:52,  3.36s/it, running train loss: 0.68431]\u001b[A\n",
            "Trainging:  24%|██▍       | 129/537 [08:54<22:52,  3.36s/it, running train loss: 0.55130]\u001b[A\n",
            "Trainging:  24%|██▍       | 130/537 [08:54<23:51,  3.52s/it, running train loss: 0.55130]\u001b[A\n",
            "Trainging:  24%|██▍       | 130/537 [08:58<23:51,  3.52s/it, running train loss: 0.52855]\u001b[A\n",
            "Trainging:  24%|██▍       | 131/537 [08:58<23:51,  3.52s/it, running train loss: 0.52855]\u001b[A\n",
            "Trainging:  24%|██▍       | 131/537 [09:01<23:51,  3.52s/it, running train loss: 0.55400]\u001b[A\n",
            "Trainging:  25%|██▍       | 132/537 [09:01<24:15,  3.59s/it, running train loss: 0.55400]\u001b[A\n",
            "Trainging:  25%|██▍       | 132/537 [09:05<24:15,  3.59s/it, running train loss: 0.66429]\u001b[A\n",
            "Trainging:  25%|██▍       | 133/537 [09:05<23:57,  3.56s/it, running train loss: 0.66429]\u001b[A\n",
            "Trainging:  25%|██▍       | 133/537 [09:08<23:57,  3.56s/it, running train loss: 0.67925]\u001b[A\n",
            "Trainging:  25%|██▍       | 134/537 [09:08<23:15,  3.46s/it, running train loss: 0.67925]\u001b[A\n",
            "Trainging:  25%|██▍       | 134/537 [09:12<23:15,  3.46s/it, running train loss: 0.65358]\u001b[A\n",
            "Trainging:  25%|██▌       | 135/537 [09:12<24:48,  3.70s/it, running train loss: 0.65358]\u001b[A\n",
            "Trainging:  25%|██▌       | 135/537 [09:16<24:48,  3.70s/it, running train loss: 0.54564]\u001b[A\n",
            "Trainging:  25%|██▌       | 136/537 [09:16<24:24,  3.65s/it, running train loss: 0.54564]\u001b[A\n",
            "Trainging:  25%|██▌       | 136/537 [09:19<24:24,  3.65s/it, running train loss: 0.59897]\u001b[A\n",
            "Trainging:  26%|██▌       | 137/537 [09:19<23:34,  3.54s/it, running train loss: 0.59897]\u001b[A\n",
            "Trainging:  26%|██▌       | 137/537 [09:25<23:34,  3.54s/it, running train loss: 0.62508]\u001b[A\n",
            "Trainging:  26%|██▌       | 138/537 [09:25<28:25,  4.27s/it, running train loss: 0.62508]\u001b[A\n",
            "Trainging:  26%|██▌       | 138/537 [09:29<28:25,  4.27s/it, running train loss: 0.51550]\u001b[A\n",
            "Trainging:  26%|██▌       | 139/537 [09:29<27:41,  4.17s/it, running train loss: 0.51550]\u001b[A\n",
            "Trainging:  26%|██▌       | 139/537 [09:33<27:41,  4.17s/it, running train loss: 0.56875]\u001b[A\n",
            "Trainging:  26%|██▌       | 140/537 [09:33<28:07,  4.25s/it, running train loss: 0.56875]\u001b[A\n",
            "Trainging:  26%|██▌       | 140/537 [09:38<28:07,  4.25s/it, running train loss: 0.53700]\u001b[A\n",
            "Trainging:  26%|██▋       | 141/537 [09:38<28:55,  4.38s/it, running train loss: 0.53700]\u001b[A\n",
            "Trainging:  26%|██▋       | 141/537 [09:42<28:55,  4.38s/it, running train loss: 0.51579]\u001b[A\n",
            "Trainging:  26%|██▋       | 142/537 [09:42<27:05,  4.11s/it, running train loss: 0.51579]\u001b[A\n",
            "Trainging:  26%|██▋       | 142/537 [09:45<27:05,  4.11s/it, running train loss: 0.57816]\u001b[A\n",
            "Trainging:  27%|██▋       | 143/537 [09:45<25:55,  3.95s/it, running train loss: 0.57816]\u001b[A\n",
            "Trainging:  27%|██▋       | 143/537 [09:49<25:55,  3.95s/it, running train loss: 0.54613]\u001b[A\n",
            "Trainging:  27%|██▋       | 144/537 [09:49<25:03,  3.83s/it, running train loss: 0.54613]\u001b[A\n",
            "Trainging:  27%|██▋       | 144/537 [09:52<25:03,  3.83s/it, running train loss: 0.57638]\u001b[A\n",
            "Trainging:  27%|██▋       | 145/537 [09:52<24:20,  3.73s/it, running train loss: 0.57638]\u001b[A\n",
            "Trainging:  27%|██▋       | 145/537 [09:56<24:20,  3.73s/it, running train loss: 0.59712]\u001b[A\n",
            "Trainging:  27%|██▋       | 146/537 [09:56<24:23,  3.74s/it, running train loss: 0.59712]\u001b[A\n",
            "Trainging:  27%|██▋       | 146/537 [10:00<24:23,  3.74s/it, running train loss: 0.59399]\u001b[A\n",
            "Trainging:  27%|██▋       | 147/537 [10:00<24:31,  3.77s/it, running train loss: 0.59399]\u001b[A\n",
            "Trainging:  27%|██▋       | 147/537 [10:05<24:31,  3.77s/it, running train loss: 0.61446]\u001b[A\n",
            "Trainging:  28%|██▊       | 148/537 [10:05<27:30,  4.24s/it, running train loss: 0.61446]\u001b[A\n",
            "Trainging:  28%|██▊       | 148/537 [10:09<27:30,  4.24s/it, running train loss: 0.57428]\u001b[A\n",
            "Trainging:  28%|██▊       | 149/537 [10:09<26:11,  4.05s/it, running train loss: 0.57428]\u001b[A\n",
            "Trainging:  28%|██▊       | 149/537 [10:13<26:11,  4.05s/it, running train loss: 0.45245]\u001b[A\n",
            "Trainging:  28%|██▊       | 150/537 [10:13<25:41,  3.98s/it, running train loss: 0.45245]\u001b[A\n",
            "Trainging:  28%|██▊       | 150/537 [10:16<25:41,  3.98s/it, running train loss: 0.52694]\u001b[A\n",
            "Trainging:  28%|██▊       | 151/537 [10:16<24:34,  3.82s/it, running train loss: 0.52694]\u001b[A\n",
            "Trainging:  28%|██▊       | 151/537 [10:20<24:34,  3.82s/it, running train loss: 0.45722]\u001b[A\n",
            "Trainging:  28%|██▊       | 152/537 [10:20<25:15,  3.94s/it, running train loss: 0.45722]\u001b[A\n",
            "Trainging:  28%|██▊       | 152/537 [10:24<25:15,  3.94s/it, running train loss: 0.59440]\u001b[A\n",
            "Trainging:  28%|██▊       | 153/537 [10:24<25:03,  3.92s/it, running train loss: 0.59440]\u001b[A\n",
            "Trainging:  28%|██▊       | 153/537 [10:28<25:03,  3.92s/it, running train loss: 0.57010]\u001b[A\n",
            "Trainging:  29%|██▊       | 154/537 [10:28<24:12,  3.79s/it, running train loss: 0.57010]\u001b[A\n",
            "Trainging:  29%|██▊       | 154/537 [10:31<24:12,  3.79s/it, running train loss: 0.65090]\u001b[A\n",
            "Trainging:  29%|██▉       | 155/537 [10:31<23:22,  3.67s/it, running train loss: 0.65090]\u001b[A\n",
            "Trainging:  29%|██▉       | 155/537 [10:34<23:22,  3.67s/it, running train loss: 0.62600]\u001b[A\n",
            "Trainging:  29%|██▉       | 156/537 [10:34<22:48,  3.59s/it, running train loss: 0.62600]\u001b[A\n",
            "Trainging:  29%|██▉       | 156/537 [10:38<22:48,  3.59s/it, running train loss: 0.53213]\u001b[A\n",
            "Trainging:  29%|██▉       | 157/537 [10:38<22:45,  3.59s/it, running train loss: 0.53213]\u001b[A\n",
            "Trainging:  29%|██▉       | 157/537 [10:42<22:45,  3.59s/it, running train loss: 0.39908]\u001b[A\n",
            "Trainging:  29%|██▉       | 158/537 [10:42<23:17,  3.69s/it, running train loss: 0.39908]\u001b[A\n",
            "Trainging:  29%|██▉       | 158/537 [10:45<23:17,  3.69s/it, running train loss: 0.66637]\u001b[A\n",
            "Trainging:  30%|██▉       | 159/537 [10:45<22:44,  3.61s/it, running train loss: 0.66637]\u001b[A\n",
            "Trainging:  30%|██▉       | 159/537 [10:49<22:44,  3.61s/it, running train loss: 0.64026]\u001b[A\n",
            "Trainging:  30%|██▉       | 160/537 [10:49<23:15,  3.70s/it, running train loss: 0.64026]\u001b[A\n",
            "Trainging:  30%|██▉       | 160/537 [10:53<23:15,  3.70s/it, running train loss: 0.51049]\u001b[A\n",
            "Trainging:  30%|██▉       | 161/537 [10:53<23:25,  3.74s/it, running train loss: 0.51049]\u001b[A\n",
            "Trainging:  30%|██▉       | 161/537 [10:58<23:25,  3.74s/it, running train loss: 0.60485]\u001b[A\n",
            "Trainging:  30%|███       | 162/537 [10:58<24:43,  3.96s/it, running train loss: 0.60485]\u001b[A\n",
            "Trainging:  30%|███       | 162/537 [11:01<24:43,  3.96s/it, running train loss: 0.54971]\u001b[A\n",
            "Trainging:  30%|███       | 163/537 [11:01<24:19,  3.90s/it, running train loss: 0.54971]\u001b[A\n",
            "Trainging:  30%|███       | 163/537 [11:05<24:19,  3.90s/it, running train loss: 0.58819]\u001b[A\n",
            "Trainging:  31%|███       | 164/537 [11:05<23:46,  3.82s/it, running train loss: 0.58819]\u001b[A\n",
            "Trainging:  31%|███       | 164/537 [11:09<23:46,  3.82s/it, running train loss: 0.53630]\u001b[A\n",
            "Trainging:  31%|███       | 165/537 [11:09<23:06,  3.73s/it, running train loss: 0.53630]\u001b[A\n",
            "Trainging:  31%|███       | 165/537 [11:12<23:06,  3.73s/it, running train loss: 0.53293]\u001b[A\n",
            "Trainging:  31%|███       | 166/537 [11:12<22:25,  3.63s/it, running train loss: 0.53293]\u001b[A\n",
            "Trainging:  31%|███       | 166/537 [11:16<22:25,  3.63s/it, running train loss: 0.50455]\u001b[A\n",
            "Trainging:  31%|███       | 167/537 [11:16<22:22,  3.63s/it, running train loss: 0.50455]\u001b[A\n",
            "Trainging:  31%|███       | 167/537 [11:19<22:22,  3.63s/it, running train loss: 0.51482]\u001b[A\n",
            "Trainging:  31%|███▏      | 168/537 [11:19<22:24,  3.64s/it, running train loss: 0.51482]\u001b[A\n",
            "Trainging:  31%|███▏      | 168/537 [11:23<22:24,  3.64s/it, running train loss: 0.46498]\u001b[A\n",
            "Trainging:  31%|███▏      | 169/537 [11:23<21:48,  3.56s/it, running train loss: 0.46498]\u001b[A\n",
            "Trainging:  31%|███▏      | 169/537 [11:28<21:48,  3.56s/it, running train loss: 0.68251]\u001b[A\n",
            "Trainging:  32%|███▏      | 170/537 [11:28<25:50,  4.23s/it, running train loss: 0.68251]\u001b[A\n",
            "Trainging:  32%|███▏      | 170/537 [11:32<25:50,  4.23s/it, running train loss: 0.58385]\u001b[A\n",
            "Trainging:  32%|███▏      | 171/537 [11:32<25:14,  4.14s/it, running train loss: 0.58385]\u001b[A\n",
            "Trainging:  32%|███▏      | 171/537 [11:37<25:14,  4.14s/it, running train loss: 0.58950]\u001b[A\n",
            "Trainging:  32%|███▏      | 172/537 [11:37<25:42,  4.23s/it, running train loss: 0.58950]\u001b[A\n",
            "Trainging:  32%|███▏      | 172/537 [11:40<25:42,  4.23s/it, running train loss: 0.51311]\u001b[A\n",
            "Trainging:  32%|███▏      | 173/537 [11:40<24:27,  4.03s/it, running train loss: 0.51311]\u001b[A\n",
            "Trainging:  32%|███▏      | 173/537 [11:44<24:27,  4.03s/it, running train loss: 0.53718]\u001b[A\n",
            "Trainging:  32%|███▏      | 174/537 [11:44<23:19,  3.86s/it, running train loss: 0.53718]\u001b[A\n",
            "Trainging:  32%|███▏      | 174/537 [11:48<23:19,  3.86s/it, running train loss: 0.55749]\u001b[A\n",
            "Trainging:  33%|███▎      | 175/537 [11:48<23:55,  3.97s/it, running train loss: 0.55749]\u001b[A\n",
            "Trainging:  33%|███▎      | 175/537 [11:51<23:55,  3.97s/it, running train loss: 0.49639]\u001b[A\n",
            "Trainging:  33%|███▎      | 176/537 [11:51<22:49,  3.79s/it, running train loss: 0.49639]\u001b[A\n",
            "Trainging:  33%|███▎      | 176/537 [11:56<22:49,  3.79s/it, running train loss: 0.52997]\u001b[A\n",
            "Trainging:  33%|███▎      | 177/537 [11:56<23:37,  3.94s/it, running train loss: 0.52997]\u001b[A\n",
            "Trainging:  33%|███▎      | 177/537 [11:59<23:37,  3.94s/it, running train loss: 0.48725]\u001b[A\n",
            "Trainging:  33%|███▎      | 178/537 [11:59<23:14,  3.89s/it, running train loss: 0.48725]\u001b[A\n",
            "Trainging:  33%|███▎      | 178/537 [12:04<23:14,  3.89s/it, running train loss: 0.61684]\u001b[A\n",
            "Trainging:  33%|███▎      | 179/537 [12:04<24:55,  4.18s/it, running train loss: 0.61684]\u001b[A\n",
            "Trainging:  33%|███▎      | 179/537 [12:09<24:55,  4.18s/it, running train loss: 0.59739]\u001b[A\n",
            "Trainging:  34%|███▎      | 180/537 [12:09<25:33,  4.29s/it, running train loss: 0.59739]\u001b[A\n",
            "Trainging:  34%|███▎      | 180/537 [12:13<25:33,  4.29s/it, running train loss: 0.45736]\u001b[A\n",
            "Trainging:  34%|███▎      | 181/537 [12:13<24:34,  4.14s/it, running train loss: 0.45736]\u001b[A\n",
            "Trainging:  34%|███▎      | 181/537 [12:16<24:34,  4.14s/it, running train loss: 0.61877]\u001b[A\n",
            "Trainging:  34%|███▍      | 182/537 [12:16<23:46,  4.02s/it, running train loss: 0.61877]\u001b[A\n",
            "Trainging:  34%|███▍      | 182/537 [12:20<23:46,  4.02s/it, running train loss: 0.56133]\u001b[A\n",
            "Trainging:  34%|███▍      | 183/537 [12:20<23:16,  3.95s/it, running train loss: 0.56133]\u001b[A\n",
            "Trainging:  34%|███▍      | 183/537 [12:24<23:16,  3.95s/it, running train loss: 0.58876]\u001b[A\n",
            "Trainging:  34%|███▍      | 184/537 [12:24<22:19,  3.79s/it, running train loss: 0.58876]\u001b[A\n",
            "Trainging:  34%|███▍      | 184/537 [12:29<22:19,  3.79s/it, running train loss: 0.47648]\u001b[A\n",
            "Trainging:  34%|███▍      | 185/537 [12:29<24:26,  4.17s/it, running train loss: 0.47648]\u001b[A\n",
            "Trainging:  34%|███▍      | 185/537 [12:32<24:26,  4.17s/it, running train loss: 0.56303]\u001b[A\n",
            "Trainging:  35%|███▍      | 186/537 [12:32<23:11,  3.96s/it, running train loss: 0.56303]\u001b[A\n",
            "Trainging:  35%|███▍      | 186/537 [12:36<23:11,  3.96s/it, running train loss: 0.57499]\u001b[A\n",
            "Trainging:  35%|███▍      | 187/537 [12:36<23:04,  3.96s/it, running train loss: 0.57499]\u001b[A\n",
            "Trainging:  35%|███▍      | 187/537 [12:40<23:04,  3.96s/it, running train loss: 0.51828]\u001b[A\n",
            "Trainging:  35%|███▌      | 188/537 [12:40<22:56,  3.94s/it, running train loss: 0.51828]\u001b[A\n",
            "Trainging:  35%|███▌      | 188/537 [12:45<22:56,  3.94s/it, running train loss: 0.50975]\u001b[A\n",
            "Trainging:  35%|███▌      | 189/537 [12:45<24:10,  4.17s/it, running train loss: 0.50975]\u001b[A\n",
            "Trainging:  35%|███▌      | 189/537 [12:51<24:10,  4.17s/it, running train loss: 0.64016]\u001b[A\n",
            "Trainging:  35%|███▌      | 190/537 [12:51<27:20,  4.73s/it, running train loss: 0.64016]\u001b[A\n",
            "Trainging:  35%|███▌      | 190/537 [12:56<27:20,  4.73s/it, running train loss: 0.56470]\u001b[A\n",
            "Trainging:  36%|███▌      | 191/537 [12:56<28:51,  5.01s/it, running train loss: 0.56470]\u001b[A\n",
            "Trainging:  36%|███▌      | 191/537 [13:00<28:51,  5.01s/it, running train loss: 0.55084]\u001b[A\n",
            "Trainging:  36%|███▌      | 192/537 [13:00<26:55,  4.68s/it, running train loss: 0.55084]\u001b[A\n",
            "Trainging:  36%|███▌      | 192/537 [13:04<26:55,  4.68s/it, running train loss: 0.52823]\u001b[A\n",
            "Trainging:  36%|███▌      | 193/537 [13:04<24:44,  4.32s/it, running train loss: 0.52823]\u001b[A\n",
            "Trainging:  36%|███▌      | 193/537 [13:09<24:44,  4.32s/it, running train loss: 0.55915]\u001b[A\n",
            "Trainging:  36%|███▌      | 194/537 [13:09<26:18,  4.60s/it, running train loss: 0.55915]\u001b[A\n",
            "Trainging:  36%|███▌      | 194/537 [13:12<26:18,  4.60s/it, running train loss: 0.60830]\u001b[A\n",
            "Trainging:  36%|███▋      | 195/537 [13:12<24:06,  4.23s/it, running train loss: 0.60830]\u001b[A\n",
            "Trainging:  36%|███▋      | 195/537 [13:16<24:06,  4.23s/it, running train loss: 0.52755]\u001b[A\n",
            "Trainging:  36%|███▋      | 196/537 [13:16<22:57,  4.04s/it, running train loss: 0.52755]\u001b[A\n",
            "Trainging:  36%|███▋      | 196/537 [13:20<22:57,  4.04s/it, running train loss: 0.47792]\u001b[A\n",
            "Trainging:  37%|███▋      | 197/537 [13:20<22:28,  3.97s/it, running train loss: 0.47792]\u001b[A\n",
            "Trainging:  37%|███▋      | 197/537 [13:23<22:28,  3.97s/it, running train loss: 0.55694]\u001b[A\n",
            "Trainging:  37%|███▋      | 198/537 [13:23<21:42,  3.84s/it, running train loss: 0.55694]\u001b[A\n",
            "Trainging:  37%|███▋      | 198/537 [13:27<21:42,  3.84s/it, running train loss: 0.53899]\u001b[A\n",
            "Trainging:  37%|███▋      | 199/537 [13:27<21:46,  3.87s/it, running train loss: 0.53899]\u001b[A\n",
            "Trainging:  37%|███▋      | 199/537 [13:31<21:46,  3.87s/it, running train loss: 0.58712]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|▏         | 1/68 [00:00<00:40,  1.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 2/68 [00:01<00:34,  1.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 3/68 [00:01<00:28,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 4/68 [00:01<00:28,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 5/68 [00:02<00:27,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 6/68 [00:02<00:24,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|█         | 7/68 [00:03<00:24,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 8/68 [00:03<00:24,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 9/68 [00:03<00:24,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 10/68 [00:04<00:24,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 11/68 [00:04<00:23,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 12/68 [00:05<00:23,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 13/68 [00:05<00:23,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 14/68 [00:06<00:23,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 15/68 [00:06<00:23,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▎       | 16/68 [00:06<00:22,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▌       | 17/68 [00:07<00:20,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 18/68 [00:07<00:20,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 19/68 [00:08<00:19,  2.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 20/68 [00:08<00:18,  2.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 21/68 [00:08<00:18,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 22/68 [00:09<00:18,  2.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 23/68 [00:09<00:18,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▌      | 24/68 [00:09<00:17,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 25/68 [00:10<00:17,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 26/68 [00:10<00:17,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 27/68 [00:11<00:17,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████      | 28/68 [00:11<00:15,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 29/68 [00:12<00:15,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 30/68 [00:12<00:15,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▌     | 31/68 [00:12<00:14,  2.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 32/68 [00:13<00:14,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▊     | 33/68 [00:13<00:14,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 34/68 [00:14<00:13,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████▏    | 35/68 [00:14<00:12,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 36/68 [00:14<00:12,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▍    | 37/68 [00:15<00:12,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 38/68 [00:15<00:12,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 39/68 [00:16<00:12,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▉    | 40/68 [00:16<00:11,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 41/68 [00:16<00:10,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 42/68 [00:17<00:10,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 43/68 [00:17<00:10,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▍   | 44/68 [00:18<00:10,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 45/68 [00:18<00:09,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 46/68 [00:19<00:09,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 47/68 [00:19<00:09,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 48/68 [00:19<00:08,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 49/68 [00:20<00:08,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 50/68 [00:20<00:07,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 51/68 [00:21<00:07,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▋  | 52/68 [00:21<00:06,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 53/68 [00:22<00:06,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 54/68 [00:22<00:06,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 55/68 [00:22<00:05,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 56/68 [00:23<00:04,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 57/68 [00:23<00:04,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 58/68 [00:24<00:04,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 59/68 [00:24<00:03,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 60/68 [00:24<00:03,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|████████▉ | 61/68 [00:25<00:02,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 62/68 [00:25<00:02,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 63/68 [00:26<00:02,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 64/68 [00:26<00:01,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 65/68 [00:26<00:01,  2.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 66/68 [00:27<00:00,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▊| 67/68 [00:27<00:00,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 68/68 [00:28<00:00,  2.41it/s]\n",
            "\n",
            "Trainging:  37%|███▋      | 200/537 [14:00<1:09:48, 12.43s/it, running train loss: 0.58712]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> training loss: 0.563966, valid loss: 0.538982,valid f1: 0.000000, valid acc:0.689991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Trainging:  37%|███▋      | 200/537 [14:03<1:09:48, 12.43s/it, running train loss: 0.46188]\u001b[A\n",
            "Trainging:  37%|███▋      | 201/537 [14:03<55:12,  9.86s/it, running train loss: 0.46188]  \u001b[A\n",
            "Trainging:  37%|███▋      | 201/537 [14:07<55:12,  9.86s/it, running train loss: 0.42793]\u001b[A\n",
            "Trainging:  38%|███▊      | 202/537 [14:07<45:05,  8.08s/it, running train loss: 0.42793]\u001b[A\n",
            "Trainging:  38%|███▊      | 202/537 [14:11<45:05,  8.08s/it, running train loss: 0.55234]\u001b[A\n",
            "Trainging:  38%|███▊      | 203/537 [14:11<37:59,  6.83s/it, running train loss: 0.55234]\u001b[A\n",
            "Trainging:  38%|███▊      | 203/537 [14:15<37:59,  6.83s/it, running train loss: 0.59766]\u001b[A\n",
            "Trainging:  38%|███▊      | 204/537 [14:15<32:19,  5.82s/it, running train loss: 0.59766]\u001b[A\n",
            "Trainging:  38%|███▊      | 204/537 [14:19<32:19,  5.82s/it, running train loss: 0.49247]\u001b[A\n",
            "Trainging:  38%|███▊      | 205/537 [14:19<29:59,  5.42s/it, running train loss: 0.49247]\u001b[A\n",
            "Trainging:  38%|███▊      | 205/537 [14:23<29:59,  5.42s/it, running train loss: 0.62042]\u001b[A\n",
            "Trainging:  38%|███▊      | 206/537 [14:23<27:15,  4.94s/it, running train loss: 0.62042]\u001b[A\n",
            "Trainging:  38%|███▊      | 206/537 [14:26<27:15,  4.94s/it, running train loss: 0.55874]\u001b[A\n",
            "Trainging:  39%|███▊      | 207/537 [14:26<24:37,  4.48s/it, running train loss: 0.55874]\u001b[A\n",
            "Trainging:  39%|███▊      | 207/537 [14:30<24:37,  4.48s/it, running train loss: 0.54519]\u001b[A\n",
            "Trainging:  39%|███▊      | 208/537 [14:30<23:31,  4.29s/it, running train loss: 0.54519]\u001b[A\n",
            "Trainging:  39%|███▊      | 208/537 [14:33<23:31,  4.29s/it, running train loss: 0.61666]\u001b[A\n",
            "Trainging:  39%|███▉      | 209/537 [14:33<21:35,  3.95s/it, running train loss: 0.61666]\u001b[A\n",
            "Trainging:  39%|███▉      | 209/537 [14:37<21:35,  3.95s/it, running train loss: 0.55944]\u001b[A\n",
            "Trainging:  39%|███▉      | 210/537 [14:37<21:15,  3.90s/it, running train loss: 0.55944]\u001b[A\n",
            "Trainging:  39%|███▉      | 210/537 [14:41<21:15,  3.90s/it, running train loss: 0.56601]\u001b[A\n",
            "Trainging:  39%|███▉      | 211/537 [14:41<20:54,  3.85s/it, running train loss: 0.56601]\u001b[A\n",
            "Trainging:  39%|███▉      | 211/537 [14:45<20:54,  3.85s/it, running train loss: 0.57983]\u001b[A\n",
            "Trainging:  39%|███▉      | 212/537 [14:45<20:30,  3.78s/it, running train loss: 0.57983]\u001b[A\n",
            "Trainging:  39%|███▉      | 212/537 [14:48<20:30,  3.78s/it, running train loss: 0.56149]\u001b[A\n",
            "Trainging:  40%|███▉      | 213/537 [14:48<19:35,  3.63s/it, running train loss: 0.56149]\u001b[A\n",
            "Trainging:  40%|███▉      | 213/537 [14:52<19:35,  3.63s/it, running train loss: 0.55717]\u001b[A\n",
            "Trainging:  40%|███▉      | 214/537 [14:52<20:00,  3.72s/it, running train loss: 0.55717]\u001b[A\n",
            "Trainging:  40%|███▉      | 214/537 [14:56<20:00,  3.72s/it, running train loss: 0.53874]\u001b[A\n",
            "Trainging:  40%|████      | 215/537 [14:56<21:00,  3.91s/it, running train loss: 0.53874]\u001b[A\n",
            "Trainging:  40%|████      | 215/537 [15:00<21:00,  3.91s/it, running train loss: 0.54556]\u001b[A\n",
            "Trainging:  40%|████      | 216/537 [15:00<21:24,  4.00s/it, running train loss: 0.54556]\u001b[A\n",
            "Trainging:  40%|████      | 216/537 [15:04<21:24,  4.00s/it, running train loss: 0.57758]\u001b[A\n",
            "Trainging:  40%|████      | 217/537 [15:04<21:04,  3.95s/it, running train loss: 0.57758]\u001b[A\n",
            "Trainging:  40%|████      | 217/537 [15:09<21:04,  3.95s/it, running train loss: 0.58933]\u001b[A\n",
            "Trainging:  41%|████      | 218/537 [15:09<21:48,  4.10s/it, running train loss: 0.58933]\u001b[A\n",
            "Trainging:  41%|████      | 218/537 [15:13<21:48,  4.10s/it, running train loss: 0.50676]\u001b[A\n",
            "Trainging:  41%|████      | 219/537 [15:13<22:01,  4.16s/it, running train loss: 0.50676]\u001b[A\n",
            "Trainging:  41%|████      | 219/537 [15:16<22:01,  4.16s/it, running train loss: 0.53773]\u001b[A\n",
            "Trainging:  41%|████      | 220/537 [15:17<20:57,  3.97s/it, running train loss: 0.53773]\u001b[A\n",
            "Trainging:  41%|████      | 220/537 [15:20<20:57,  3.97s/it, running train loss: 0.58251]\u001b[A\n",
            "Trainging:  41%|████      | 221/537 [15:20<20:35,  3.91s/it, running train loss: 0.58251]\u001b[A\n",
            "Trainging:  41%|████      | 221/537 [15:24<20:35,  3.91s/it, running train loss: 0.45074]\u001b[A\n",
            "Trainging:  41%|████▏     | 222/537 [15:24<20:19,  3.87s/it, running train loss: 0.45074]\u001b[A\n",
            "Trainging:  41%|████▏     | 222/537 [15:28<20:19,  3.87s/it, running train loss: 0.56540]\u001b[A\n",
            "Trainging:  42%|████▏     | 223/537 [15:28<20:09,  3.85s/it, running train loss: 0.56540]\u001b[A\n",
            "Trainging:  42%|████▏     | 223/537 [15:32<20:09,  3.85s/it, running train loss: 0.45551]\u001b[A\n",
            "Trainging:  42%|████▏     | 224/537 [15:32<19:55,  3.82s/it, running train loss: 0.45551]\u001b[A\n",
            "Trainging:  42%|████▏     | 224/537 [15:36<19:55,  3.82s/it, running train loss: 0.60034]\u001b[A\n",
            "Trainging:  42%|████▏     | 225/537 [15:36<20:02,  3.85s/it, running train loss: 0.60034]\u001b[A\n",
            "Trainging:  42%|████▏     | 225/537 [15:39<20:02,  3.85s/it, running train loss: 0.51041]\u001b[A\n",
            "Trainging:  42%|████▏     | 226/537 [15:39<19:55,  3.84s/it, running train loss: 0.51041]\u001b[A\n",
            "Trainging:  42%|████▏     | 226/537 [15:44<19:55,  3.84s/it, running train loss: 0.58900]\u001b[A\n",
            "Trainging:  42%|████▏     | 227/537 [15:44<20:57,  4.06s/it, running train loss: 0.58900]\u001b[A\n",
            "Trainging:  42%|████▏     | 227/537 [15:49<20:57,  4.06s/it, running train loss: 0.60051]\u001b[A\n",
            "Trainging:  42%|████▏     | 228/537 [15:49<21:43,  4.22s/it, running train loss: 0.60051]\u001b[A\n",
            "Trainging:  42%|████▏     | 228/537 [15:52<21:43,  4.22s/it, running train loss: 0.49123]\u001b[A\n",
            "Trainging:  43%|████▎     | 229/537 [15:52<20:10,  3.93s/it, running train loss: 0.49123]\u001b[A\n",
            "Trainging:  43%|████▎     | 229/537 [15:55<20:10,  3.93s/it, running train loss: 0.57204]\u001b[A\n",
            "Trainging:  43%|████▎     | 230/537 [15:55<18:56,  3.70s/it, running train loss: 0.57204]\u001b[A\n",
            "Trainging:  43%|████▎     | 230/537 [15:59<18:56,  3.70s/it, running train loss: 0.60263]\u001b[A\n",
            "Trainging:  43%|████▎     | 231/537 [15:59<18:44,  3.67s/it, running train loss: 0.60263]\u001b[A\n",
            "Trainging:  43%|████▎     | 231/537 [16:03<18:44,  3.67s/it, running train loss: 0.57346]\u001b[A\n",
            "Trainging:  43%|████▎     | 232/537 [16:03<19:35,  3.85s/it, running train loss: 0.57346]\u001b[A\n",
            "Trainging:  43%|████▎     | 232/537 [16:06<19:35,  3.85s/it, running train loss: 0.46762]\u001b[A\n",
            "Trainging:  43%|████▎     | 233/537 [16:06<18:42,  3.69s/it, running train loss: 0.46762]\u001b[A\n",
            "Trainging:  43%|████▎     | 233/537 [16:10<18:42,  3.69s/it, running train loss: 0.55480]\u001b[A\n",
            "Trainging:  44%|████▎     | 234/537 [16:10<19:32,  3.87s/it, running train loss: 0.55480]\u001b[A\n",
            "Trainging:  44%|████▎     | 234/537 [16:14<19:32,  3.87s/it, running train loss: 0.50115]\u001b[A\n",
            "Trainging:  44%|████▍     | 235/537 [16:14<19:03,  3.79s/it, running train loss: 0.50115]\u001b[A\n",
            "Trainging:  44%|████▍     | 235/537 [16:17<19:03,  3.79s/it, running train loss: 0.45831]\u001b[A\n",
            "Trainging:  44%|████▍     | 236/537 [16:17<18:24,  3.67s/it, running train loss: 0.45831]\u001b[A\n",
            "Trainging:  44%|████▍     | 236/537 [16:21<18:24,  3.67s/it, running train loss: 0.50462]\u001b[A\n",
            "Trainging:  44%|████▍     | 237/537 [16:21<18:34,  3.72s/it, running train loss: 0.50462]\u001b[A\n",
            "Trainging:  44%|████▍     | 237/537 [16:25<18:34,  3.72s/it, running train loss: 0.54149]\u001b[A\n",
            "Trainging:  44%|████▍     | 238/537 [16:25<18:34,  3.73s/it, running train loss: 0.54149]\u001b[A\n",
            "Trainging:  44%|████▍     | 238/537 [16:31<18:34,  3.73s/it, running train loss: 0.59232]\u001b[A\n",
            "Trainging:  45%|████▍     | 239/537 [16:31<21:57,  4.42s/it, running train loss: 0.59232]\u001b[A\n",
            "Trainging:  45%|████▍     | 239/537 [16:35<21:57,  4.42s/it, running train loss: 0.48834]\u001b[A\n",
            "Trainging:  45%|████▍     | 240/537 [16:35<21:09,  4.27s/it, running train loss: 0.48834]\u001b[A\n",
            "Trainging:  45%|████▍     | 240/537 [16:39<21:09,  4.27s/it, running train loss: 0.57493]\u001b[A\n",
            "Trainging:  45%|████▍     | 241/537 [16:39<21:22,  4.33s/it, running train loss: 0.57493]\u001b[A\n",
            "Trainging:  45%|████▍     | 241/537 [16:44<21:22,  4.33s/it, running train loss: 0.47372]\u001b[A\n",
            "Trainging:  45%|████▌     | 242/537 [16:44<21:06,  4.29s/it, running train loss: 0.47372]\u001b[A\n",
            "Trainging:  45%|████▌     | 242/537 [16:47<21:06,  4.29s/it, running train loss: 0.57007]\u001b[A\n",
            "Trainging:  45%|████▌     | 243/537 [16:47<19:57,  4.07s/it, running train loss: 0.57007]\u001b[A\n",
            "Trainging:  45%|████▌     | 243/537 [16:51<19:57,  4.07s/it, running train loss: 0.61051]\u001b[A\n",
            "Trainging:  45%|████▌     | 244/537 [16:51<19:05,  3.91s/it, running train loss: 0.61051]\u001b[A\n",
            "Trainging:  45%|████▌     | 244/537 [16:54<19:05,  3.91s/it, running train loss: 0.57537]\u001b[A\n",
            "Trainging:  46%|████▌     | 245/537 [16:54<18:28,  3.80s/it, running train loss: 0.57537]\u001b[A\n",
            "Trainging:  46%|████▌     | 245/537 [16:58<18:28,  3.80s/it, running train loss: 0.58451]\u001b[A\n",
            "Trainging:  46%|████▌     | 246/537 [16:58<17:57,  3.70s/it, running train loss: 0.58451]\u001b[A\n",
            "Trainging:  46%|████▌     | 246/537 [17:02<17:57,  3.70s/it, running train loss: 0.57890]\u001b[A\n",
            "Trainging:  46%|████▌     | 247/537 [17:02<18:04,  3.74s/it, running train loss: 0.57890]\u001b[A\n",
            "Trainging:  46%|████▌     | 247/537 [17:05<18:04,  3.74s/it, running train loss: 0.61720]\u001b[A\n",
            "Trainging:  46%|████▌     | 248/537 [17:05<17:42,  3.68s/it, running train loss: 0.61720]\u001b[A\n",
            "Trainging:  46%|████▌     | 248/537 [17:09<17:42,  3.68s/it, running train loss: 0.50205]\u001b[A\n",
            "Trainging:  46%|████▋     | 249/537 [17:09<17:32,  3.65s/it, running train loss: 0.50205]\u001b[A\n",
            "Trainging:  46%|████▋     | 249/537 [17:12<17:32,  3.65s/it, running train loss: 0.47959]\u001b[A\n",
            "Trainging:  47%|████▋     | 250/537 [17:12<17:14,  3.60s/it, running train loss: 0.47959]\u001b[A\n",
            "Trainging:  47%|████▋     | 250/537 [17:16<17:14,  3.60s/it, running train loss: 0.39495]\u001b[A\n",
            "Trainging:  47%|████▋     | 251/537 [17:16<16:48,  3.53s/it, running train loss: 0.39495]\u001b[A\n",
            "Trainging:  47%|████▋     | 251/537 [17:20<16:48,  3.53s/it, running train loss: 0.63381]\u001b[A\n",
            "Trainging:  47%|████▋     | 252/537 [17:20<17:49,  3.75s/it, running train loss: 0.63381]\u001b[A\n",
            "Trainging:  47%|████▋     | 252/537 [17:23<17:49,  3.75s/it, running train loss: 0.59772]\u001b[A\n",
            "Trainging:  47%|████▋     | 253/537 [17:23<17:35,  3.72s/it, running train loss: 0.59772]\u001b[A\n",
            "Trainging:  47%|████▋     | 253/537 [17:27<17:35,  3.72s/it, running train loss: 0.61240]\u001b[A\n",
            "Trainging:  47%|████▋     | 254/537 [17:27<17:51,  3.79s/it, running train loss: 0.61240]\u001b[A\n",
            "Trainging:  47%|████▋     | 254/537 [17:31<17:51,  3.79s/it, running train loss: 0.51479]\u001b[A\n",
            "Trainging:  47%|████▋     | 255/537 [17:31<17:44,  3.77s/it, running train loss: 0.51479]\u001b[A\n",
            "Trainging:  47%|████▋     | 255/537 [17:35<17:44,  3.77s/it, running train loss: 0.49448]\u001b[A\n",
            "Trainging:  48%|████▊     | 256/537 [17:35<17:53,  3.82s/it, running train loss: 0.49448]\u001b[A\n",
            "Trainging:  48%|████▊     | 256/537 [17:39<17:53,  3.82s/it, running train loss: 0.55463]\u001b[A\n",
            "Trainging:  48%|████▊     | 257/537 [17:39<17:43,  3.80s/it, running train loss: 0.55463]\u001b[A\n",
            "Trainging:  48%|████▊     | 257/537 [17:42<17:43,  3.80s/it, running train loss: 0.49469]\u001b[A\n",
            "Trainging:  48%|████▊     | 258/537 [17:42<17:16,  3.72s/it, running train loss: 0.49469]\u001b[A\n",
            "Trainging:  48%|████▊     | 258/537 [17:47<17:16,  3.72s/it, running train loss: 0.66251]\u001b[A\n",
            "Trainging:  48%|████▊     | 259/537 [17:47<18:27,  3.98s/it, running train loss: 0.66251]\u001b[A\n",
            "Trainging:  48%|████▊     | 259/537 [17:50<18:27,  3.98s/it, running train loss: 0.52035]\u001b[A\n",
            "Trainging:  48%|████▊     | 260/537 [17:50<17:37,  3.82s/it, running train loss: 0.52035]\u001b[A\n",
            "Trainging:  48%|████▊     | 260/537 [17:55<17:37,  3.82s/it, running train loss: 0.50455]\u001b[A\n",
            "Trainging:  49%|████▊     | 261/537 [17:55<18:12,  3.96s/it, running train loss: 0.50455]\u001b[A\n",
            "Trainging:  49%|████▊     | 261/537 [17:59<18:12,  3.96s/it, running train loss: 0.55839]\u001b[A\n",
            "Trainging:  49%|████▉     | 262/537 [17:59<18:01,  3.93s/it, running train loss: 0.55839]\u001b[A\n",
            "Trainging:  49%|████▉     | 262/537 [18:02<18:01,  3.93s/it, running train loss: 0.54262]\u001b[A\n",
            "Trainging:  49%|████▉     | 263/537 [18:02<17:55,  3.92s/it, running train loss: 0.54262]\u001b[A\n",
            "Trainging:  49%|████▉     | 263/537 [18:06<17:55,  3.92s/it, running train loss: 0.52616]\u001b[A\n",
            "Trainging:  49%|████▉     | 264/537 [18:06<17:48,  3.91s/it, running train loss: 0.52616]\u001b[A\n",
            "Trainging:  49%|████▉     | 264/537 [18:10<17:48,  3.91s/it, running train loss: 0.40541]\u001b[A\n",
            "Trainging:  49%|████▉     | 265/537 [18:10<17:01,  3.76s/it, running train loss: 0.40541]\u001b[A\n",
            "Trainging:  49%|████▉     | 265/537 [18:13<17:01,  3.76s/it, running train loss: 0.56578]\u001b[A\n",
            "Trainging:  50%|████▉     | 266/537 [18:13<16:32,  3.66s/it, running train loss: 0.56578]\u001b[A\n",
            "Trainging:  50%|████▉     | 266/537 [18:17<16:32,  3.66s/it, running train loss: 0.70378]\u001b[A\n",
            "Trainging:  50%|████▉     | 267/537 [18:17<16:42,  3.71s/it, running train loss: 0.70378]\u001b[A\n",
            "Trainging:  50%|████▉     | 267/537 [18:20<16:42,  3.71s/it, running train loss: 0.62417]\u001b[A\n",
            "Trainging:  50%|████▉     | 268/537 [18:20<16:19,  3.64s/it, running train loss: 0.62417]\u001b[A\n",
            "Trainging:  50%|████▉     | 268/537 [18:24<16:19,  3.64s/it, running train loss: 0.57052]\u001b[A\n",
            "Trainging:  50%|█████     | 269/537 [18:24<16:03,  3.60s/it, running train loss: 0.57052]\u001b[A\n",
            "Trainging:  50%|█████     | 269/537 [18:28<16:03,  3.60s/it, running train loss: 0.50486]\u001b[A\n",
            "Trainging:  50%|█████     | 270/537 [18:28<17:01,  3.83s/it, running train loss: 0.50486]\u001b[A\n",
            "Trainging:  50%|█████     | 270/537 [18:34<17:01,  3.83s/it, running train loss: 0.53753]\u001b[A\n",
            "Trainging:  50%|█████     | 271/537 [18:34<19:23,  4.38s/it, running train loss: 0.53753]\u001b[A\n",
            "Trainging:  50%|█████     | 271/537 [18:37<19:23,  4.38s/it, running train loss: 0.52964]\u001b[A\n",
            "Trainging:  51%|█████     | 272/537 [18:38<18:12,  4.12s/it, running train loss: 0.52964]\u001b[A\n",
            "Trainging:  51%|█████     | 272/537 [18:41<18:12,  4.12s/it, running train loss: 0.52692]\u001b[A\n",
            "Trainging:  51%|█████     | 273/537 [18:41<17:32,  3.99s/it, running train loss: 0.52692]\u001b[A\n",
            "Trainging:  51%|█████     | 273/537 [18:45<17:32,  3.99s/it, running train loss: 0.53466]\u001b[A\n",
            "Trainging:  51%|█████     | 274/537 [18:45<16:38,  3.80s/it, running train loss: 0.53466]\u001b[A\n",
            "Trainging:  51%|█████     | 274/537 [18:50<16:38,  3.80s/it, running train loss: 0.46618]\u001b[A\n",
            "Trainging:  51%|█████     | 275/537 [18:50<18:08,  4.16s/it, running train loss: 0.46618]\u001b[A\n",
            "Trainging:  51%|█████     | 275/537 [18:53<18:08,  4.16s/it, running train loss: 0.50774]\u001b[A\n",
            "Trainging:  51%|█████▏    | 276/537 [18:53<17:01,  3.91s/it, running train loss: 0.50774]\u001b[A\n",
            "Trainging:  51%|█████▏    | 276/537 [18:57<17:01,  3.91s/it, running train loss: 0.50213]\u001b[A\n",
            "Trainging:  52%|█████▏    | 277/537 [18:57<16:46,  3.87s/it, running train loss: 0.50213]\u001b[A\n",
            "Trainging:  52%|█████▏    | 277/537 [19:01<16:46,  3.87s/it, running train loss: 0.52694]\u001b[A\n",
            "Trainging:  52%|█████▏    | 278/537 [19:01<17:28,  4.05s/it, running train loss: 0.52694]\u001b[A\n",
            "Trainging:  52%|█████▏    | 278/537 [19:05<17:28,  4.05s/it, running train loss: 0.53351]\u001b[A\n",
            "Trainging:  52%|█████▏    | 279/537 [19:05<16:36,  3.86s/it, running train loss: 0.53351]\u001b[A\n",
            "Trainging:  52%|█████▏    | 279/537 [19:08<16:36,  3.86s/it, running train loss: 0.69738]\u001b[A\n",
            "Trainging:  52%|█████▏    | 280/537 [19:08<16:16,  3.80s/it, running train loss: 0.69738]\u001b[A\n",
            "Trainging:  52%|█████▏    | 280/537 [19:12<16:16,  3.80s/it, running train loss: 0.63183]\u001b[A\n",
            "Trainging:  52%|█████▏    | 281/537 [19:12<15:52,  3.72s/it, running train loss: 0.63183]\u001b[A\n",
            "Trainging:  52%|█████▏    | 281/537 [19:15<15:52,  3.72s/it, running train loss: 0.54338]\u001b[A\n",
            "Trainging:  53%|█████▎    | 282/537 [19:15<15:53,  3.74s/it, running train loss: 0.54338]\u001b[A\n",
            "Trainging:  53%|█████▎    | 282/537 [19:19<15:53,  3.74s/it, running train loss: 0.53542]\u001b[A\n",
            "Trainging:  53%|█████▎    | 283/537 [19:19<15:41,  3.71s/it, running train loss: 0.53542]\u001b[A\n",
            "Trainging:  53%|█████▎    | 283/537 [19:23<15:41,  3.71s/it, running train loss: 0.48339]\u001b[A\n",
            "Trainging:  53%|█████▎    | 284/537 [19:23<15:17,  3.63s/it, running train loss: 0.48339]\u001b[A\n",
            "Trainging:  53%|█████▎    | 284/537 [19:27<15:17,  3.63s/it, running train loss: 0.45072]\u001b[A\n",
            "Trainging:  53%|█████▎    | 285/537 [19:27<16:14,  3.87s/it, running train loss: 0.45072]\u001b[A\n",
            "Trainging:  53%|█████▎    | 285/537 [19:31<16:14,  3.87s/it, running train loss: 0.51934]\u001b[A\n",
            "Trainging:  53%|█████▎    | 286/537 [19:31<16:00,  3.83s/it, running train loss: 0.51934]\u001b[A\n",
            "Trainging:  53%|█████▎    | 286/537 [19:34<16:00,  3.83s/it, running train loss: 0.51890]\u001b[A\n",
            "Trainging:  53%|█████▎    | 287/537 [19:35<15:53,  3.81s/it, running train loss: 0.51890]\u001b[A\n",
            "Trainging:  53%|█████▎    | 287/537 [19:38<15:53,  3.81s/it, running train loss: 0.64258]\u001b[A\n",
            "Trainging:  54%|█████▎    | 288/537 [19:38<15:26,  3.72s/it, running train loss: 0.64258]\u001b[A\n",
            "Trainging:  54%|█████▎    | 288/537 [19:42<15:26,  3.72s/it, running train loss: 0.50234]\u001b[A\n",
            "Trainging:  54%|█████▍    | 289/537 [19:42<15:59,  3.87s/it, running train loss: 0.50234]\u001b[A\n",
            "Trainging:  54%|█████▍    | 289/537 [19:46<15:59,  3.87s/it, running train loss: 0.42252]\u001b[A\n",
            "Trainging:  54%|█████▍    | 290/537 [19:46<15:49,  3.84s/it, running train loss: 0.42252]\u001b[A\n",
            "Trainging:  54%|█████▍    | 290/537 [19:51<15:49,  3.84s/it, running train loss: 0.55653]\u001b[A\n",
            "Trainging:  54%|█████▍    | 291/537 [19:51<16:52,  4.11s/it, running train loss: 0.55653]\u001b[A\n",
            "Trainging:  54%|█████▍    | 291/537 [19:55<16:52,  4.11s/it, running train loss: 0.58082]\u001b[A\n",
            "Trainging:  54%|█████▍    | 292/537 [19:55<17:16,  4.23s/it, running train loss: 0.58082]\u001b[A\n",
            "Trainging:  54%|█████▍    | 292/537 [19:59<17:16,  4.23s/it, running train loss: 0.57788]\u001b[A\n",
            "Trainging:  55%|█████▍    | 293/537 [19:59<16:20,  4.02s/it, running train loss: 0.57788]\u001b[A\n",
            "Trainging:  55%|█████▍    | 293/537 [20:03<16:20,  4.02s/it, running train loss: 0.47782]\u001b[A\n",
            "Trainging:  55%|█████▍    | 294/537 [20:03<15:56,  3.94s/it, running train loss: 0.47782]\u001b[A\n",
            "Trainging:  55%|█████▍    | 294/537 [20:05<15:56,  3.94s/it, running train loss: 0.52844]\u001b[A\n",
            "Trainging:  55%|█████▍    | 295/537 [20:05<13:44,  3.41s/it, running train loss: 0.52844]\u001b[A\n",
            "Trainging:  55%|█████▍    | 295/537 [20:08<13:44,  3.41s/it, running train loss: 0.58850]\u001b[A\n",
            "Trainging:  55%|█████▌    | 296/537 [20:08<13:47,  3.43s/it, running train loss: 0.58850]\u001b[A\n",
            "Trainging:  55%|█████▌    | 296/537 [20:13<13:47,  3.43s/it, running train loss: 0.59702]\u001b[A\n",
            "Trainging:  55%|█████▌    | 297/537 [20:13<15:00,  3.75s/it, running train loss: 0.59702]\u001b[A\n",
            "Trainging:  55%|█████▌    | 297/537 [20:16<15:00,  3.75s/it, running train loss: 0.57122]\u001b[A\n",
            "Trainging:  55%|█████▌    | 298/537 [20:16<14:29,  3.64s/it, running train loss: 0.57122]\u001b[A\n",
            "Trainging:  55%|█████▌    | 298/537 [20:21<14:29,  3.64s/it, running train loss: 0.59969]\u001b[A\n",
            "Trainging:  56%|█████▌    | 299/537 [20:21<15:41,  3.95s/it, running train loss: 0.59969]\u001b[A\n",
            "Trainging:  56%|█████▌    | 299/537 [20:25<15:41,  3.95s/it, running train loss: 0.54539]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|▏         | 1/68 [00:00<00:40,  1.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 2/68 [00:01<00:34,  1.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 3/68 [00:01<00:28,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 4/68 [00:01<00:28,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 5/68 [00:02<00:27,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 6/68 [00:02<00:24,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|█         | 7/68 [00:03<00:25,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 8/68 [00:03<00:25,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 9/68 [00:03<00:24,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 10/68 [00:04<00:24,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 11/68 [00:04<00:24,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 12/68 [00:05<00:24,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 13/68 [00:05<00:23,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 14/68 [00:06<00:23,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 15/68 [00:06<00:23,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▎       | 16/68 [00:06<00:22,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▌       | 17/68 [00:07<00:20,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 18/68 [00:07<00:20,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 19/68 [00:08<00:19,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 20/68 [00:08<00:18,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 21/68 [00:08<00:18,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 22/68 [00:09<00:18,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 23/68 [00:09<00:18,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▌      | 24/68 [00:10<00:17,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 25/68 [00:10<00:17,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 26/68 [00:10<00:17,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 27/68 [00:11<00:17,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████      | 28/68 [00:11<00:15,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 29/68 [00:12<00:15,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 30/68 [00:12<00:15,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▌     | 31/68 [00:12<00:14,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 32/68 [00:13<00:14,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▊     | 33/68 [00:13<00:14,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 34/68 [00:14<00:13,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████▏    | 35/68 [00:14<00:12,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 36/68 [00:14<00:12,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▍    | 37/68 [00:15<00:12,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 38/68 [00:15<00:12,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 39/68 [00:16<00:12,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▉    | 40/68 [00:16<00:11,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 41/68 [00:17<00:10,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 42/68 [00:17<00:10,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 43/68 [00:17<00:10,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▍   | 44/68 [00:18<00:10,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 45/68 [00:18<00:09,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 46/68 [00:19<00:09,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 47/68 [00:19<00:09,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 48/68 [00:20<00:08,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 49/68 [00:20<00:08,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 50/68 [00:20<00:07,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 51/68 [00:21<00:07,  2.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▋  | 52/68 [00:21<00:07,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 53/68 [00:22<00:06,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 54/68 [00:22<00:06,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 55/68 [00:23<00:05,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 56/68 [00:23<00:04,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 57/68 [00:23<00:04,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 58/68 [00:24<00:04,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 59/68 [00:24<00:03,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 60/68 [00:25<00:03,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|████████▉ | 61/68 [00:25<00:02,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 62/68 [00:25<00:02,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 63/68 [00:26<00:02,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 64/68 [00:26<00:01,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 65/68 [00:27<00:01,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 66/68 [00:27<00:00,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▊| 67/68 [00:28<00:00,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 68/68 [00:28<00:00,  2.39it/s]\n",
            "\n",
            "Trainging:  56%|█████▌    | 300/537 [20:56<52:12, 13.22s/it, running train loss: 0.54539]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> training loss: 0.544851, valid loss: 0.526998,valid f1: 0.477660, valid acc:0.702039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Trainging:  56%|█████▌    | 300/537 [20:59<52:12, 13.22s/it, running train loss: 0.48187]\u001b[A\n",
            "Trainging:  56%|█████▌    | 301/537 [20:59<40:44, 10.36s/it, running train loss: 0.48187]\u001b[A\n",
            "Trainging:  56%|█████▌    | 301/537 [21:03<40:44, 10.36s/it, running train loss: 0.60991]\u001b[A\n",
            "Trainging:  56%|█████▌    | 302/537 [21:03<32:23,  8.27s/it, running train loss: 0.60991]\u001b[A\n",
            "Trainging:  56%|█████▌    | 302/537 [21:08<32:23,  8.27s/it, running train loss: 0.50224]\u001b[A\n",
            "Trainging:  56%|█████▋    | 303/537 [21:08<28:46,  7.38s/it, running train loss: 0.50224]\u001b[A\n",
            "Trainging:  56%|█████▋    | 303/537 [21:11<28:46,  7.38s/it, running train loss: 0.47104]\u001b[A\n",
            "Trainging:  57%|█████▋    | 304/537 [21:11<23:57,  6.17s/it, running train loss: 0.47104]\u001b[A\n",
            "Trainging:  57%|█████▋    | 304/537 [21:15<23:57,  6.17s/it, running train loss: 0.46893]\u001b[A\n",
            "Trainging:  57%|█████▋    | 305/537 [21:15<20:28,  5.29s/it, running train loss: 0.46893]\u001b[A\n",
            "Trainging:  57%|█████▋    | 305/537 [21:18<20:28,  5.29s/it, running train loss: 0.62619]\u001b[A\n",
            "Trainging:  57%|█████▋    | 306/537 [21:18<18:17,  4.75s/it, running train loss: 0.62619]\u001b[A\n",
            "Trainging:  57%|█████▋    | 306/537 [21:21<18:17,  4.75s/it, running train loss: 0.58073]\u001b[A\n",
            "Trainging:  57%|█████▋    | 307/537 [21:21<16:32,  4.31s/it, running train loss: 0.58073]\u001b[A\n",
            "Trainging:  57%|█████▋    | 307/537 [21:25<16:32,  4.31s/it, running train loss: 0.45767]\u001b[A\n",
            "Trainging:  57%|█████▋    | 308/537 [21:25<15:16,  4.00s/it, running train loss: 0.45767]\u001b[A\n",
            "Trainging:  57%|█████▋    | 308/537 [21:28<15:16,  4.00s/it, running train loss: 0.49011]\u001b[A\n",
            "Trainging:  58%|█████▊    | 309/537 [21:28<14:23,  3.79s/it, running train loss: 0.49011]\u001b[A\n",
            "Trainging:  58%|█████▊    | 309/537 [21:31<14:23,  3.79s/it, running train loss: 0.55893]\u001b[A\n",
            "Trainging:  58%|█████▊    | 310/537 [21:31<13:55,  3.68s/it, running train loss: 0.55893]\u001b[A\n",
            "Trainging:  58%|█████▊    | 310/537 [21:35<13:55,  3.68s/it, running train loss: 0.42425]\u001b[A\n",
            "Trainging:  58%|█████▊    | 311/537 [21:35<13:25,  3.56s/it, running train loss: 0.42425]\u001b[A\n",
            "Trainging:  58%|█████▊    | 311/537 [21:38<13:25,  3.56s/it, running train loss: 0.46615]\u001b[A\n",
            "Trainging:  58%|█████▊    | 312/537 [21:38<13:03,  3.48s/it, running train loss: 0.46615]\u001b[A\n",
            "Trainging:  58%|█████▊    | 312/537 [21:42<13:03,  3.48s/it, running train loss: 0.44684]\u001b[A\n",
            "Trainging:  58%|█████▊    | 313/537 [21:42<13:31,  3.62s/it, running train loss: 0.44684]\u001b[A\n",
            "Trainging:  58%|█████▊    | 313/537 [21:45<13:31,  3.62s/it, running train loss: 0.53440]\u001b[A\n",
            "Trainging:  58%|█████▊    | 314/537 [21:45<13:11,  3.55s/it, running train loss: 0.53440]\u001b[A\n",
            "Trainging:  58%|█████▊    | 314/537 [21:50<13:11,  3.55s/it, running train loss: 0.54245]\u001b[A\n",
            "Trainging:  59%|█████▊    | 315/537 [21:50<14:29,  3.92s/it, running train loss: 0.54245]\u001b[A\n",
            "Trainging:  59%|█████▊    | 315/537 [21:54<14:29,  3.92s/it, running train loss: 0.56011]\u001b[A\n",
            "Trainging:  59%|█████▉    | 316/537 [21:54<13:57,  3.79s/it, running train loss: 0.56011]\u001b[A\n",
            "Trainging:  59%|█████▉    | 316/537 [21:57<13:57,  3.79s/it, running train loss: 0.55684]\u001b[A\n",
            "Trainging:  59%|█████▉    | 317/537 [21:57<14:03,  3.84s/it, running train loss: 0.55684]\u001b[A\n",
            "Trainging:  59%|█████▉    | 317/537 [22:03<14:03,  3.84s/it, running train loss: 0.51263]\u001b[A\n",
            "Trainging:  59%|█████▉    | 318/537 [22:03<15:51,  4.35s/it, running train loss: 0.51263]\u001b[A\n",
            "Trainging:  59%|█████▉    | 318/537 [22:07<15:51,  4.35s/it, running train loss: 0.50927]\u001b[A\n",
            "Trainging:  59%|█████▉    | 319/537 [22:07<15:20,  4.22s/it, running train loss: 0.50927]\u001b[A\n",
            "Trainging:  59%|█████▉    | 319/537 [22:11<15:20,  4.22s/it, running train loss: 0.47393]\u001b[A\n",
            "Trainging:  60%|█████▉    | 320/537 [22:11<14:48,  4.09s/it, running train loss: 0.47393]\u001b[A\n",
            "Trainging:  60%|█████▉    | 320/537 [22:15<14:48,  4.09s/it, running train loss: 0.45906]\u001b[A\n",
            "Trainging:  60%|█████▉    | 321/537 [22:15<14:32,  4.04s/it, running train loss: 0.45906]\u001b[A\n",
            "Trainging:  60%|█████▉    | 321/537 [22:18<14:32,  4.04s/it, running train loss: 0.46185]\u001b[A\n",
            "Trainging:  60%|█████▉    | 322/537 [22:18<13:41,  3.82s/it, running train loss: 0.46185]\u001b[A\n",
            "Trainging:  60%|█████▉    | 322/537 [22:21<13:41,  3.82s/it, running train loss: 0.46044]\u001b[A\n",
            "Trainging:  60%|██████    | 323/537 [22:21<13:11,  3.70s/it, running train loss: 0.46044]\u001b[A\n",
            "Trainging:  60%|██████    | 323/537 [22:25<13:11,  3.70s/it, running train loss: 0.60975]\u001b[A\n",
            "Trainging:  60%|██████    | 324/537 [22:25<12:56,  3.64s/it, running train loss: 0.60975]\u001b[A\n",
            "Trainging:  60%|██████    | 324/537 [22:29<12:56,  3.64s/it, running train loss: 0.40798]\u001b[A\n",
            "Trainging:  61%|██████    | 325/537 [22:29<13:04,  3.70s/it, running train loss: 0.40798]\u001b[A\n",
            "Trainging:  61%|██████    | 325/537 [22:33<13:04,  3.70s/it, running train loss: 0.49321]\u001b[A\n",
            "Trainging:  61%|██████    | 326/537 [22:33<13:11,  3.75s/it, running train loss: 0.49321]\u001b[A\n",
            "Trainging:  61%|██████    | 326/537 [22:36<13:11,  3.75s/it, running train loss: 0.64643]\u001b[A\n",
            "Trainging:  61%|██████    | 327/537 [22:36<12:57,  3.70s/it, running train loss: 0.64643]\u001b[A\n",
            "Trainging:  61%|██████    | 327/537 [22:40<12:57,  3.70s/it, running train loss: 0.68584]\u001b[A\n",
            "Trainging:  61%|██████    | 328/537 [22:40<12:52,  3.70s/it, running train loss: 0.68584]\u001b[A\n",
            "Trainging:  61%|██████    | 328/537 [22:43<12:52,  3.70s/it, running train loss: 0.45190]\u001b[A\n",
            "Trainging:  61%|██████▏   | 329/537 [22:43<12:29,  3.60s/it, running train loss: 0.45190]\u001b[A\n",
            "Trainging:  61%|██████▏   | 329/537 [22:47<12:29,  3.60s/it, running train loss: 0.57940]\u001b[A\n",
            "Trainging:  61%|██████▏   | 330/537 [22:47<12:25,  3.60s/it, running train loss: 0.57940]\u001b[A\n",
            "Trainging:  61%|██████▏   | 330/537 [22:51<12:25,  3.60s/it, running train loss: 0.45707]\u001b[A\n",
            "Trainging:  62%|██████▏   | 331/537 [22:51<13:23,  3.90s/it, running train loss: 0.45707]\u001b[A\n",
            "Trainging:  62%|██████▏   | 331/537 [22:55<13:23,  3.90s/it, running train loss: 0.47419]\u001b[A\n",
            "Trainging:  62%|██████▏   | 332/537 [22:55<12:45,  3.73s/it, running train loss: 0.47419]\u001b[A\n",
            "Trainging:  62%|██████▏   | 332/537 [23:00<12:45,  3.73s/it, running train loss: 0.52421]\u001b[A\n",
            "Trainging:  62%|██████▏   | 333/537 [23:00<14:21,  4.22s/it, running train loss: 0.52421]\u001b[A\n",
            "Trainging:  62%|██████▏   | 333/537 [23:04<14:21,  4.22s/it, running train loss: 0.49004]\u001b[A\n",
            "Trainging:  62%|██████▏   | 334/537 [23:04<13:34,  4.01s/it, running train loss: 0.49004]\u001b[A\n",
            "Trainging:  62%|██████▏   | 334/537 [23:07<13:34,  4.01s/it, running train loss: 0.52279]\u001b[A\n",
            "Trainging:  62%|██████▏   | 335/537 [23:07<13:14,  3.93s/it, running train loss: 0.52279]\u001b[A\n",
            "Trainging:  62%|██████▏   | 335/537 [23:11<13:14,  3.93s/it, running train loss: 0.53851]\u001b[A\n",
            "Trainging:  63%|██████▎   | 336/537 [23:11<12:50,  3.83s/it, running train loss: 0.53851]\u001b[A\n",
            "Trainging:  63%|██████▎   | 336/537 [23:14<12:50,  3.83s/it, running train loss: 0.50238]\u001b[A\n",
            "Trainging:  63%|██████▎   | 337/537 [23:14<12:20,  3.70s/it, running train loss: 0.50238]\u001b[A\n",
            "Trainging:  63%|██████▎   | 337/537 [23:18<12:20,  3.70s/it, running train loss: 0.51155]\u001b[A\n",
            "Trainging:  63%|██████▎   | 338/537 [23:18<12:28,  3.76s/it, running train loss: 0.51155]\u001b[A\n",
            "Trainging:  63%|██████▎   | 338/537 [23:23<12:28,  3.76s/it, running train loss: 0.62787]\u001b[A\n",
            "Trainging:  63%|██████▎   | 339/537 [23:23<13:00,  3.94s/it, running train loss: 0.62787]\u001b[A\n",
            "Trainging:  63%|██████▎   | 339/537 [23:27<13:00,  3.94s/it, running train loss: 0.50516]\u001b[A\n",
            "Trainging:  63%|██████▎   | 340/537 [23:27<13:43,  4.18s/it, running train loss: 0.50516]\u001b[A\n",
            "Trainging:  63%|██████▎   | 340/537 [23:31<13:43,  4.18s/it, running train loss: 0.48801]\u001b[A\n",
            "Trainging:  64%|██████▎   | 341/537 [23:31<12:53,  3.94s/it, running train loss: 0.48801]\u001b[A\n",
            "Trainging:  64%|██████▎   | 341/537 [23:35<12:53,  3.94s/it, running train loss: 0.54270]\u001b[A\n",
            "Trainging:  64%|██████▎   | 342/537 [23:35<12:36,  3.88s/it, running train loss: 0.54270]\u001b[A\n",
            "Trainging:  64%|██████▎   | 342/537 [23:38<12:36,  3.88s/it, running train loss: 0.48053]\u001b[A\n",
            "Trainging:  64%|██████▍   | 343/537 [23:38<12:03,  3.73s/it, running train loss: 0.48053]\u001b[A\n",
            "Trainging:  64%|██████▍   | 343/537 [23:42<12:03,  3.73s/it, running train loss: 0.66658]\u001b[A\n",
            "Trainging:  64%|██████▍   | 344/537 [23:42<12:36,  3.92s/it, running train loss: 0.66658]\u001b[A\n",
            "Trainging:  64%|██████▍   | 344/537 [23:46<12:36,  3.92s/it, running train loss: 0.51611]\u001b[A\n",
            "Trainging:  64%|██████▍   | 345/537 [23:46<12:08,  3.79s/it, running train loss: 0.51611]\u001b[A\n",
            "Trainging:  64%|██████▍   | 345/537 [23:49<12:08,  3.79s/it, running train loss: 0.48639]\u001b[A\n",
            "Trainging:  64%|██████▍   | 346/537 [23:49<11:38,  3.66s/it, running train loss: 0.48639]\u001b[A\n",
            "Trainging:  64%|██████▍   | 346/537 [23:54<11:38,  3.66s/it, running train loss: 0.51958]\u001b[A\n",
            "Trainging:  65%|██████▍   | 347/537 [23:54<12:46,  4.03s/it, running train loss: 0.51958]\u001b[A\n",
            "Trainging:  65%|██████▍   | 347/537 [23:58<12:46,  4.03s/it, running train loss: 0.48634]\u001b[A\n",
            "Trainging:  65%|██████▍   | 348/537 [23:58<12:22,  3.93s/it, running train loss: 0.48634]\u001b[A\n",
            "Trainging:  65%|██████▍   | 348/537 [24:01<12:22,  3.93s/it, running train loss: 0.46209]\u001b[A\n",
            "Trainging:  65%|██████▍   | 349/537 [24:01<11:56,  3.81s/it, running train loss: 0.46209]\u001b[A\n",
            "Trainging:  65%|██████▍   | 349/537 [24:05<11:56,  3.81s/it, running train loss: 0.52199]\u001b[A\n",
            "Trainging:  65%|██████▌   | 350/537 [24:05<11:43,  3.76s/it, running train loss: 0.52199]\u001b[A\n",
            "Trainging:  65%|██████▌   | 350/537 [24:09<11:43,  3.76s/it, running train loss: 0.45902]\u001b[A\n",
            "Trainging:  65%|██████▌   | 351/537 [24:09<11:40,  3.77s/it, running train loss: 0.45902]\u001b[A\n",
            "Trainging:  65%|██████▌   | 351/537 [24:12<11:40,  3.77s/it, running train loss: 0.59624]\u001b[A\n",
            "Trainging:  66%|██████▌   | 352/537 [24:12<11:30,  3.73s/it, running train loss: 0.59624]\u001b[A\n",
            "Trainging:  66%|██████▌   | 352/537 [24:16<11:30,  3.73s/it, running train loss: 0.67702]\u001b[A\n",
            "Trainging:  66%|██████▌   | 353/537 [24:16<11:20,  3.70s/it, running train loss: 0.67702]\u001b[A\n",
            "Trainging:  66%|██████▌   | 353/537 [24:20<11:20,  3.70s/it, running train loss: 0.47254]\u001b[A\n",
            "Trainging:  66%|██████▌   | 354/537 [24:20<11:21,  3.72s/it, running train loss: 0.47254]\u001b[A\n",
            "Trainging:  66%|██████▌   | 354/537 [24:24<11:21,  3.72s/it, running train loss: 0.49755]\u001b[A\n",
            "Trainging:  66%|██████▌   | 355/537 [24:24<11:26,  3.77s/it, running train loss: 0.49755]\u001b[A\n",
            "Trainging:  66%|██████▌   | 355/537 [24:27<11:26,  3.77s/it, running train loss: 0.55760]\u001b[A\n",
            "Trainging:  66%|██████▋   | 356/537 [24:27<11:10,  3.70s/it, running train loss: 0.55760]\u001b[A\n",
            "Trainging:  66%|██████▋   | 356/537 [24:31<11:10,  3.70s/it, running train loss: 0.42466]\u001b[A\n",
            "Trainging:  66%|██████▋   | 357/537 [24:31<11:18,  3.77s/it, running train loss: 0.42466]\u001b[A\n",
            "Trainging:  66%|██████▋   | 357/537 [24:37<11:18,  3.77s/it, running train loss: 0.62339]\u001b[A\n",
            "Trainging:  67%|██████▋   | 358/537 [24:37<12:45,  4.28s/it, running train loss: 0.62339]\u001b[A\n",
            "Trainging:  67%|██████▋   | 358/537 [24:41<12:45,  4.28s/it, running train loss: 0.54052]\u001b[A\n",
            "Trainging:  67%|██████▋   | 359/537 [24:41<13:08,  4.43s/it, running train loss: 0.54052]\u001b[A\n",
            "Trainging:  67%|██████▋   | 359/537 [24:46<13:08,  4.43s/it, running train loss: 0.48605]\u001b[A\n",
            "Trainging:  67%|██████▋   | 360/537 [24:46<13:32,  4.59s/it, running train loss: 0.48605]\u001b[A\n",
            "Trainging:  67%|██████▋   | 360/537 [24:50<13:32,  4.59s/it, running train loss: 0.66082]\u001b[A\n",
            "Trainging:  67%|██████▋   | 361/537 [24:50<13:07,  4.48s/it, running train loss: 0.66082]\u001b[A\n",
            "Trainging:  67%|██████▋   | 361/537 [24:55<13:07,  4.48s/it, running train loss: 0.64342]\u001b[A\n",
            "Trainging:  67%|██████▋   | 362/537 [24:55<13:23,  4.59s/it, running train loss: 0.64342]\u001b[A\n",
            "Trainging:  67%|██████▋   | 362/537 [24:59<13:23,  4.59s/it, running train loss: 0.51576]\u001b[A\n",
            "Trainging:  68%|██████▊   | 363/537 [24:59<12:15,  4.22s/it, running train loss: 0.51576]\u001b[A\n",
            "Trainging:  68%|██████▊   | 363/537 [25:03<12:15,  4.22s/it, running train loss: 0.51375]\u001b[A\n",
            "Trainging:  68%|██████▊   | 364/537 [25:03<11:54,  4.13s/it, running train loss: 0.51375]\u001b[A\n",
            "Trainging:  68%|██████▊   | 364/537 [25:06<11:54,  4.13s/it, running train loss: 0.51344]\u001b[A\n",
            "Trainging:  68%|██████▊   | 365/537 [25:06<11:29,  4.01s/it, running train loss: 0.51344]\u001b[A\n",
            "Trainging:  68%|██████▊   | 365/537 [25:11<11:29,  4.01s/it, running train loss: 0.62004]\u001b[A\n",
            "Trainging:  68%|██████▊   | 366/537 [25:11<11:46,  4.13s/it, running train loss: 0.62004]\u001b[A\n",
            "Trainging:  68%|██████▊   | 366/537 [25:17<11:46,  4.13s/it, running train loss: 0.58437]\u001b[A\n",
            "Trainging:  68%|██████▊   | 367/537 [25:17<13:13,  4.67s/it, running train loss: 0.58437]\u001b[A\n",
            "Trainging:  68%|██████▊   | 367/537 [25:22<13:13,  4.67s/it, running train loss: 0.53779]\u001b[A\n",
            "Trainging:  69%|██████▊   | 368/537 [25:22<13:27,  4.78s/it, running train loss: 0.53779]\u001b[A\n",
            "Trainging:  69%|██████▊   | 368/537 [25:26<13:27,  4.78s/it, running train loss: 0.53893]\u001b[A\n",
            "Trainging:  69%|██████▊   | 369/537 [25:26<12:54,  4.61s/it, running train loss: 0.53893]\u001b[A\n",
            "Trainging:  69%|██████▊   | 369/537 [25:29<12:54,  4.61s/it, running train loss: 0.55529]\u001b[A\n",
            "Trainging:  69%|██████▉   | 370/537 [25:29<11:50,  4.26s/it, running train loss: 0.55529]\u001b[A\n",
            "Trainging:  69%|██████▉   | 370/537 [25:33<11:50,  4.26s/it, running train loss: 0.55688]\u001b[A\n",
            "Trainging:  69%|██████▉   | 371/537 [25:33<11:16,  4.07s/it, running train loss: 0.55688]\u001b[A\n",
            "Trainging:  69%|██████▉   | 371/537 [25:39<11:16,  4.07s/it, running train loss: 0.58825]\u001b[A\n",
            "Trainging:  69%|██████▉   | 372/537 [25:39<12:35,  4.58s/it, running train loss: 0.58825]\u001b[A\n",
            "Trainging:  69%|██████▉   | 372/537 [25:45<12:35,  4.58s/it, running train loss: 0.56312]\u001b[A\n",
            "Trainging:  69%|██████▉   | 373/537 [25:45<13:43,  5.02s/it, running train loss: 0.56312]\u001b[A\n",
            "Trainging:  69%|██████▉   | 373/537 [25:49<13:43,  5.02s/it, running train loss: 0.59686]\u001b[A\n",
            "Trainging:  70%|██████▉   | 374/537 [25:49<13:12,  4.86s/it, running train loss: 0.59686]\u001b[A\n",
            "Trainging:  70%|██████▉   | 374/537 [25:53<13:12,  4.86s/it, running train loss: 0.64070]\u001b[A\n",
            "Trainging:  70%|██████▉   | 375/537 [25:53<12:19,  4.57s/it, running train loss: 0.64070]\u001b[A\n",
            "Trainging:  70%|██████▉   | 375/537 [25:59<12:19,  4.57s/it, running train loss: 0.88859]\u001b[A\n",
            "Trainging:  70%|███████   | 376/537 [25:59<13:25,  5.00s/it, running train loss: 0.88859]\u001b[A\n",
            "Trainging:  70%|███████   | 376/537 [26:03<13:25,  5.00s/it, running train loss: 0.51293]\u001b[A\n",
            "Trainging:  70%|███████   | 377/537 [26:03<12:26,  4.66s/it, running train loss: 0.51293]\u001b[A\n",
            "Trainging:  70%|███████   | 377/537 [26:06<12:26,  4.66s/it, running train loss: 0.48448]\u001b[A\n",
            "Trainging:  70%|███████   | 378/537 [26:06<11:19,  4.27s/it, running train loss: 0.48448]\u001b[A\n",
            "Trainging:  70%|███████   | 378/537 [26:09<11:19,  4.27s/it, running train loss: 0.55024]\u001b[A\n",
            "Trainging:  71%|███████   | 379/537 [26:09<09:35,  3.64s/it, running train loss: 0.55024]\u001b[A\n",
            "Trainging:  71%|███████   | 379/537 [26:13<09:35,  3.64s/it, running train loss: 0.53080]\u001b[A\n",
            "Trainging:  71%|███████   | 380/537 [26:13<10:15,  3.92s/it, running train loss: 0.53080]\u001b[A\n",
            "Trainging:  71%|███████   | 380/537 [26:17<10:15,  3.92s/it, running train loss: 0.47143]\u001b[A\n",
            "Trainging:  71%|███████   | 381/537 [26:17<09:56,  3.82s/it, running train loss: 0.47143]\u001b[A\n",
            "Trainging:  71%|███████   | 381/537 [26:21<09:56,  3.82s/it, running train loss: 0.54684]\u001b[A\n",
            "Trainging:  71%|███████   | 382/537 [26:21<09:57,  3.86s/it, running train loss: 0.54684]\u001b[A\n",
            "Trainging:  71%|███████   | 382/537 [26:24<09:57,  3.86s/it, running train loss: 0.46600]\u001b[A\n",
            "Trainging:  71%|███████▏  | 383/537 [26:24<09:29,  3.70s/it, running train loss: 0.46600]\u001b[A\n",
            "Trainging:  71%|███████▏  | 383/537 [26:28<09:29,  3.70s/it, running train loss: 0.46600]\u001b[A\n",
            "Trainging:  72%|███████▏  | 384/537 [26:28<09:23,  3.68s/it, running train loss: 0.46600]\u001b[A\n",
            "Trainging:  72%|███████▏  | 384/537 [26:33<09:23,  3.68s/it, running train loss: 0.60269]\u001b[A\n",
            "Trainging:  72%|███████▏  | 385/537 [26:33<10:20,  4.08s/it, running train loss: 0.60269]\u001b[A\n",
            "Trainging:  72%|███████▏  | 385/537 [26:37<10:20,  4.08s/it, running train loss: 0.49637]\u001b[A\n",
            "Trainging:  72%|███████▏  | 386/537 [26:37<10:09,  4.03s/it, running train loss: 0.49637]\u001b[A\n",
            "Trainging:  72%|███████▏  | 386/537 [26:41<10:09,  4.03s/it, running train loss: 0.49989]\u001b[A\n",
            "Trainging:  72%|███████▏  | 387/537 [26:41<10:12,  4.08s/it, running train loss: 0.49989]\u001b[A\n",
            "Trainging:  72%|███████▏  | 387/537 [26:44<10:12,  4.08s/it, running train loss: 0.54615]\u001b[A\n",
            "Trainging:  72%|███████▏  | 388/537 [26:44<09:48,  3.95s/it, running train loss: 0.54615]\u001b[A\n",
            "Trainging:  72%|███████▏  | 388/537 [26:48<09:48,  3.95s/it, running train loss: 0.53030]\u001b[A\n",
            "Trainging:  72%|███████▏  | 389/537 [26:48<09:39,  3.91s/it, running train loss: 0.53030]\u001b[A\n",
            "Trainging:  72%|███████▏  | 389/537 [26:52<09:39,  3.91s/it, running train loss: 0.43990]\u001b[A\n",
            "Trainging:  73%|███████▎  | 390/537 [26:52<09:18,  3.80s/it, running train loss: 0.43990]\u001b[A\n",
            "Trainging:  73%|███████▎  | 390/537 [26:56<09:18,  3.80s/it, running train loss: 0.56278]\u001b[A\n",
            "Trainging:  73%|███████▎  | 391/537 [26:56<09:21,  3.84s/it, running train loss: 0.56278]\u001b[A\n",
            "Trainging:  73%|███████▎  | 391/537 [26:59<09:21,  3.84s/it, running train loss: 0.63965]\u001b[A\n",
            "Trainging:  73%|███████▎  | 392/537 [26:59<08:59,  3.72s/it, running train loss: 0.63965]\u001b[A\n",
            "Trainging:  73%|███████▎  | 392/537 [27:01<08:59,  3.72s/it, running train loss: 0.57593]\u001b[A\n",
            "Trainging:  73%|███████▎  | 393/537 [27:01<07:48,  3.25s/it, running train loss: 0.57593]\u001b[A\n",
            "Trainging:  73%|███████▎  | 393/537 [27:05<07:48,  3.25s/it, running train loss: 0.43215]\u001b[A\n",
            "Trainging:  73%|███████▎  | 394/537 [27:05<08:05,  3.40s/it, running train loss: 0.43215]\u001b[A\n",
            "Trainging:  73%|███████▎  | 394/537 [27:09<08:05,  3.40s/it, running train loss: 0.56422]\u001b[A\n",
            "Trainging:  74%|███████▎  | 395/537 [27:09<08:08,  3.44s/it, running train loss: 0.56422]\u001b[A\n",
            "Trainging:  74%|███████▎  | 395/537 [27:14<08:08,  3.44s/it, running train loss: 0.52159]\u001b[A\n",
            "Trainging:  74%|███████▎  | 396/537 [27:14<09:07,  3.88s/it, running train loss: 0.52159]\u001b[A\n",
            "Trainging:  74%|███████▎  | 396/537 [27:18<09:07,  3.88s/it, running train loss: 0.55451]\u001b[A\n",
            "Trainging:  74%|███████▍  | 397/537 [27:18<09:28,  4.06s/it, running train loss: 0.55451]\u001b[A\n",
            "Trainging:  74%|███████▍  | 397/537 [27:22<09:28,  4.06s/it, running train loss: 0.56884]\u001b[A\n",
            "Trainging:  74%|███████▍  | 398/537 [27:22<09:34,  4.13s/it, running train loss: 0.56884]\u001b[A\n",
            "Trainging:  74%|███████▍  | 398/537 [27:27<09:34,  4.13s/it, running train loss: 0.61560]\u001b[A\n",
            "Trainging:  74%|███████▍  | 399/537 [27:27<09:44,  4.23s/it, running train loss: 0.61560]\u001b[A\n",
            "Trainging:  74%|███████▍  | 399/537 [27:30<09:44,  4.23s/it, running train loss: 0.48899]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|▏         | 1/68 [00:00<00:42,  1.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 2/68 [00:01<00:35,  1.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 3/68 [00:01<00:29,  2.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 4/68 [00:01<00:28,  2.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 5/68 [00:02<00:27,  2.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 6/68 [00:02<00:25,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|█         | 7/68 [00:03<00:25,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 8/68 [00:03<00:25,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 9/68 [00:03<00:24,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 10/68 [00:04<00:24,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 11/68 [00:04<00:24,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 12/68 [00:05<00:23,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 13/68 [00:05<00:23,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 14/68 [00:06<00:23,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 15/68 [00:06<00:23,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▎       | 16/68 [00:06<00:22,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▌       | 17/68 [00:07<00:20,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 18/68 [00:07<00:20,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 19/68 [00:08<00:19,  2.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 20/68 [00:08<00:18,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 21/68 [00:08<00:19,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 22/68 [00:09<00:18,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 23/68 [00:09<00:18,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▌      | 24/68 [00:10<00:17,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 25/68 [00:10<00:17,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 26/68 [00:10<00:17,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 27/68 [00:11<00:17,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████      | 28/68 [00:11<00:16,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 29/68 [00:12<00:15,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 30/68 [00:12<00:15,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▌     | 31/68 [00:12<00:14,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 32/68 [00:13<00:14,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▊     | 33/68 [00:13<00:14,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 34/68 [00:14<00:13,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████▏    | 35/68 [00:14<00:12,  2.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 36/68 [00:14<00:13,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▍    | 37/68 [00:15<00:12,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 38/68 [00:15<00:12,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 39/68 [00:16<00:12,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▉    | 40/68 [00:16<00:11,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 41/68 [00:17<00:10,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 42/68 [00:17<00:10,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 43/68 [00:17<00:10,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▍   | 44/68 [00:18<00:10,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 45/68 [00:18<00:09,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 46/68 [00:19<00:09,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 47/68 [00:19<00:09,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 48/68 [00:20<00:08,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 49/68 [00:20<00:08,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 50/68 [00:21<00:07,  2.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 51/68 [00:21<00:07,  2.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▋  | 52/68 [00:21<00:07,  2.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 53/68 [00:22<00:06,  2.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 54/68 [00:22<00:06,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 55/68 [00:23<00:05,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 56/68 [00:23<00:04,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 57/68 [00:23<00:04,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 58/68 [00:24<00:04,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 59/68 [00:24<00:03,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 60/68 [00:25<00:03,  2.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|████████▉ | 61/68 [00:25<00:02,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 62/68 [00:26<00:02,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 63/68 [00:26<00:02,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 64/68 [00:26<00:01,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 65/68 [00:27<00:01,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 66/68 [00:27<00:00,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▊| 67/68 [00:28<00:00,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 68/68 [00:28<00:00,  2.39it/s]\n",
            "\n",
            "Trainging:  74%|███████▍  | 400/537 [28:01<29:54, 13.10s/it, running train loss: 0.48899]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> training loss: 0.533753, valid loss: 0.513328,valid f1: 0.458169, valid acc:0.713392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Trainging:  74%|███████▍  | 400/537 [28:04<29:54, 13.10s/it, running train loss: 0.53525]\u001b[A\n",
            "Trainging:  75%|███████▍  | 401/537 [28:04<23:10, 10.23s/it, running train loss: 0.53525]\u001b[A\n",
            "Trainging:  75%|███████▍  | 401/537 [28:08<23:10, 10.23s/it, running train loss: 0.52939]\u001b[A\n",
            "Trainging:  75%|███████▍  | 402/537 [28:08<18:24,  8.18s/it, running train loss: 0.52939]\u001b[A\n",
            "Trainging:  75%|███████▍  | 402/537 [28:11<18:24,  8.18s/it, running train loss: 0.53405]\u001b[A\n",
            "Trainging:  75%|███████▌  | 403/537 [28:11<15:22,  6.88s/it, running train loss: 0.53405]\u001b[A\n",
            "Trainging:  75%|███████▌  | 403/537 [28:15<15:22,  6.88s/it, running train loss: 0.46558]\u001b[A\n",
            "Trainging:  75%|███████▌  | 404/537 [28:15<12:58,  5.85s/it, running train loss: 0.46558]\u001b[A\n",
            "Trainging:  75%|███████▌  | 404/537 [28:18<12:58,  5.85s/it, running train loss: 0.52522]\u001b[A\n",
            "Trainging:  75%|███████▌  | 405/537 [28:18<11:11,  5.09s/it, running train loss: 0.52522]\u001b[A\n",
            "Trainging:  75%|███████▌  | 405/537 [28:22<11:11,  5.09s/it, running train loss: 0.55810]\u001b[A\n",
            "Trainging:  76%|███████▌  | 406/537 [28:22<10:18,  4.72s/it, running train loss: 0.55810]\u001b[A\n",
            "Trainging:  76%|███████▌  | 406/537 [28:25<10:18,  4.72s/it, running train loss: 0.57497]\u001b[A\n",
            "Trainging:  76%|███████▌  | 407/537 [28:25<09:20,  4.31s/it, running train loss: 0.57497]\u001b[A\n",
            "Trainging:  76%|███████▌  | 407/537 [28:30<09:20,  4.31s/it, running train loss: 0.57006]\u001b[A\n",
            "Trainging:  76%|███████▌  | 408/537 [28:30<09:27,  4.40s/it, running train loss: 0.57006]\u001b[A\n",
            "Trainging:  76%|███████▌  | 408/537 [28:35<09:27,  4.40s/it, running train loss: 0.47309]\u001b[A\n",
            "Trainging:  76%|███████▌  | 409/537 [28:35<09:36,  4.50s/it, running train loss: 0.47309]\u001b[A\n",
            "Trainging:  76%|███████▌  | 409/537 [28:38<09:36,  4.50s/it, running train loss: 0.42120]\u001b[A\n",
            "Trainging:  76%|███████▋  | 410/537 [28:38<09:02,  4.27s/it, running train loss: 0.42120]\u001b[A\n",
            "Trainging:  76%|███████▋  | 410/537 [28:42<09:02,  4.27s/it, running train loss: 0.51446]\u001b[A\n",
            "Trainging:  77%|███████▋  | 411/537 [28:42<08:30,  4.05s/it, running train loss: 0.51446]\u001b[A\n",
            "Trainging:  77%|███████▋  | 411/537 [28:46<08:30,  4.05s/it, running train loss: 0.55093]\u001b[A\n",
            "Trainging:  77%|███████▋  | 412/537 [28:46<08:09,  3.92s/it, running train loss: 0.55093]\u001b[A\n",
            "Trainging:  77%|███████▋  | 412/537 [28:49<08:09,  3.92s/it, running train loss: 0.51386]\u001b[A\n",
            "Trainging:  77%|███████▋  | 413/537 [28:49<08:02,  3.89s/it, running train loss: 0.51386]\u001b[A\n",
            "Trainging:  77%|███████▋  | 413/537 [28:54<08:02,  3.89s/it, running train loss: 0.55468]\u001b[A\n",
            "Trainging:  77%|███████▋  | 414/537 [28:54<08:27,  4.13s/it, running train loss: 0.55468]\u001b[A\n",
            "Trainging:  77%|███████▋  | 414/537 [28:57<08:27,  4.13s/it, running train loss: 0.50004]\u001b[A\n",
            "Trainging:  77%|███████▋  | 415/537 [28:57<07:53,  3.88s/it, running train loss: 0.50004]\u001b[A\n",
            "Trainging:  77%|███████▋  | 415/537 [29:03<07:53,  3.88s/it, running train loss: 0.51449]\u001b[A\n",
            "Trainging:  77%|███████▋  | 416/537 [29:03<08:53,  4.41s/it, running train loss: 0.51449]\u001b[A\n",
            "Trainging:  77%|███████▋  | 416/537 [29:07<08:53,  4.41s/it, running train loss: 0.65311]\u001b[A\n",
            "Trainging:  78%|███████▊  | 417/537 [29:07<08:30,  4.25s/it, running train loss: 0.65311]\u001b[A\n",
            "Trainging:  78%|███████▊  | 417/537 [29:11<08:30,  4.25s/it, running train loss: 0.49677]\u001b[A\n",
            "Trainging:  78%|███████▊  | 418/537 [29:11<08:11,  4.13s/it, running train loss: 0.49677]\u001b[A\n",
            "Trainging:  78%|███████▊  | 418/537 [29:14<08:11,  4.13s/it, running train loss: 0.58108]\u001b[A\n",
            "Trainging:  78%|███████▊  | 419/537 [29:14<07:38,  3.88s/it, running train loss: 0.58108]\u001b[A\n",
            "Trainging:  78%|███████▊  | 419/537 [29:18<07:38,  3.88s/it, running train loss: 0.51903]\u001b[A\n",
            "Trainging:  78%|███████▊  | 420/537 [29:18<07:29,  3.84s/it, running train loss: 0.51903]\u001b[A\n",
            "Trainging:  78%|███████▊  | 420/537 [29:21<07:29,  3.84s/it, running train loss: 0.77544]\u001b[A\n",
            "Trainging:  78%|███████▊  | 421/537 [29:21<06:54,  3.58s/it, running train loss: 0.77544]\u001b[A\n",
            "Trainging:  78%|███████▊  | 421/537 [29:25<06:54,  3.58s/it, running train loss: 0.40536]\u001b[A\n",
            "Trainging:  79%|███████▊  | 422/537 [29:25<07:02,  3.68s/it, running train loss: 0.40536]\u001b[A\n",
            "Trainging:  79%|███████▊  | 422/537 [29:28<07:02,  3.68s/it, running train loss: 0.63568]\u001b[A\n",
            "Trainging:  79%|███████▉  | 423/537 [29:28<06:50,  3.60s/it, running train loss: 0.63568]\u001b[A\n",
            "Trainging:  79%|███████▉  | 423/537 [29:32<06:50,  3.60s/it, running train loss: 0.54778]\u001b[A\n",
            "Trainging:  79%|███████▉  | 424/537 [29:32<07:09,  3.80s/it, running train loss: 0.54778]\u001b[A\n",
            "Trainging:  79%|███████▉  | 424/537 [29:36<07:09,  3.80s/it, running train loss: 0.49518]\u001b[A\n",
            "Trainging:  79%|███████▉  | 425/537 [29:36<07:00,  3.76s/it, running train loss: 0.49518]\u001b[A\n",
            "Trainging:  79%|███████▉  | 425/537 [29:42<07:00,  3.76s/it, running train loss: 0.52211]\u001b[A\n",
            "Trainging:  79%|███████▉  | 426/537 [29:42<07:54,  4.28s/it, running train loss: 0.52211]\u001b[A\n",
            "Trainging:  79%|███████▉  | 426/537 [29:46<07:54,  4.28s/it, running train loss: 0.55538]\u001b[A\n",
            "Trainging:  80%|███████▉  | 427/537 [29:46<07:50,  4.27s/it, running train loss: 0.55538]\u001b[A\n",
            "Trainging:  80%|███████▉  | 427/537 [29:49<07:50,  4.27s/it, running train loss: 0.56854]\u001b[A\n",
            "Trainging:  80%|███████▉  | 428/537 [29:49<07:24,  4.08s/it, running train loss: 0.56854]\u001b[A\n",
            "Trainging:  80%|███████▉  | 428/537 [29:53<07:24,  4.08s/it, running train loss: 0.40969]\u001b[A\n",
            "Trainging:  80%|███████▉  | 429/537 [29:53<06:56,  3.86s/it, running train loss: 0.40969]\u001b[A\n",
            "Trainging:  80%|███████▉  | 429/537 [29:56<06:56,  3.86s/it, running train loss: 0.57652]\u001b[A\n",
            "Trainging:  80%|████████  | 430/537 [29:57<06:49,  3.82s/it, running train loss: 0.57652]\u001b[A\n",
            "Trainging:  80%|████████  | 430/537 [30:00<06:49,  3.82s/it, running train loss: 0.55130]\u001b[A\n",
            "Trainging:  80%|████████  | 431/537 [30:00<06:44,  3.82s/it, running train loss: 0.55130]\u001b[A\n",
            "Trainging:  80%|████████  | 431/537 [30:04<06:44,  3.82s/it, running train loss: 0.55413]\u001b[A\n",
            "Trainging:  80%|████████  | 432/537 [30:04<06:31,  3.73s/it, running train loss: 0.55413]\u001b[A\n",
            "Trainging:  80%|████████  | 432/537 [30:08<06:31,  3.73s/it, running train loss: 0.49032]\u001b[A\n",
            "Trainging:  81%|████████  | 433/537 [30:08<06:45,  3.90s/it, running train loss: 0.49032]\u001b[A\n",
            "Trainging:  81%|████████  | 433/537 [30:12<06:45,  3.90s/it, running train loss: 0.61134]\u001b[A\n",
            "Trainging:  81%|████████  | 434/537 [30:12<06:33,  3.82s/it, running train loss: 0.61134]\u001b[A\n",
            "Trainging:  81%|████████  | 434/537 [30:15<06:33,  3.82s/it, running train loss: 0.47966]\u001b[A\n",
            "Trainging:  81%|████████  | 435/537 [30:15<06:19,  3.72s/it, running train loss: 0.47966]\u001b[A\n",
            "Trainging:  81%|████████  | 435/537 [30:19<06:19,  3.72s/it, running train loss: 0.44137]\u001b[A\n",
            "Trainging:  81%|████████  | 436/537 [30:19<06:22,  3.78s/it, running train loss: 0.44137]\u001b[A\n",
            "Trainging:  81%|████████  | 436/537 [30:23<06:22,  3.78s/it, running train loss: 0.53282]\u001b[A\n",
            "Trainging:  81%|████████▏ | 437/537 [30:23<06:07,  3.68s/it, running train loss: 0.53282]\u001b[A\n",
            "Trainging:  81%|████████▏ | 437/537 [30:27<06:07,  3.68s/it, running train loss: 0.52233]\u001b[A\n",
            "Trainging:  82%|████████▏ | 438/537 [30:27<06:20,  3.84s/it, running train loss: 0.52233]\u001b[A\n",
            "Trainging:  82%|████████▏ | 438/537 [30:31<06:20,  3.84s/it, running train loss: 0.47998]\u001b[A\n",
            "Trainging:  82%|████████▏ | 439/537 [30:31<06:14,  3.82s/it, running train loss: 0.47998]\u001b[A\n",
            "Trainging:  82%|████████▏ | 439/537 [30:34<06:14,  3.82s/it, running train loss: 0.48463]\u001b[A\n",
            "Trainging:  82%|████████▏ | 440/537 [30:34<06:04,  3.75s/it, running train loss: 0.48463]\u001b[A\n",
            "Trainging:  82%|████████▏ | 440/537 [30:38<06:04,  3.75s/it, running train loss: 0.58208]\u001b[A\n",
            "Trainging:  82%|████████▏ | 441/537 [30:38<05:59,  3.75s/it, running train loss: 0.58208]\u001b[A\n",
            "Trainging:  82%|████████▏ | 441/537 [30:41<05:59,  3.75s/it, running train loss: 0.53979]\u001b[A\n",
            "Trainging:  82%|████████▏ | 442/537 [30:41<05:44,  3.62s/it, running train loss: 0.53979]\u001b[A\n",
            "Trainging:  82%|████████▏ | 442/537 [30:45<05:44,  3.62s/it, running train loss: 0.52027]\u001b[A\n",
            "Trainging:  82%|████████▏ | 443/537 [30:45<05:37,  3.59s/it, running train loss: 0.52027]\u001b[A\n",
            "Trainging:  82%|████████▏ | 443/537 [30:48<05:37,  3.59s/it, running train loss: 0.46840]\u001b[A\n",
            "Trainging:  83%|████████▎ | 444/537 [30:48<05:28,  3.53s/it, running train loss: 0.46840]\u001b[A\n",
            "Trainging:  83%|████████▎ | 444/537 [30:51<05:28,  3.53s/it, running train loss: 0.49662]\u001b[A\n",
            "Trainging:  83%|████████▎ | 445/537 [30:51<05:14,  3.42s/it, running train loss: 0.49662]\u001b[A\n",
            "Trainging:  83%|████████▎ | 445/537 [30:55<05:14,  3.42s/it, running train loss: 0.36593]\u001b[A\n",
            "Trainging:  83%|████████▎ | 446/537 [30:55<05:13,  3.44s/it, running train loss: 0.36593]\u001b[A\n",
            "Trainging:  83%|████████▎ | 446/537 [30:58<05:13,  3.44s/it, running train loss: 0.50795]\u001b[A\n",
            "Trainging:  83%|████████▎ | 447/537 [30:58<05:11,  3.46s/it, running train loss: 0.50795]\u001b[A\n",
            "Trainging:  83%|████████▎ | 447/537 [31:02<05:11,  3.46s/it, running train loss: 0.43443]\u001b[A\n",
            "Trainging:  83%|████████▎ | 448/537 [31:02<05:07,  3.46s/it, running train loss: 0.43443]\u001b[A\n",
            "Trainging:  83%|████████▎ | 448/537 [31:06<05:07,  3.46s/it, running train loss: 0.48948]\u001b[A\n",
            "Trainging:  84%|████████▎ | 449/537 [31:06<05:16,  3.60s/it, running train loss: 0.48948]\u001b[A\n",
            "Trainging:  84%|████████▎ | 449/537 [31:10<05:16,  3.60s/it, running train loss: 0.52595]\u001b[A\n",
            "Trainging:  84%|████████▍ | 450/537 [31:10<05:22,  3.70s/it, running train loss: 0.52595]\u001b[A\n",
            "Trainging:  84%|████████▍ | 450/537 [31:14<05:22,  3.70s/it, running train loss: 0.58763]\u001b[A\n",
            "Trainging:  84%|████████▍ | 451/537 [31:14<05:38,  3.93s/it, running train loss: 0.58763]\u001b[A\n",
            "Trainging:  84%|████████▍ | 451/537 [31:18<05:38,  3.93s/it, running train loss: 0.49184]\u001b[A\n",
            "Trainging:  84%|████████▍ | 452/537 [31:18<05:22,  3.80s/it, running train loss: 0.49184]\u001b[A\n",
            "Trainging:  84%|████████▍ | 452/537 [31:22<05:22,  3.80s/it, running train loss: 0.49439]\u001b[A\n",
            "Trainging:  84%|████████▍ | 453/537 [31:22<05:22,  3.84s/it, running train loss: 0.49439]\u001b[A\n",
            "Trainging:  84%|████████▍ | 453/537 [31:25<05:22,  3.84s/it, running train loss: 0.51246]\u001b[A\n",
            "Trainging:  85%|████████▍ | 454/537 [31:25<05:04,  3.67s/it, running train loss: 0.51246]\u001b[A\n",
            "Trainging:  85%|████████▍ | 454/537 [31:29<05:04,  3.67s/it, running train loss: 0.57456]\u001b[A\n",
            "Trainging:  85%|████████▍ | 455/537 [31:29<05:19,  3.89s/it, running train loss: 0.57456]\u001b[A\n",
            "Trainging:  85%|████████▍ | 455/537 [31:33<05:19,  3.89s/it, running train loss: 0.51838]\u001b[A\n",
            "Trainging:  85%|████████▍ | 456/537 [31:33<05:11,  3.85s/it, running train loss: 0.51838]\u001b[A\n",
            "Trainging:  85%|████████▍ | 456/537 [31:37<05:11,  3.85s/it, running train loss: 0.52621]\u001b[A\n",
            "Trainging:  85%|████████▌ | 457/537 [31:37<05:02,  3.79s/it, running train loss: 0.52621]\u001b[A\n",
            "Trainging:  85%|████████▌ | 457/537 [31:40<05:02,  3.79s/it, running train loss: 0.51749]\u001b[A\n",
            "Trainging:  85%|████████▌ | 458/537 [31:40<04:58,  3.78s/it, running train loss: 0.51749]\u001b[A\n",
            "Trainging:  85%|████████▌ | 458/537 [31:44<04:58,  3.78s/it, running train loss: 0.46449]\u001b[A\n",
            "Trainging:  85%|████████▌ | 459/537 [31:44<04:54,  3.78s/it, running train loss: 0.46449]\u001b[A\n",
            "Trainging:  85%|████████▌ | 459/537 [31:48<04:54,  3.78s/it, running train loss: 0.57927]\u001b[A\n",
            "Trainging:  86%|████████▌ | 460/537 [31:48<04:47,  3.74s/it, running train loss: 0.57927]\u001b[A\n",
            "Trainging:  86%|████████▌ | 460/537 [31:53<04:47,  3.74s/it, running train loss: 0.50335]\u001b[A\n",
            "Trainging:  86%|████████▌ | 461/537 [31:53<05:08,  4.06s/it, running train loss: 0.50335]\u001b[A\n",
            "Trainging:  86%|████████▌ | 461/537 [31:56<05:08,  4.06s/it, running train loss: 0.61880]\u001b[A\n",
            "Trainging:  86%|████████▌ | 462/537 [31:56<04:57,  3.96s/it, running train loss: 0.61880]\u001b[A\n",
            "Trainging:  86%|████████▌ | 462/537 [32:01<04:57,  3.96s/it, running train loss: 0.51312]\u001b[A\n",
            "Trainging:  86%|████████▌ | 463/537 [32:01<05:03,  4.11s/it, running train loss: 0.51312]\u001b[A\n",
            "Trainging:  86%|████████▌ | 463/537 [32:05<05:03,  4.11s/it, running train loss: 0.54580]\u001b[A\n",
            "Trainging:  86%|████████▋ | 464/537 [32:05<04:54,  4.04s/it, running train loss: 0.54580]\u001b[A\n",
            "Trainging:  86%|████████▋ | 464/537 [32:08<04:54,  4.04s/it, running train loss: 0.48388]\u001b[A\n",
            "Trainging:  87%|████████▋ | 465/537 [32:08<04:41,  3.91s/it, running train loss: 0.48388]\u001b[A\n",
            "Trainging:  87%|████████▋ | 465/537 [32:13<04:41,  3.91s/it, running train loss: 0.52822]\u001b[A\n",
            "Trainging:  87%|████████▋ | 466/537 [32:13<04:54,  4.14s/it, running train loss: 0.52822]\u001b[A\n",
            "Trainging:  87%|████████▋ | 466/537 [32:17<04:54,  4.14s/it, running train loss: 0.51231]\u001b[A\n",
            "Trainging:  87%|████████▋ | 467/537 [32:17<04:45,  4.08s/it, running train loss: 0.51231]\u001b[A\n",
            "Trainging:  87%|████████▋ | 467/537 [32:22<04:45,  4.08s/it, running train loss: 0.47359]\u001b[A\n",
            "Trainging:  87%|████████▋ | 468/537 [32:22<04:58,  4.32s/it, running train loss: 0.47359]\u001b[A\n",
            "Trainging:  87%|████████▋ | 468/537 [32:26<04:58,  4.32s/it, running train loss: 0.54116]\u001b[A\n",
            "Trainging:  87%|████████▋ | 469/537 [32:26<04:51,  4.29s/it, running train loss: 0.54116]\u001b[A\n",
            "Trainging:  87%|████████▋ | 469/537 [32:30<04:51,  4.29s/it, running train loss: 0.43448]\u001b[A\n",
            "Trainging:  88%|████████▊ | 470/537 [32:30<04:40,  4.18s/it, running train loss: 0.43448]\u001b[A\n",
            "Trainging:  88%|████████▊ | 470/537 [32:35<04:40,  4.18s/it, running train loss: 0.64985]\u001b[A\n",
            "Trainging:  88%|████████▊ | 471/537 [32:35<04:48,  4.37s/it, running train loss: 0.64985]\u001b[A\n",
            "Trainging:  88%|████████▊ | 471/537 [32:39<04:48,  4.37s/it, running train loss: 0.60344]\u001b[A\n",
            "Trainging:  88%|████████▊ | 472/537 [32:39<04:35,  4.24s/it, running train loss: 0.60344]\u001b[A\n",
            "Trainging:  88%|████████▊ | 472/537 [32:42<04:35,  4.24s/it, running train loss: 0.44870]\u001b[A\n",
            "Trainging:  88%|████████▊ | 473/537 [32:42<04:18,  4.04s/it, running train loss: 0.44870]\u001b[A\n",
            "Trainging:  88%|████████▊ | 473/537 [32:46<04:18,  4.04s/it, running train loss: 0.52243]\u001b[A\n",
            "Trainging:  88%|████████▊ | 474/537 [32:46<04:17,  4.09s/it, running train loss: 0.52243]\u001b[A\n",
            "Trainging:  88%|████████▊ | 474/537 [32:50<04:17,  4.09s/it, running train loss: 0.55249]\u001b[A\n",
            "Trainging:  88%|████████▊ | 475/537 [32:50<04:04,  3.94s/it, running train loss: 0.55249]\u001b[A\n",
            "Trainging:  88%|████████▊ | 475/537 [32:54<04:04,  3.94s/it, running train loss: 0.55795]\u001b[A\n",
            "Trainging:  89%|████████▊ | 476/537 [32:54<03:59,  3.92s/it, running train loss: 0.55795]\u001b[A\n",
            "Trainging:  89%|████████▊ | 476/537 [32:57<03:59,  3.92s/it, running train loss: 0.60920]\u001b[A\n",
            "Trainging:  89%|████████▉ | 477/537 [32:57<03:45,  3.75s/it, running train loss: 0.60920]\u001b[A\n",
            "Trainging:  89%|████████▉ | 477/537 [33:02<03:45,  3.75s/it, running train loss: 0.51657]\u001b[A\n",
            "Trainging:  89%|████████▉ | 478/537 [33:02<03:55,  3.98s/it, running train loss: 0.51657]\u001b[A\n",
            "Trainging:  89%|████████▉ | 478/537 [33:06<03:55,  3.98s/it, running train loss: 0.43578]\u001b[A\n",
            "Trainging:  89%|████████▉ | 479/537 [33:06<03:57,  4.10s/it, running train loss: 0.43578]\u001b[A\n",
            "Trainging:  89%|████████▉ | 479/537 [33:10<03:57,  4.10s/it, running train loss: 0.48748]\u001b[A\n",
            "Trainging:  89%|████████▉ | 480/537 [33:10<03:49,  4.02s/it, running train loss: 0.48748]\u001b[A\n",
            "Trainging:  89%|████████▉ | 480/537 [33:14<03:49,  4.02s/it, running train loss: 0.59078]\u001b[A\n",
            "Trainging:  90%|████████▉ | 481/537 [33:15<03:53,  4.17s/it, running train loss: 0.59078]\u001b[A\n",
            "Trainging:  90%|████████▉ | 481/537 [33:19<03:53,  4.17s/it, running train loss: 0.46182]\u001b[A\n",
            "Trainging:  90%|████████▉ | 482/537 [33:19<03:56,  4.30s/it, running train loss: 0.46182]\u001b[A\n",
            "Trainging:  90%|████████▉ | 482/537 [33:22<03:56,  4.30s/it, running train loss: 0.54956]\u001b[A\n",
            "Trainging:  90%|████████▉ | 483/537 [33:22<03:36,  4.01s/it, running train loss: 0.54956]\u001b[A\n",
            "Trainging:  90%|████████▉ | 483/537 [33:27<03:36,  4.01s/it, running train loss: 0.53248]\u001b[A\n",
            "Trainging:  90%|█████████ | 484/537 [33:27<03:39,  4.14s/it, running train loss: 0.53248]\u001b[A\n",
            "Trainging:  90%|█████████ | 484/537 [33:31<03:39,  4.14s/it, running train loss: 0.47779]\u001b[A\n",
            "Trainging:  90%|█████████ | 485/537 [33:31<03:30,  4.05s/it, running train loss: 0.47779]\u001b[A\n",
            "Trainging:  90%|█████████ | 485/537 [33:34<03:30,  4.05s/it, running train loss: 0.51962]\u001b[A\n",
            "Trainging:  91%|█████████ | 486/537 [33:34<03:18,  3.90s/it, running train loss: 0.51962]\u001b[A\n",
            "Trainging:  91%|█████████ | 486/537 [33:36<03:18,  3.90s/it, running train loss: 0.61010]\u001b[A\n",
            "Trainging:  91%|█████████ | 487/537 [33:36<02:49,  3.38s/it, running train loss: 0.61010]\u001b[A\n",
            "Trainging:  91%|█████████ | 487/537 [33:39<02:49,  3.38s/it, running train loss: 0.45962]\u001b[A\n",
            "Trainging:  91%|█████████ | 488/537 [33:39<02:28,  3.03s/it, running train loss: 0.45962]\u001b[A\n",
            "Trainging:  91%|█████████ | 488/537 [33:42<02:28,  3.03s/it, running train loss: 0.61265]\u001b[A\n",
            "Trainging:  91%|█████████ | 489/537 [33:42<02:31,  3.15s/it, running train loss: 0.61265]\u001b[A\n",
            "Trainging:  91%|█████████ | 489/537 [33:45<02:31,  3.15s/it, running train loss: 0.52593]\u001b[A\n",
            "Trainging:  91%|█████████ | 490/537 [33:45<02:30,  3.20s/it, running train loss: 0.52593]\u001b[A\n",
            "Trainging:  91%|█████████ | 490/537 [33:50<02:30,  3.20s/it, running train loss: 0.55256]\u001b[A\n",
            "Trainging:  91%|█████████▏| 491/537 [33:50<02:44,  3.57s/it, running train loss: 0.55256]\u001b[A\n",
            "Trainging:  91%|█████████▏| 491/537 [33:55<02:44,  3.57s/it, running train loss: 0.59957]\u001b[A\n",
            "Trainging:  92%|█████████▏| 492/537 [33:55<03:03,  4.08s/it, running train loss: 0.59957]\u001b[A\n",
            "Trainging:  92%|█████████▏| 492/537 [33:58<03:03,  4.08s/it, running train loss: 0.64648]\u001b[A\n",
            "Trainging:  92%|█████████▏| 493/537 [33:58<02:48,  3.84s/it, running train loss: 0.64648]\u001b[A\n",
            "Trainging:  92%|█████████▏| 493/537 [34:04<02:48,  3.84s/it, running train loss: 0.60624]\u001b[A\n",
            "Trainging:  92%|█████████▏| 494/537 [34:04<03:04,  4.29s/it, running train loss: 0.60624]\u001b[A\n",
            "Trainging:  92%|█████████▏| 494/537 [34:08<03:04,  4.29s/it, running train loss: 0.54138]\u001b[A\n",
            "Trainging:  92%|█████████▏| 495/537 [34:08<02:53,  4.14s/it, running train loss: 0.54138]\u001b[A\n",
            "Trainging:  92%|█████████▏| 495/537 [34:11<02:53,  4.14s/it, running train loss: 0.54305]\u001b[A\n",
            "Trainging:  92%|█████████▏| 496/537 [34:11<02:43,  3.99s/it, running train loss: 0.54305]\u001b[A\n",
            "Trainging:  92%|█████████▏| 496/537 [34:15<02:43,  3.99s/it, running train loss: 0.52761]\u001b[A\n",
            "Trainging:  93%|█████████▎| 497/537 [34:15<02:38,  3.95s/it, running train loss: 0.52761]\u001b[A\n",
            "Trainging:  93%|█████████▎| 497/537 [34:19<02:38,  3.95s/it, running train loss: 0.50966]\u001b[A\n",
            "Trainging:  93%|█████████▎| 498/537 [34:19<02:29,  3.83s/it, running train loss: 0.50966]\u001b[A\n",
            "Trainging:  93%|█████████▎| 498/537 [34:23<02:29,  3.83s/it, running train loss: 0.53740]\u001b[A\n",
            "Trainging:  93%|█████████▎| 499/537 [34:23<02:31,  4.00s/it, running train loss: 0.53740]\u001b[A\n",
            "Trainging:  93%|█████████▎| 499/537 [34:27<02:31,  4.00s/it, running train loss: 0.50977]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> EMA starting .....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Evaluation:   0%|          | 0/68 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|▏         | 1/68 [00:00<00:40,  1.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 2/68 [00:01<00:34,  1.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 3/68 [00:01<00:28,  2.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 4/68 [00:01<00:28,  2.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 5/68 [00:02<00:27,  2.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 6/68 [00:02<00:25,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|█         | 7/68 [00:03<00:25,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 8/68 [00:03<00:25,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 9/68 [00:03<00:24,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 10/68 [00:04<00:24,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 11/68 [00:04<00:24,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 12/68 [00:05<00:23,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 13/68 [00:05<00:23,  2.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 14/68 [00:06<00:23,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 15/68 [00:06<00:23,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▎       | 16/68 [00:06<00:22,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▌       | 17/68 [00:07<00:20,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 18/68 [00:07<00:20,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 19/68 [00:08<00:19,  2.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 20/68 [00:08<00:18,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 21/68 [00:08<00:18,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 22/68 [00:09<00:18,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 23/68 [00:09<00:18,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▌      | 24/68 [00:10<00:16,  2.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 25/68 [00:10<00:17,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 26/68 [00:10<00:17,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 27/68 [00:11<00:16,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████      | 28/68 [00:11<00:15,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 29/68 [00:12<00:15,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 30/68 [00:12<00:15,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▌     | 31/68 [00:12<00:14,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 32/68 [00:13<00:14,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▊     | 33/68 [00:13<00:14,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 34/68 [00:14<00:13,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████▏    | 35/68 [00:14<00:12,  2.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 36/68 [00:14<00:12,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▍    | 37/68 [00:15<00:12,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 38/68 [00:15<00:12,  2.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 39/68 [00:16<00:12,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▉    | 40/68 [00:16<00:11,  2.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 41/68 [00:16<00:10,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 42/68 [00:17<00:10,  2.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 43/68 [00:17<00:10,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▍   | 44/68 [00:18<00:10,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 45/68 [00:18<00:09,  2.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 46/68 [00:19<00:09,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 47/68 [00:19<00:09,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 48/68 [00:20<00:08,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 49/68 [00:20<00:08,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 50/68 [00:20<00:07,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 51/68 [00:21<00:07,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▋  | 52/68 [00:21<00:06,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 53/68 [00:22<00:06,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 54/68 [00:22<00:06,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 55/68 [00:22<00:05,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 56/68 [00:23<00:04,  2.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 57/68 [00:23<00:04,  2.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 58/68 [00:24<00:04,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 59/68 [00:24<00:03,  2.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 60/68 [00:24<00:03,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|████████▉ | 61/68 [00:25<00:02,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 62/68 [00:25<00:02,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 63/68 [00:26<00:02,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 64/68 [00:26<00:01,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 65/68 [00:27<00:01,  2.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 66/68 [00:27<00:00,  2.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▊| 67/68 [00:27<00:00,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 68/68 [00:28<00:00,  2.41it/s]\n",
            "\n",
            "Trainging:  93%|█████████▎| 500/537 [34:57<07:58, 12.94s/it, running train loss: 0.50977]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> training loss: 0.528752, valid loss: 0.508450,valid f1: 0.525030, valid acc:0.720806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Trainging:  93%|█████████▎| 500/537 [35:00<07:58, 12.94s/it, running train loss: 0.59890]\u001b[A\n",
            "Trainging:  93%|█████████▎| 501/537 [35:00<06:06, 10.17s/it, running train loss: 0.59890]\u001b[A\n",
            "Trainging:  93%|█████████▎| 501/537 [35:04<06:06, 10.17s/it, running train loss: 0.48773]\u001b[A\n",
            "Trainging:  93%|█████████▎| 502/537 [35:04<04:45,  8.14s/it, running train loss: 0.48773]\u001b[A\n",
            "Trainging:  93%|█████████▎| 502/537 [35:10<04:45,  8.14s/it, running train loss: 0.51034]\u001b[A\n",
            "Trainging:  94%|█████████▎| 503/537 [35:10<04:13,  7.46s/it, running train loss: 0.51034]\u001b[A\n",
            "Trainging:  94%|█████████▎| 503/537 [35:13<04:13,  7.46s/it, running train loss: 0.55732]\u001b[A\n",
            "Trainging:  94%|█████████▍| 504/537 [35:13<03:28,  6.31s/it, running train loss: 0.55732]\u001b[A\n",
            "Trainging:  94%|█████████▍| 504/537 [35:18<03:28,  6.31s/it, running train loss: 0.56393]\u001b[A\n",
            "Trainging:  94%|█████████▍| 505/537 [35:18<03:09,  5.93s/it, running train loss: 0.56393]\u001b[A\n",
            "Trainging:  94%|█████████▍| 505/537 [35:22<03:09,  5.93s/it, running train loss: 0.45962]\u001b[A\n",
            "Trainging:  94%|█████████▍| 506/537 [35:22<02:42,  5.24s/it, running train loss: 0.45962]\u001b[A\n",
            "Trainging:  94%|█████████▍| 506/537 [35:26<02:42,  5.24s/it, running train loss: 0.45491]\u001b[A\n",
            "Trainging:  94%|█████████▍| 507/537 [35:26<02:24,  4.83s/it, running train loss: 0.45491]\u001b[A\n",
            "Trainging:  94%|█████████▍| 507/537 [35:29<02:24,  4.83s/it, running train loss: 0.56994]\u001b[A\n",
            "Trainging:  95%|█████████▍| 508/537 [35:29<02:08,  4.45s/it, running train loss: 0.56994]\u001b[A\n",
            "Trainging:  95%|█████████▍| 508/537 [35:34<02:08,  4.45s/it, running train loss: 0.45953]\u001b[A\n",
            "Trainging:  95%|█████████▍| 509/537 [35:34<02:05,  4.49s/it, running train loss: 0.45953]\u001b[A\n",
            "Trainging:  95%|█████████▍| 509/537 [35:38<02:05,  4.49s/it, running train loss: 0.49372]\u001b[A\n",
            "Trainging:  95%|█████████▍| 510/537 [35:38<01:53,  4.21s/it, running train loss: 0.49372]\u001b[A\n",
            "Trainging:  95%|█████████▍| 510/537 [35:41<01:53,  4.21s/it, running train loss: 0.53921]\u001b[A\n",
            "Trainging:  95%|█████████▌| 511/537 [35:41<01:46,  4.09s/it, running train loss: 0.53921]\u001b[A\n",
            "Trainging:  95%|█████████▌| 511/537 [35:47<01:46,  4.09s/it, running train loss: 0.45824]\u001b[A\n",
            "Trainging:  95%|█████████▌| 512/537 [35:47<01:56,  4.67s/it, running train loss: 0.45824]\u001b[A\n",
            "Trainging:  95%|█████████▌| 512/537 [35:51<01:56,  4.67s/it, running train loss: 0.39312]\u001b[A\n",
            "Trainging:  96%|█████████▌| 513/537 [35:51<01:42,  4.29s/it, running train loss: 0.39312]\u001b[A\n",
            "Trainging:  96%|█████████▌| 513/537 [35:55<01:42,  4.29s/it, running train loss: 0.58530]\u001b[A\n",
            "Trainging:  96%|█████████▌| 514/537 [35:55<01:34,  4.10s/it, running train loss: 0.58530]\u001b[A\n",
            "Trainging:  96%|█████████▌| 514/537 [35:58<01:34,  4.10s/it, running train loss: 0.44700]\u001b[A\n",
            "Trainging:  96%|█████████▌| 515/537 [35:58<01:24,  3.86s/it, running train loss: 0.44700]\u001b[A\n",
            "Trainging:  96%|█████████▌| 515/537 [36:02<01:24,  3.86s/it, running train loss: 0.48986]\u001b[A\n",
            "Trainging:  96%|█████████▌| 516/537 [36:02<01:23,  3.99s/it, running train loss: 0.48986]\u001b[A\n",
            "Trainging:  96%|█████████▌| 516/537 [36:06<01:23,  3.99s/it, running train loss: 0.51272]\u001b[A\n",
            "Trainging:  96%|█████████▋| 517/537 [36:06<01:17,  3.86s/it, running train loss: 0.51272]\u001b[A\n",
            "Trainging:  96%|█████████▋| 517/537 [36:12<01:17,  3.86s/it, running train loss: 0.66785]\u001b[A\n",
            "Trainging:  96%|█████████▋| 518/537 [36:12<01:25,  4.52s/it, running train loss: 0.66785]\u001b[A\n",
            "Trainging:  96%|█████████▋| 518/537 [36:15<01:25,  4.52s/it, running train loss: 0.51037]\u001b[A\n",
            "Trainging:  97%|█████████▋| 519/537 [36:15<01:16,  4.23s/it, running train loss: 0.51037]\u001b[A\n",
            "Trainging:  97%|█████████▋| 519/537 [36:19<01:16,  4.23s/it, running train loss: 0.52458]\u001b[A\n",
            "Trainging:  97%|█████████▋| 520/537 [36:19<01:08,  4.02s/it, running train loss: 0.52458]\u001b[A\n",
            "Trainging:  97%|█████████▋| 520/537 [36:23<01:08,  4.02s/it, running train loss: 0.50643]\u001b[A\n",
            "Trainging:  97%|█████████▋| 521/537 [36:23<01:05,  4.08s/it, running train loss: 0.50643]\u001b[A\n",
            "Trainging:  97%|█████████▋| 521/537 [36:28<01:05,  4.08s/it, running train loss: 0.43124]\u001b[A\n",
            "Trainging:  97%|█████████▋| 522/537 [36:28<01:05,  4.34s/it, running train loss: 0.43124]\u001b[A\n",
            "Trainging:  97%|█████████▋| 522/537 [36:31<01:05,  4.34s/it, running train loss: 0.54398]\u001b[A\n",
            "Trainging:  97%|█████████▋| 523/537 [36:31<00:57,  4.09s/it, running train loss: 0.54398]\u001b[A\n",
            "Trainging:  97%|█████████▋| 523/537 [36:35<00:57,  4.09s/it, running train loss: 0.53666]\u001b[A\n",
            "Trainging:  98%|█████████▊| 524/537 [36:35<00:50,  3.89s/it, running train loss: 0.53666]\u001b[A\n",
            "Trainging:  98%|█████████▊| 524/537 [36:39<00:50,  3.89s/it, running train loss: 0.46254]\u001b[A\n",
            "Trainging:  98%|█████████▊| 525/537 [36:39<00:46,  3.89s/it, running train loss: 0.46254]\u001b[A\n",
            "Trainging:  98%|█████████▊| 525/537 [36:43<00:46,  3.89s/it, running train loss: 0.54715]\u001b[A\n",
            "Trainging:  98%|█████████▊| 526/537 [36:43<00:42,  3.88s/it, running train loss: 0.54715]\u001b[A\n",
            "Trainging:  98%|█████████▊| 526/537 [36:47<00:42,  3.88s/it, running train loss: 0.51325]\u001b[A\n",
            "Trainging:  98%|█████████▊| 527/537 [36:47<00:39,  3.98s/it, running train loss: 0.51325]\u001b[A\n",
            "Trainging:  98%|█████████▊| 527/537 [36:50<00:39,  3.98s/it, running train loss: 0.48419]\u001b[A\n",
            "Trainging:  98%|█████████▊| 528/537 [36:50<00:34,  3.84s/it, running train loss: 0.48419]\u001b[A\n",
            "Trainging:  98%|█████████▊| 528/537 [36:55<00:34,  3.84s/it, running train loss: 0.57385]\u001b[A\n",
            "Trainging:  99%|█████████▊| 529/537 [36:55<00:33,  4.18s/it, running train loss: 0.57385]\u001b[A\n",
            "Trainging:  99%|█████████▊| 529/537 [36:59<00:33,  4.18s/it, running train loss: 0.51870]\u001b[A\n",
            "Trainging:  99%|█████████▊| 530/537 [36:59<00:28,  4.08s/it, running train loss: 0.51870]\u001b[A\n",
            "Trainging:  99%|█████████▊| 530/537 [37:04<00:28,  4.08s/it, running train loss: 0.51028]\u001b[A\n",
            "Trainging:  99%|█████████▉| 531/537 [37:04<00:25,  4.18s/it, running train loss: 0.51028]\u001b[A\n",
            "Trainging:  99%|█████████▉| 531/537 [37:07<00:25,  4.18s/it, running train loss: 0.50982]\u001b[A\n",
            "Trainging:  99%|█████████▉| 532/537 [37:07<00:19,  3.94s/it, running train loss: 0.50982]\u001b[A\n",
            "Trainging:  99%|█████████▉| 532/537 [37:11<00:19,  3.94s/it, running train loss: 0.49961]\u001b[A\n",
            "Trainging:  99%|█████████▉| 533/537 [37:11<00:15,  3.95s/it, running train loss: 0.49961]\u001b[A\n",
            "Trainging:  99%|█████████▉| 533/537 [37:15<00:15,  3.95s/it, running train loss: 0.55173]\u001b[A\n",
            "Trainging:  99%|█████████▉| 534/537 [37:15<00:11,  3.86s/it, running train loss: 0.55173]\u001b[A\n",
            "Trainging:  99%|█████████▉| 534/537 [37:18<00:11,  3.86s/it, running train loss: 0.52598]\u001b[A\n",
            "Trainging: 100%|█████████▉| 535/537 [37:18<00:07,  3.84s/it, running train loss: 0.52598]\u001b[A\n",
            "Trainging: 100%|█████████▉| 535/537 [37:23<00:07,  3.84s/it, running train loss: 0.48228]\u001b[A\n",
            "Trainging: 100%|█████████▉| 536/537 [37:23<00:04,  4.05s/it, running train loss: 0.48228]\u001b[A\n",
            "Trainging: 100%|█████████▉| 536/537 [37:26<00:04,  4.05s/it, running train loss: 0.60387]\u001b[A\n",
            "Trainging: 100%|██████████| 537/537 [37:26<00:00,  4.18s/it, running train loss: 0.60387]\n",
            "100%|██████████| 1/1 [37:27<00:00, 2247.01s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(BertForSequenceClassification(\n",
              "   (bert): BertModel(\n",
              "     (embeddings): BertEmbeddings(\n",
              "       (word_embeddings): Embedding(21128, 768, padding_idx=1)\n",
              "       (position_embeddings): Embedding(512, 768)\n",
              "       (token_type_embeddings): Embedding(2, 768)\n",
              "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "     )\n",
              "     (encoder): BertEncoder(\n",
              "       (layer): ModuleList(\n",
              "         (0): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (1): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (2): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (3): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (4): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (5): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (6): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (7): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (8): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (9): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (10): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (11): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "       )\n",
              "     )\n",
              "     (pooler): BertPooler(\n",
              "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "       (activation): Tanh()\n",
              "     )\n",
              "   )\n",
              "   (dropout): Dropout(p=0.1, inplace=False)\n",
              "   (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              " ), './checkpoint- 500 - 0.720806')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}