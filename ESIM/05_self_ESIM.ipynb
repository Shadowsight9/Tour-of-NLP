{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_self_ESIM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faxwvxbp34mR",
        "outputId": "33d2b02e-292f-406a-afef-4ebba7047f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 22 04:44:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "#设置路径\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd2KAF6r4j-J",
        "outputId": "ecc330c6-a689-45aa-cd24-a9883b9b953f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers==4.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwtR4C9e5Vca",
        "outputId": "aecd6a59-8f31-4cf0-d953-124172598269"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.0.1\n",
            "  Downloading transformers-4.0.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 27.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 61.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (4.64.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.0.1) (3.0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.1.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.49 tokenizers-0.9.4 transformers-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==1.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNWZrNKAJFr_",
        "outputId": "dee80172-dffc-4d76-d016-9de7f179e54c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.4 kB/s \n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "config = {\n",
        "    'train_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/train.json',\n",
        "    'dev_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/dev.json',\n",
        "    'test_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/test.json',\n",
        "    'embedding_file_path':'/content/drive/MyDrive/Colab Notebooks/dataset/sgns.weibo.word.bz2',\n",
        "    'train_val_ratio':0.1,\n",
        "    'vocab_size':30000,\n",
        "    'batch_size':64,\n",
        "    # 从后面可以看出 64能容纳99%的句子长度\n",
        "    'max_seq_len':64,\n",
        "    'num_epochs':1,\n",
        "    'learning_rate':2e-5,\n",
        "    'logging_step':500,\n",
        "    'seed':2022\n",
        "}\n",
        "\n",
        "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  return seed\n",
        "\n",
        "seed_everything(config['seed'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mdTOeMT6U1w",
        "outputId": "a383e5a1-afe8-478e-dd91-a47ccd0f655b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2022"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def read_data(path):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "  with open(path, 'r', encoding='utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc='Reading data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      labels.append(int(line['label']))\n",
        "  \n",
        "  df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a','text_b','labels'])\n",
        "  return df"
      ],
      "metadata": {
        "id": "v9eF7CtP5ado"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 复习一下zip函数:\n",
        "# 这里有两个元组()，每个元组有两个句子，一个label\n",
        "datas = [([1,2,3],[4,5,6],1),([777,888,999],[321,543,654],0)]\n",
        "# 取出列表中每个元素（元组）\n",
        "print(*datas)\n",
        "print(zip(*datas))\n",
        "# 把 datas 中的同一类别的放在一起\n",
        "print(list(zip(*datas)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkIjefdbLZnZ",
        "outputId": "5f44e9c9-3db7-492b-8eda-fdbeba8a5bc2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([1, 2, 3], [4, 5, 6], 1) ([777, 888, 999], [321, 543, 654], 0)\n",
            "<zip object at 0x7f61de987640>\n",
            "[([1, 2, 3], [777, 888, 999]), ([4, 5, 6], [321, 543, 654]), (1, 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = read_data(config['train_file_path'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6SDvo_BHzxr",
        "outputId": "7c9dad69-3fea-40d7-a8ab-bf7cbd32b7c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data: 100%|██████████| 34334/34334 [00:00<00:00, 249849.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "u7g5tNarIggE",
        "outputId": "42847a39-2d7a-48df-f311-91f830106b64"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                text_a                                 text_b  labels\n",
              "0    蚂蚁借呗等额还款可以换成先息后本吗                             借呗有先息到期还本吗       0\n",
              "1           蚂蚁花呗说我违约一次                            蚂蚁花呗违约行为是什么       0\n",
              "2     帮我看一下本月花呗账单有没有结清                                 下月花呗账单       0\n",
              "3       蚂蚁借呗多长时间综合评估一次                                借呗得评估多久       0\n",
              "4  我的花呗账单是***，还款怎么是***  我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c74e7bf8-06f6-4495-a70f-385b2bc5dfa4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_a</th>\n",
              "      <th>text_b</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
              "      <td>借呗有先息到期还本吗</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>蚂蚁花呗说我违约一次</td>\n",
              "      <td>蚂蚁花呗违约行为是什么</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
              "      <td>下月花呗账单</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>蚂蚁借呗多长时间综合评估一次</td>\n",
              "      <td>借呗得评估多久</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我的花呗账单是***，还款怎么是***</td>\n",
              "      <td>我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c74e7bf8-06f6-4495-a70f-385b2bc5dfa4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c74e7bf8-06f6-4495-a70f-385b2bc5dfa4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c74e7bf8-06f6-4495-a70f-385b2bc5dfa4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = read_data(config['dev_file_path'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEQpzZ1qI7FB",
        "outputId": "654477f4-ee1e-4c80-e778-4464ce950fc5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading data: 100%|██████████| 4316/4316 [00:00<00:00, 250805.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "xYHahZ0BJBLJ",
        "outputId": "f069fbeb-f6d5-402e-f16f-337161861481"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             text_a                text_b  labels\n",
              "0         双十一花呗提额在哪              里可以提花呗额度       0\n",
              "1        花呗支持高铁票支付吗         为什么友付宝不支持花呗付款       0\n",
              "2  我的蚂蚁花呗支付金额怎么会有限制  我到支付宝实体店消费用花呗支付受金额限制       1\n",
              "3    为什么有花呗额度不能分期付款              花呗分期额度不足       0\n",
              "4       赠品不能设置用花呗付款            怎么不能花呗分期付款       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-755a77b2-3e24-4961-b7c6-cfcbc58c3b90\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_a</th>\n",
              "      <th>text_b</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>双十一花呗提额在哪</td>\n",
              "      <td>里可以提花呗额度</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>花呗支持高铁票支付吗</td>\n",
              "      <td>为什么友付宝不支持花呗付款</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我的蚂蚁花呗支付金额怎么会有限制</td>\n",
              "      <td>我到支付宝实体店消费用花呗支付受金额限制</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>为什么有花呗额度不能分期付款</td>\n",
              "      <td>花呗分期额度不足</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>赠品不能设置用花呗付款</td>\n",
              "      <td>怎么不能花呗分期付款</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-755a77b2-3e24-4961-b7c6-cfcbc58c3b90')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-755a77b2-3e24-4961-b7c6-cfcbc58c3b90 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-755a77b2-3e24-4961-b7c6-cfcbc58c3b90');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_df.text_a.str.len() + train_df.text_b.str.len()).hist(bins = 20);\n",
        "(dev_df.text_a.str.len() + dev_df.text_b.str.len()).hist(bins = 20);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8wo2ged9JNGI",
        "outputId": "c4f09f54-3816-4931-e2f6-17ba18cf1cd4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 64能容纳99%的句子长度\n",
        "(train_df.text_a.str.len() + train_df.text_b.str.len()).quantile(0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpJl4UuwJmAC",
        "outputId": "8432964d-b299-428b-c437-e9e225466409"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 处理文件，生层词表放回关于train_df[train.json + dev.json],test_df\n",
        "from collections import Counter\n",
        "import jieba\n",
        "import bz2\n",
        "def preprocess(config):\n",
        "  def convert2df(file_path, dataset='train'):\n",
        "    sentence_a = []\n",
        "    sentence_b = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r', encoding='utf8') as f:\n",
        "      for line in tqdm(f.readlines(), desc=f'Reading {dataset} data'):\n",
        "        line = json.loads(line)\n",
        "        sentence_a.append(line['sentence1'])\n",
        "        sentence_b.append(line['sentence2'])\n",
        "        if dataset != 'test':\n",
        "          labels.append(int(line['label']))\n",
        "        else:\n",
        "          labels.append(0)\n",
        "        # tokens为每次新加入的句子\n",
        "        tokens = list(jieba.cut(sentence_a[-1])) + list(jieba.cut(sentence_b[-1])) \n",
        "        # print('tokens:',tokens)\n",
        "        # tokens: ['蚂蚁', '花', '呗', '说', '我', '违约', '一次', '蚂蚁', '花', '呗', '违约', '行为', '是', '什么']\n",
        "        token_counter.update(tokens)\n",
        "    df = pd.DataFrame(zip(sentence_a,sentence_b,labels),columns=['text_a','text_b','labels'])\n",
        "    return df\n",
        "  \n",
        "  token_counter = Counter()\n",
        "\n",
        "  train_df = convert2df(config['train_file_path'],'train')\n",
        "  dev_df = convert2df(config['dev_file_path'],'dev')\n",
        "  test_df = convert2df(config['test_file_path'],'test')\n",
        "\n",
        "  train_df = train_df.append(dev_df)\n",
        "  vocab = set(token for token, _ in token_counter.most_common(config['vocab_size']))\n",
        "  # print('vocab:',vocab)\n",
        "  # vocab: {'城', '帮不上', '百度', '钱分', '会加分', '我晚', '手机短信'.......}\n",
        "  return train_df, test_df, vocab\n",
        "\n"
      ],
      "metadata": {
        "id": "64kVEP5GBNg6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df, vocab = preprocess(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmSHkwAyEwmI",
        "outputId": "656fee35-ed88-4306-b808-5ba378bc7eac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rReading train data:   0%|          | 0/34334 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.966 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Reading train data: 100%|██████████| 34334/34334 [00:09<00:00, 3656.15it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:02<00:00, 2083.90it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:02<00:00, 1902.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TextCNN 词表对应成向量\n",
        "def get_embedding(vocab, embedding_file_path):\n",
        "  print('processing embedding file, please wait...')\n",
        "\n",
        "  token2embedding = {}\n",
        "\n",
        "  with bz2.open(embedding_file_path) as f:\n",
        "    token_vectors = f.readlines()\n",
        "    # print('token_vectors:',list(token_vectors[0:10]))\n",
        "    # token_vectors: [b'195202 300\\n', b'\\xef\\xbc\\x8c 0.094386 -0.200944 -0.030828 0.277130 ... 0.126182 -0.554329 -0.328050 \\n', b'......\\n'...]\n",
        "    meta_info = token_vectors[0].split()\n",
        "    # print('meta_info:',meta_info)\n",
        "    # meta_info: [b'195202', b'300']\n",
        "    print(f'{meta_info[0]} tokens in embedding file in total, vector size is {meta_info[-1]}')\n",
        "\n",
        "    for line in tqdm(token_vectors[1:]):\n",
        "      line = line.split()\n",
        "      token = line[0].decode('utf8')\n",
        "      # print('token:',token)\n",
        "      # token: ！\n",
        "      # token: 可以\n",
        "      # token: 等\n",
        "      # ...\n",
        "      vector = line[1:]\n",
        "      if token in vocab:\n",
        "        token2embedding[token] = [float(num) for num in vector]\n",
        "        # print('token2embedding[token]:',token2embedding[token])\n",
        "        # token2embedding[token]: [-0.124044, -0.053688, 0.157958, ... -0.127075, -0.020528, 0.032646]\n",
        "  #从4开始\n",
        "  token2idx = {token: idx for idx, token in enumerate(token2embedding.keys(),4)}\n",
        "  UNK, PAD, BOS, EOS = '<unk>', '<pad>', '<bos>', '<eos>'\n",
        "  token2idx[PAD] = 0 \n",
        "  token2idx[UNK] = 1\n",
        "  token2idx[BOS] = 2\n",
        "  token2idx[EOS] = 3\n",
        "  idx2token = {idx: token for token, idx in token2idx.items()}\n",
        "  idx2embedding = {token2idx[token]: embedding for token, embedding in token2embedding.items()}\n",
        "\n",
        "  idx2embedding[0] = [.0] * int(meta_info[-1])\n",
        "  idx2embedding[1] = [.0] * int(meta_info[-1])\n",
        "  idx2embedding[2] = np.random.random(int(meta_info[-1])).tolist()\n",
        "  idx2embedding[3] = np.random.random(int(meta_info[-1])).tolist()\n",
        "  emb_mat = [idx2embedding[idx] for idx in range(len(idx2embedding))]\n",
        "\n",
        "  return torch.tensor(emb_mat, dtype=torch.float), token2idx, len(vocab) + 4"
      ],
      "metadata": {
        "id": "U0PRXceWF3Fv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix, token2idx, config['vocab_size'] = get_embedding(vocab, config['embedding_file_path'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD-tn73jSuBi",
        "outputId": "3fd6c779-5f72-4220-aef2-4ce7206fb76a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing embedding file, please wait...\n",
            "b'195202' tokens in embedding file in total, vector size is b'300'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195202/195202 [00:02<00:00, 66453.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "def tokenizer(sent, token2id):\n",
        "  # .get() 找到返回 token的id, 没找到就返回 1 1->UNK\n",
        "  ids = [token2id.get(token, 1) for token in jieba.cut(sent)]\n",
        "  return ids"
      ],
      "metadata": {
        "id": "qgfv519gw_JU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "df = pd.DataFrame(np.random.randn(4, 3), columns = ['col1', 'col2', 'col3'])\n",
        "df\n",
        "\n",
        "\tcol1\t  col2\t   col3\n",
        "0\t0.956880\t0.787811\t0.099237\n",
        "1\t0.413166\t-0.541869\t0.548336\n",
        "2\t0.951179\t0.113981\t1.130187\n",
        "3\t0.802346\t1.953860\t-2.062042\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "  print('i:',i,'row:',row)\n",
        "\n",
        "i: 0 row: col1    0.956880\n",
        "col2    0.787811\n",
        "col3    0.099237\n",
        "Name: 0, dtype: float64\n",
        "i: 1 row: col1    0.413166\n",
        "col2   -0.541869\n",
        "col3    0.548336\n",
        "Name: 1, dtype: float64\n",
        "i: 2 row: col1    0.951179\n",
        "col2    0.113981\n",
        "col3    1.130187\n",
        "Name: 2, dtype: float64\n",
        "i: 3 row: col1    0.802346\n",
        "col2    1.953860\n",
        "col3   -2.062042\n",
        "Name: 3, dtype: float64\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "  print('row[0]:',row[0])\n",
        "\n",
        "row[0]: 0.9568798249976214\n",
        "row[0]: 0.41316597459220283\n",
        "row[0]: 0.9511794577235149\n",
        "row[0]: 0.8023455023210482\n",
        "```"
      ],
      "metadata": {
        "id": "6-qPPSz-z9rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(data_df, train_val_ratio, token2id, mode = 'train'):\n",
        "  if mode == 'train':\n",
        "    X_train, y_train = defaultdict(list),[]\n",
        "    X_val, y_val = defaultdict(list),[]\n",
        "    num_val = int(len(data_df) * train_val_ratio)\n",
        "  else:\n",
        "    X_test, y_test = defaultdict(list),[]\n",
        "  \n",
        "  for i, row in tqdm(data_df.iterrows(), desc=f'Preprocessing {mode} data', total = len(data_df)):\n",
        "    text_left = row[0]\n",
        "    text_right = row[1]\n",
        "    label = row[2]\n",
        "\n",
        "    input_a = tokenizer(text_left, token2id = token2idx)\n",
        "    input_b = tokenizer(text_right, token2id = token2idx)\n",
        "    \n",
        "    if mode == 'train':\n",
        "      if i<num_val:\n",
        "        X_val['text_left'].append(input_a)\n",
        "        X_val['text_right'].append(input_b)\n",
        "        y_val.append(label)\n",
        "      else:\n",
        "        X_train['text_left'].append(input_a)\n",
        "        X_train['text_right'].append(input_b)\n",
        "        y_train.append(label)\n",
        "    else:\n",
        "      X_test['text_left'].append(input_a)\n",
        "      X_test['text_right'].append(input_b)\n",
        "      y_test.append(label)\n",
        "\n",
        "  if mode == 'train':\n",
        "    label2id = {label : i for i,label in enumerate(np.unique(y_train))}\n",
        "    id2label = {i : label for label, i in label2id.items()}\n",
        "    y_train = torch.tensor([label2id[label] for label in y_train], dtype= torch.long)\n",
        "    y_val = torch.tensor([label2id[label] for label in y_val], dtype= torch.long)\n",
        "    return X_train, y_train, X_val, y_val, label2id, id2label\n",
        "  else:\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "    return X_test, y_test"
      ],
      "metadata": {
        "id": "l8ICD-VFxjym"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_val, y_val, label2id, id2label = read_data(train_df, config['train_val_ratio'], token2idx, mode='train')\n",
        "X_test, y_test = read_data(test_df, config['train_val_ratio'], token2idx, mode='test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJxabq4n4zYw",
        "outputId": "a29f3688-344f-446f-c1b9-330bc05ff7f8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing train data: 100%|██████████| 38650/38650 [00:09<00:00, 3999.94it/s]\n",
            "Preprocessing test data: 100%|██████████| 3861/3861 [00:00<00:00, 4198.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    data = (self.x['text_left'][idx],\n",
        "         self.x['text_right'][idx],\n",
        "         self.y[idx]   \n",
        "            )\n",
        "    return data\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.y.size(0)#行数"
      ],
      "metadata": {
        "id": "4FM96AIK-19O"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "TextCNN中collete_fn函数\n",
        "def collete_fn(examples):\n",
        "    input_ids_list = []\n",
        "    labels =[]\n",
        "    for example in examples:\n",
        "        input_ids_list.append(example['input_ids'])\n",
        "        labels.append(example['label'])\n",
        "\n",
        "    # 对齐操作 -- 找到 input_ids_list 中 最长的 句子， 执行短句子补齐\n",
        "    # 1. 找到 input_ids_list 中 最长的 句子\n",
        "    max_length = max(len(input_ids) for input_ids in input_ids_list) \n",
        "    # 2. 定义一个 input_ids_tensor, 我们要把 每个 input_ids 放入 tensor 中\n",
        "    input_ids_tensor = torch.zeros((len(labels), max_length), dtype=torch.long)\n",
        "    for i, input_ids in enumerate(input_ids_list):\n",
        "        # 得到当前句子的长度\n",
        "        seq_len = len(input_ids)\n",
        "        # 第i个句子，填充 seq_len 这么长\n",
        "        input_ids_tensor[i, :seq_len] = torch.tensor(input_ids, dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'input_ids' : input_ids_tensor,\n",
        "        'labels' : torch.tensor(labels, dtype=torch.long)\n",
        "    }\n",
        "```"
      ],
      "metadata": {
        "id": "f42aK8fiAagL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# __call__方法的调用\n",
        "class MyClass():\n",
        "  def __call__(self):\n",
        "    print('__call__方法被调用')\n",
        "    return 'done'"
      ],
      "metadata": {
        "id": "SngM5VBdATU8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 类实例化\n",
        "obj = MyClass()"
      ],
      "metadata": {
        "id": "mxbuYVYqBhnk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化的对象 当作函数用\n",
        "res = obj()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mru9AQZJBlEr",
        "outputId": "7b61dc36-3211-406d-afba-5dbfdc02d3bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__call__方法被调用\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 从 AFQMCDataset 输出的 data  = (sentence1, sentence2, label])\n",
        "# 1. 将元组中属于sentence1的放在一起，属于sentence2的放在一起，属于label的放在一起\n",
        "# 2. 对齐操作，找到sentence1, sentence2,最长的句子，执行短句子补齐\n",
        "# 3. 定义一个tensor，把数据放里面\n",
        "\n",
        "class Collator:\n",
        "  def __init__(self, max_seq_len):\n",
        "    self.max_seq_len = max_seq_len\n",
        "  \n",
        "  def get_max_seq_len(self, ids_list):\n",
        "    cur_max_seq_len = max(len(input_id) for input_id in ids_list)\n",
        "    max_seq_len = min (self.max_seq_len, cur_max_seq_len)\n",
        "    return max_seq_len\n",
        "\n",
        "  #当某个方法不需要用到对象中的任何资源(没有self),将这个方法改为一个静态方法, 加一个@staticmethod。\n",
        "  #加上之后, 这个方法就和普通的函数没有什么区别了, 只不过写在了一个类中, 可以使用这个类的对象调用,\n",
        "  @staticmethod\n",
        "  def pad_and_truncate(text_ids_list, max_seq_len):\n",
        "    input_ids = torch.zeros((len(text_ids_list), max_seq_len),dtype=torch.long)\n",
        "    for i, text_ids in enumerate(text_ids_list):\n",
        "      seq_len = min(len(text_ids), max_seq_len)\n",
        "      input_ids[i, :seq_len] = torch.tensor(text_ids[:seq_len],dtype=torch.long)\n",
        "    return input_ids\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    # 1. 将元组中属于sentence1的放在一起，属于sentence2的放在一起，属于label的放在一起\n",
        "    text_ids_left_list, text_ids_right_list, labels_list = list(zip(*examples))\n",
        "    # 2.1 找到 text_ids_left_list, text_ids_right_list 最长的句子长度\n",
        "    max_text_left_length = self.get_max_seq_len(text_ids_left_list)\n",
        "    max_text_right_length = self.get_max_seq_len(text_ids_right_list)\n",
        "\n",
        "    # 2.2 执行短句子补齐, 3.定义一个tensor，把数据放里面\n",
        "    text_left_ids = self.pad_and_truncate(text_ids_left_list, max_text_left_length)\n",
        "    text_right_ids = self.pad_and_truncate(text_ids_right_list, max_text_right_length)\n",
        "    labels = torch.tensor(labels_list, dtype = torch.long)\n",
        "\n",
        "    data_list = [text_left_ids, text_right_ids, labels]\n",
        "    return data_list"
      ],
      "metadata": {
        "id": "vPyMg2fQCYzT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def build_dataloader(train_df, test_df, config, vocab):\n",
        "  X_train, y_train ,X_val, y_val, label2id, id2label = read_data(train_df, config['train_val_ratio'], vocab, mode='train')\n",
        "  X_test, y_test = read_data(test_df, config['train_val_ratio'], vocab, mode='test')\n",
        "\n",
        "  train_dataset = AFQMCDataset(X_train, y_train)\n",
        "  val_dataset = AFQMCDataset(X_val, y_val)\n",
        "  test_dataset = AFQMCDataset(X_test, y_test)\n",
        "  \n",
        "  collate_fn = Collator(config['max_seq_len'])\n",
        "\n",
        "  train_dataloader = DataLoader(dataset = train_dataset, batch_size = config['batch_size'], num_workers = 4, shuffle = True, collate_fn = collate_fn)\n",
        "  val_dataloader = DataLoader(dataset = val_dataset, batch_size = config['batch_size'], num_workers = 4, shuffle = False, collate_fn = collate_fn)\n",
        "  test_dataloader = DataLoader(dataset = test_dataset, batch_size = config['batch_size'], num_workers = 4, shuffle = False, collate_fn = collate_fn)\n",
        "\n",
        "  return id2label, test_dataloader, val_dataloader, train_dataloader"
      ],
      "metadata": {
        "id": "UckMgCUjF7u5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label, test_dataloader, val_dataloader, train_dataloader = build_dataloader(train_df, test_df, config, token2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvgSVGf-Hxew",
        "outputId": "a41d027f-ef6d-4f46-ef49-065e2e28495b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing train data: 100%|██████████| 38650/38650 [00:09<00:00, 4032.80it/s]\n",
            "Preprocessing test data: 100%|██████████| 3861/3861 [00:00<00:00, 4147.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataloader:\n",
        "  print('dataloader中一个batch数据为（左+右+label）:',i)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeJ8-aPCtn3y",
        "outputId": "797d5c0a-ff74-4fe6-9361-70f21fa3df49"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataloader中一个batch数据为（左+右+label）: [tensor([[ 272, 1359,  410,  ...,    0,    0,    0],\n",
            "        [2493, 1448, 1359,  ...,    0,    0,    0],\n",
            "        [ 190, 1412,   22,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [2095,  189, 1226,  ...,    0,    0,    0],\n",
            "        [ 925,  981,  407,  ...,    0,    0,    0],\n",
            "        [2493, 1448, 1359,  ...,    0,    0,    0]]), tensor([[ 410,  272, 1359,  ...,    0,    0,    0],\n",
            "        [2493, 1448, 1359,  ...,    0,    0,    0],\n",
            "        [  22,    5, 1448,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [  22,    5,  272,  ...,    0,    0,    0],\n",
            "        [ 272, 1359,  925,  ...,    0,    0,    0],\n",
            "        [ 410,  130, 1448,  ..., 1090,   21,   69]]), tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_iterator = tqdm(val_dataloader, desc='Test_item', total=len(val_dataloader))\n",
        "for batch in val_iterator:\n",
        "  print('batch:',batch)\n",
        "  i=0\n",
        "  for item in batch:\n",
        "    print('item:',item)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzAIHuyr1QwL",
        "outputId": "36f7231e-47d4-42dd-d32e-9e1305171b5c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test_item:   0%|          | 0/121 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: [tensor([[2493, 1448, 1359,  ...,    0,    0,    0],\n",
            "        [2493,  272, 1359,  ...,    0,    0,    0],\n",
            "        [ 407,   22,   70,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 272, 1359,  607,  ...,    0,    0,    0],\n",
            "        [   1, 1359,   16,  ...,    0,    0,    0],\n",
            "        [ 272, 1359, 4406,  ...,    0,    0,    0]]), tensor([[1448, 1359,   15,  ...,    0,    0,    0],\n",
            "        [2493,  272, 1359,  ...,    0,    0,    0],\n",
            "        [ 111,   69,  272,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [1090, 1090, 1090,  ...,    0,    0,    0],\n",
            "        [   1,  272, 1359,  ...,  703,   38,    0],\n",
            "        [ 272, 1359,  272,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
            "item: tensor([[2493, 1448, 1359,  ...,    0,    0,    0],\n",
            "        [2493,  272, 1359,  ...,    0,    0,    0],\n",
            "        [ 407,   22,   70,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 272, 1359,  607,  ...,    0,    0,    0],\n",
            "        [   1, 1359,   16,  ...,    0,    0,    0],\n",
            "        [ 272, 1359, 4406,  ...,    0,    0,    0]])\n",
            "item: tensor([[1448, 1359,   15,  ...,    0,    0,    0],\n",
            "        [2493,  272, 1359,  ...,    0,    0,    0],\n",
            "        [ 111,   69,  272,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [1090, 1090, 1090,  ...,    0,    0,    0],\n",
            "        [   1,  272, 1359,  ...,  703,   38,    0],\n",
            "        [ 272, 1359,  272,  ...,    0,    0,    0]])\n",
            "item: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def evaluation(config, model, val_dataloader):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  labels = []\n",
        "  val_loss = 0.\n",
        "  val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
        "  with torch.no_grad():\n",
        "    for batch in val_iterator:\n",
        "      labels.append(batch[-1])\n",
        "      batch = [item.to(config['device']) for item in batch]\n",
        "      loss, logits = model(batch)[:2]\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      # 返回逻辑值最大的位置，要么0，要么1\n",
        "      preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "\n",
        "  avg_val_loss = val_loss / len(val_dataloader)\n",
        "  labels = torch.cat(labels, dim=0).numpy()\n",
        "  preds = torch.cat(preds, dim=0).numpy()\n",
        "  f1 = f1_score(labels, preds, average='macro')\n",
        "\n",
        "  acc = accuracy_score(labels, preds)\n",
        "\n",
        "  return avg_val_loss, f1, acc"
      ],
      "metadata": {
        "id": "Wgjrz85kt48Z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "from transformers import AdamW\n",
        "\n",
        "def train(model, config, id2label, train_dataloader, val_dataloader):\n",
        "  optimizer = AdamW(model.parameters(), lr = config['learning_rate'])\n",
        "  epoch_iterator = trange(config['num_epochs'])\n",
        "\n",
        "  global_steps = 0\n",
        "  train_loss = 0.\n",
        "  logging_loss = 0.\n",
        "\n",
        "  for epoch in epoch_iterator:\n",
        "    train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
        "    model.train()\n",
        "    for batch in train_iterator:\n",
        "\n",
        "      batch =  [item.to(config['device']) for item in batch]\n",
        "      loss = model(batch)[0]\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      global_steps += 1\n",
        "\n",
        "      if global_steps % config['logging_step'] == 0:\n",
        "        print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
        "        logging_loss = train_loss\n",
        "\n",
        "        avg_val_loss, f1, acc = evaluation(config, model, val_dataloader)\n",
        "\n",
        "        print_log = f'>>>traing loss:{print_train_loss: .5f}, valid loss:{avg_val_loss: .5f}, valid f1 score:{f1: .5f}, valid acc:{acc: .5f}'\n",
        "\n",
        "        print(print_log)\n",
        "        model.train()\n",
        "  return model"
      ],
      "metadata": {
        "id": "NYMC8N5W4yoK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(config, id2label, model, test_dataloader):\n",
        "  test_iterator = tqdm(test_dataloader, desc='Predicting', total=len(test_dataloader))\n",
        "  model.eval()\n",
        "  test_preds = []\n",
        "  with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "      batch = [item.to(config['device']) for item in batch]\n",
        "      logits = model(batch)[1]\n",
        "      test_preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "  test_preds = torch.cat(test_preds, dim = 0).numpy()\n",
        "  test_preds = [id2label[id_] for id_ in test_preds]\n",
        "  return test_preds"
      ],
      "metadata": {
        "id": "-Z4-MtoB9qQP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 预备知识"
      ],
      "metadata": {
        "id": "lfox0JdZHzKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个[2,3,4]的tensor -> [batch_size, seq_len, embedding_dim]  一个batch有两个句子，每个句子3个词，每个词的维度为4\n",
        "a = torch.tensor([[[2., 2., 2., 2.],[3., 3., 3., 3.],[4., 4., 4., 4.]],[[5., 5., 5., 5.],[6., 6., 6., 6.],[7., 7., 7., 7.]]])\n",
        "print(a)\n",
        "print(a.size())"
      ],
      "metadata": {
        "id": "lekcYK7VHyl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8b091e-22f8-499a-eb6f-f34e0c2c664a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[2., 2., 2., 2.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [4., 4., 4., 4.]],\n",
            "\n",
            "        [[5., 5., 5., 5.],\n",
            "         [6., 6., 6., 6.],\n",
            "         [7., 7., 7., 7.]]])\n",
            "torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [2,1,4]的tensor\n",
        "# torch.Tensor是一种包含单一数据类型元素的多维矩阵，torch.Tensor是默认的tensor类型（torch.FlaotTensor）的简称。\n",
        "# torch.ByteTensor 是 CPU tensor, 8-bit integer (unsigned)\n",
        "ones = torch.ByteTensor([[[1, 1, 0, 0]],[[0, 1, 1, 0]]])\n",
        "print(ones.size())"
      ],
      "metadata": {
        "id": "fVxkqiQiItQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca35061-f1f4-4379-cdbb-623a13e289a5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 相当于将词向量某些维度清零\n",
        "print(ones * a) # 首先，ones. 从[2,1,4] 变成 [2,3,4] （填充） 再与a中逐个元素相乘\n",
        "print((ones * a).size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKyKg9NaLHOE",
        "outputId": "c53bb98a-ee2e-4a15-df37-1379dc5ce0b3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[2., 2., 0., 0.],\n",
            "         [3., 3., 0., 0.],\n",
            "         [4., 4., 0., 0.]],\n",
            "\n",
            "        [[0., 5., 5., 0.],\n",
            "         [0., 6., 6., 0.],\n",
            "         [0., 7., 7., 0.]]])\n",
            "torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## masked fill 方法"
      ],
      "metadata": {
        "id": "ie4SqRHVb5Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [2, 3, 1]\n",
        "mask = torch.ByteTensor([[[1],[1],[0]],[[0],[1],[1]]])\n",
        "print(mask)\n",
        "print(mask.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hjnj8kn1b8zA",
        "outputId": "988b6c49-ff3f-43a9-cd0c-0a8d06724f76"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1],\n",
            "         [1],\n",
            "         [0]],\n",
            "\n",
            "        [[0],\n",
            "         [1],\n",
            "         [1]]], dtype=torch.uint8)\n",
            "torch.Size([2, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "a:\n",
        "tensor([[[2., 2., 2., 2.],\n",
        "    [3., 3., 3., 3.],\n",
        "    [4., 4., 4., 4.]],\n",
        "\n",
        "    [[5., 5., 5., 5.],\n",
        "    [6., 6., 6., 6.],\n",
        "    [7., 7., 7., 7.]]])\n",
        "\n",
        "mask:\n",
        "tensor([[[1],\n",
        "    [1],\n",
        "    [0]],\n",
        "\n",
        "    [[0],\n",
        "    [1],\n",
        "    [1]]], dtype=torch.uint8)\n",
        "```"
      ],
      "metadata": {
        "id": "2u6XC1WJdbYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a [2,3,4] mask[2,3,1]\n",
        "# 是将 mask 中 为1 的元素所在的索引，在 a 中 相同索引处替换为 value值\n",
        "# 把某个词向量给mask掉\n",
        "b = a.masked_fill(mask, value = torch.tensor(-1e7))\n",
        "print(b)\n",
        "print(b.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og5QgPilc6wo",
        "outputId": "62b88f6b-39f7-43ed-b565-3abb1170d1bb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
            "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
            "         [ 4.0000e+00,  4.0000e+00,  4.0000e+00,  4.0000e+00]],\n",
            "\n",
            "        [[ 5.0000e+00,  5.0000e+00,  5.0000e+00,  5.0000e+00],\n",
            "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
            "         [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07]]])\n",
            "torch.Size([2, 3, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:41: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` \n",
        "ones:\n",
        "tensor([[[1, 1, 0, 0]],\n",
        "    [[0, 1, 1, 0]]])\n",
        "```"
      ],
      "metadata": {
        "id": "vcY4I42-gtdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 同理，可以用[2, 1, 4] mask掉 [2, 3, 4]\n",
        "# 把 mask 改成 ones\n",
        "mask = ones #[2, 1, 4]\n",
        "b= a.masked_fill(mask, value = torch.tensor(-1e7))\n",
        "print(b)\n",
        "print(b.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyOsoRhBfuGm",
        "outputId": "4a56e834-4901-4094-d4dd-cb878042ed67"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.0000e+07, -1.0000e+07,  2.0000e+00,  2.0000e+00],\n",
            "         [-1.0000e+07, -1.0000e+07,  3.0000e+00,  3.0000e+00],\n",
            "         [-1.0000e+07, -1.0000e+07,  4.0000e+00,  4.0000e+00]],\n",
            "\n",
            "        [[ 5.0000e+00, -1.0000e+07, -1.0000e+07,  5.0000e+00],\n",
            "         [ 6.0000e+00, -1.0000e+07, -1.0000e+07,  6.0000e+00],\n",
            "         [ 7.0000e+00, -1.0000e+07, -1.0000e+07,  7.0000e+00]]])\n",
            "torch.Size([2, 3, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:41: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESIM \n",
        "## Enhanced LSTM for Natural Language Inference"
      ],
      "metadata": {
        "id": "NxMZp4BGiwQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    'embedding': embedding_matrix, #torch.Size([5251,300])\n",
        "    'freeze_emb': True,\n",
        "    'hidden_size': 256,\n",
        "    'dropout': 0.3,\n",
        "    'num_layers': 2,\n",
        "    'concat_layers': True,\n",
        "    'rnn_type': 'lstm',\n",
        "    'num_labels': len(id2label)\n",
        "}"
      ],
      "metadata": {
        "id": "3eZUHxRBiz58"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 一些参数\n",
        "```\n",
        "B: batch_size\n",
        "L = 'inputs left'  sequence length\n",
        "R = 'inputs right'  sequence length\n",
        "D = embedding size\n",
        "H = hidden size\n",
        "```"
      ],
      "metadata": {
        "id": "5FYVfzkKA6xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class RNNDropout(nn.Dropout):\n",
        "  # 将词向量 某些维度 清0\n",
        "  # sequences_batch [B, L, D] (和query、doc一样)\n",
        "  def forward(self, sequences_batch):\n",
        "    # ones [B, D]\n",
        "    ones = sequences_batch.data.new_ones(sequences_batch.shape[0], sequences_batch.shape[-1])\n",
        "    # 随机mask ones\n",
        "    # dropout_mask [B, D] ones后面是dropout API的参数  \n",
        "    # p – probability of an element to be zeroed. Default: 0.5\n",
        "    # training – apply dropout if is True. Default: True\n",
        "    # inplace – If set to True, will do this operation in-place[将原地执行此操作]. Default: False\n",
        "    dropout_mask = nn.functional.dropout(ones, self.p, self.training, inplace=False)\n",
        "    # unsqueeze(1)增加一个维度\n",
        "    return dropout_mask.unsqueeze(1) * sequences_batch\n"
      ],
      "metadata": {
        "id": "NROHeernAMmO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "class StackedBRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers,\n",
        "         dropout_rate = 0, dropout_output = False,\n",
        "         rnn_type = nn.LSTM, concat_layers = False):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.dropout_output = dropout_output\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.num_layers = num_layers\n",
        "    self.concat_layers = concat_layers\n",
        "    self.rnns = nn.ModuleList()\n",
        "    # 共有2层lstm\n",
        "    for i in range(num_layers):\n",
        "      input_size = input_size if i == 0 else 2*hidden_size\n",
        "      self.rnns.append(rnn_type(input_size, hidden_size, num_layers=1, bidirectional=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "      # x (B, L, D)[此时不能输入到LSTM中] -> (L, B, D)\n",
        "      x= x.transpose(0, 1)\n",
        "      outputs = [x]\n",
        "      for i in range(self.num_layers):\n",
        "        rnn_input = outputs[-1]\n",
        "\n",
        "        if self.dropout_rate > 0:\n",
        "          rnn_input = F.dropout(rnn_input, p=self.dropout_rate, training = self.training)\n",
        "\n",
        "\n",
        "        # self.rnn[i](rnn_input) (output, (h_n, c_n))\n",
        "        rnn_output = self.rnns[i](rnn_input)[0]\n",
        "        outputs.append(rnn_output)\n",
        "      # outputs [x, output0, output1]\n",
        "      if self.concat_layers:\n",
        "        output = torch.cat(outputs[1:], 2)\n",
        "      else:\n",
        "        output = outputs[-1]\n",
        "      # output (L, B, D) -> (B, L, D)\n",
        "      output = output.transpose(0, 1)\n",
        "\n",
        "      if self.dropout_output and self.dropout_rate > 0:\n",
        "        output = F.dropout(output, p = self.dropout_rate, training = self.training)\n",
        "      \n",
        "      # 进行 transpose之后，tensor在内存中不连续， contiguous将output内存连续\n",
        "      return output.contiguous()"
      ],
      "metadata": {
        "id": "Oi8C7HTgF7Lq"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}